{"file_contents":{"elevenlabs_dubbing.py":{"content":"import os\nimport time\nimport tempfile\nfrom elevenlabs.client import ElevenLabs\nfrom typing import Optional, Dict\n\nclass ElevenLabsDubbing:\n    \"\"\"Handles video dubbing using ElevenLabs Dubbing API\"\"\"\n    \n    def __init__(self, api_key: str):\n        \"\"\"Initialize ElevenLabs dubbing service\"\"\"\n        self.client = ElevenLabs(api_key=api_key)\n        \n        # Language code mapping\n        self.language_codes = {\n            'en': 'en',\n            'hi': 'hi',\n            'es': 'es',\n            'fr': 'fr',\n            'de': 'de',\n            'it': 'it',\n            'pt': 'pt',\n            'ja': 'ja',\n            'ko': 'ko',\n            'zh': 'zh'\n        }\n    \n    def create_dubbing_project(self, video_path: str, source_lang: str, \n                              target_lang: str, project_name: str = \"Dubbing Project\") -> Optional[str]:\n        \"\"\"\n        Upload video and create dubbing project on ElevenLabs\n        Returns: dubbing_id if successful\n        \"\"\"\n        try:\n            source_code = self.language_codes.get(source_lang, 'en')\n            target_code = self.language_codes.get(target_lang, 'hi')\n            \n            # Upload video to ElevenLabs for dubbing\n            with open(video_path, 'rb') as video_file:\n                response = self.client.dubbing.create(\n                    target_lang=target_code,\n                    file=video_file,\n                    mode=\"automatic\",\n                    source_lang=source_code,\n                    num_speakers=1,\n                    name=project_name,\n                    watermark=True\n                )\n            \n            dubbing_id = response.dubbing_id\n            print(f\"Created dubbing project: {dubbing_id}\")\n            return dubbing_id\n            \n        except Exception as e:\n            print(f\"Error creating dubbing project: {e}\")\n            return None\n    \n    def get_dubbing_status(self, dubbing_id: str) -> Dict:\n        \"\"\"\n        Check the status of a dubbing project\n        Returns: {'status': 'dubbing'|'dubbed'|'failed', 'metadata': ...}\n        \"\"\"\n        try:\n            metadata = self.client.dubbing.get(\n                dubbing_id=dubbing_id\n            )\n            \n            return {\n                'status': metadata.status,\n                'metadata': metadata,\n                'name': metadata.name if hasattr(metadata, 'name') else None\n            }\n            \n        except Exception as e:\n            print(f\"Error getting dubbing status: {e}\")\n            return {'status': 'error', 'metadata': None}\n    \n    def wait_for_dubbing_completion(self, dubbing_id: str, \n                                   callback=None, max_wait_seconds: int = 600) -> bool:\n        \"\"\"\n        Wait for dubbing to complete with optional progress callback\n        callback: function(status, elapsed_time) called periodically\n        Returns: True if successful, False otherwise\n        \"\"\"\n        try:\n            start_time = time.time()\n            \n            while True:\n                elapsed = time.time() - start_time\n                \n                # Check timeout\n                if elapsed > max_wait_seconds:\n                    print(f\"Dubbing timed out after {max_wait_seconds} seconds\")\n                    return False\n                \n                # Get status\n                metadata = self.client.dubbing.get(\n                    dubbing_id=dubbing_id\n                )\n                \n                status = metadata.status\n                \n                # Call progress callback\n                if callback:\n                    callback(status, int(elapsed))\n                \n                # Check completion\n                if status == \"dubbed\":\n                    print(\"Dubbing completed successfully!\")\n                    return True\n                elif status == \"dubbing\":\n                    print(f\"Still processing... ({int(elapsed)}s elapsed)\")\n                    time.sleep(5)  # Poll every 5 seconds\n                else:\n                    print(f\"Dubbing failed with status: {status}\")\n                    return False\n                    \n        except Exception as e:\n            print(f\"Error waiting for dubbing: {e}\")\n            return False\n    \n    def download_dubbed_video(self, dubbing_id: str, target_lang: str) -> Optional[str]:\n        \"\"\"\n        Download the dubbed video from ElevenLabs\n        Returns: Path to downloaded video file\n        \"\"\"\n        try:\n            target_code = self.language_codes.get(target_lang, 'hi')\n            \n            # Get the dubbed file\n            audio_stream = self.client.dubbing.audio.get(\n                dubbing_id=dubbing_id,\n                language_code=target_code\n            )\n            \n            # Save to temporary file\n            output_path = tempfile.mktemp(suffix='.mp4')\n            \n            with open(output_path, 'wb') as f:\n                for chunk in audio_stream:\n                    f.write(chunk)\n            \n            print(f\"Downloaded dubbed video to: {output_path}\")\n            return output_path\n            \n        except Exception as e:\n            print(f\"Error downloading dubbed video: {e}\")\n            return None\n    \n    def dub_video_complete(self, video_path: str, source_lang: str, \n                          target_lang: str, progress_callback=None) -> Optional[str]:\n        \"\"\"\n        Complete dubbing workflow: upload, wait, download\n        Returns: Path to dubbed video or None if failed\n        \"\"\"\n        try:\n            # Step 1: Create dubbing project\n            if progress_callback:\n                progress_callback(\"Uploading video to ElevenLabs...\", 10)\n            \n            dubbing_id = self.create_dubbing_project(\n                video_path, \n                source_lang, \n                target_lang,\n                project_name=f\"Dub {source_lang} to {target_lang}\"\n            )\n            \n            if not dubbing_id:\n                return None\n            \n            # Step 2: Wait for completion\n            if progress_callback:\n                progress_callback(\"Processing on ElevenLabs servers...\", 30)\n            \n            def status_callback(status, elapsed):\n                if progress_callback:\n                    progress = min(30 + (elapsed // 2), 80)  # Progress from 30% to 80%\n                    progress_callback(f\"Dubbing in progress... ({elapsed}s)\", progress)\n            \n            success = self.wait_for_dubbing_completion(\n                dubbing_id,\n                callback=status_callback,\n                max_wait_seconds=600\n            )\n            \n            if not success:\n                return None\n            \n            # Step 3: Download result\n            if progress_callback:\n                progress_callback(\"Downloading dubbed video...\", 90)\n            \n            dubbed_video_path = self.download_dubbed_video(dubbing_id, target_lang)\n            \n            if progress_callback:\n                progress_callback(\"Complete!\", 100)\n            \n            return dubbed_video_path\n            \n        except Exception as e:\n            print(f\"Error in complete dubbing workflow: {e}\")\n            return None\n    \n    def get_available_languages(self) -> Dict[str, str]:\n        \"\"\"Get list of supported languages\"\"\"\n        return {\n            'en': 'English',\n            'hi': 'Hindi',\n            'es': 'Spanish',\n            'fr': 'French',\n            'de': 'German',\n            'it': 'Italian',\n            'pt': 'Portuguese',\n            'ja': 'Japanese',\n            'ko': 'Korean',\n            'zh': 'Chinese'\n        }\n","size_bytes":7641},"app.py":{"content":"import streamlit as st\nimport tempfile\nimport os\nfrom pathlib import Path\nimport time\nfrom video_processor import VideoProcessor\nfrom elevenlabs_dubbing import ElevenLabsDubbing\nfrom utils import format_time, validate_video_file\nimport speech_recognition as sr\nfrom elevenlabs import ElevenLabs\nfrom google import genai\nfrom pydub import AudioSegment\nfrom youtube_summarizer import YouTubeSummarizer\nfrom story_generator import StoryGenerator\nfrom article_to_podcast import ArticleToPodcast\n\nst.set_page_config(\n    page_title=\"Anuvaad AI - Professional Video Dubbing\",\n    page_icon=\"ğŸŒ\",\n    layout=\"wide\",\n    initial_sidebar_state=\"collapsed\"\n)\n\nst.markdown(\"\"\"\n    <style>\n    @import url('https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700;800&display=swap');\n    \n    [data-testid=\"stHeader\"] {\n        display: none !important;\n    }\n    \n    [data-testid=\"stToolbar\"] {\n        display: none !important;\n    }\n    \n    html, body, [data-testid=\"stAppViewContainer\"], .main {\n        margin: 0 !important;\n        padding: 0 !important;\n    }\n    \n    * {\n        font-family: 'Inter', sans-serif;\n    }\n    \n    .stApp {\n        background: #0f172a;\n    }\n    \n    .main {\n        padding: 0 !important;\n        margin: 0 !important;\n    }\n    \n    .block-container {\n        padding: 0 2rem !important;\n        margin: 0 !important;\n        max-width: 100% !important;\n    }\n    \n    section[data-testid=\"stVerticalBlock\"] > div {\n        padding: 0 !important;\n    }\n    \n    div[data-testid=\"stVerticalBlockBorderWrapper\"] {\n        border: 5px solid !important;\n        border-image: linear-gradient(135deg, #60a5fa, #a78bfa, #ec4899) 1 !important;\n        border-radius: 24px !important;\n        padding: 2.5rem !important;\n        background: linear-gradient(135deg, rgba(15, 23, 42, 0.95), rgba(30, 41, 59, 0.95)) !important;\n        box-shadow: \n            0 0 80px rgba(96, 165, 250, 0.9),\n            0 0 150px rgba(167, 139, 250, 0.7),\n            0 25px 80px rgba(0, 0, 0, 0.8),\n            inset 0 0 60px rgba(96, 165, 250, 0.15) !important;\n        position: relative !important;\n        outline: 5px solid transparent !important;\n        outline-offset: -5px !important;\n    }\n    \n    div[data-testid=\"stVerticalBlockBorderWrapper\"]::before {\n        content: '' !important;\n        position: absolute !important;\n        inset: -5px !important;\n        border-radius: 24px !important;\n        padding: 5px !important;\n        background: linear-gradient(135deg, #60a5fa, #a78bfa, #ec4899) !important;\n        -webkit-mask: linear-gradient(#fff 0 0) content-box, linear-gradient(#fff 0 0) !important;\n        -webkit-mask-composite: xor !important;\n        mask-composite: exclude !important;\n        pointer-events: none !important;\n    }\n    \n    .header {\n        background: rgba(15, 23, 42, 0.95);\n        backdrop-filter: blur(10px);\n        padding: 1.5rem 4rem;\n        border-bottom: 1px solid rgba(148, 163, 184, 0.1);\n        position: sticky;\n        top: 0;\n        z-index: 1000;\n        box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1);\n        margin: 0;\n    }\n    \n    .logo {\n        font-size: 2rem;\n        font-weight: 800;\n        background: linear-gradient(135deg, #60a5fa 0%, #a78bfa 100%);\n        -webkit-background-clip: text;\n        -webkit-text-fill-color: transparent;\n        display: inline-block;\n    }\n    \n    .hero {\n        text-align: center;\n        padding: 5rem 2rem 3rem 2rem;\n        background: rgba(96, 165, 250, 0.05);\n        border-bottom: 1px solid rgba(148, 163, 184, 0.1);\n        margin: 0;\n    }\n    \n    .hero h1 {\n        font-size: 4rem;\n        font-weight: 800;\n        background: linear-gradient(135deg, #60a5fa 0%, #a78bfa 50%, #ec4899 100%);\n        -webkit-background-clip: text;\n        -webkit-text-fill-color: transparent;\n        margin-bottom: 1rem;\n        line-height: 1.2;\n    }\n    \n    .hero p {\n        font-size: 1.5rem;\n        color: #cbd5e1;\n        font-weight: 300;\n        max-width: 800px;\n        margin: 0 auto 2rem auto;\n    }\n    \n    .badge {\n        display: inline-block;\n        background: rgba(96, 165, 250, 0.2);\n        color: #60a5fa;\n        padding: 0.5rem 1.5rem;\n        border-radius: 50px;\n        font-weight: 600;\n        font-size: 0.9rem;\n        border: 1px solid rgba(96, 165, 250, 0.3);\n    }\n    \n    .content-section {\n        padding: 3rem 2rem;\n        max-width: 1400px;\n        margin: 0 auto;\n    }\n    \n    @media (max-width: 768px) {\n        .content-section {\n            padding: 2rem 1rem;\n        }\n        \n        .header {\n            padding: 1.5rem 2rem;\n        }\n        \n        .block-container {\n            padding: 0 1rem !important;\n        }\n    }\n    \n    .card {\n        background: rgba(30, 41, 59, 0.8);\n        backdrop-filter: blur(10px);\n        border-radius: 20px;\n        padding: 2.5rem;\n        border: 1px solid rgba(148, 163, 184, 0.1);\n        box-shadow: 0 20px 60px rgba(0, 0, 0, 0.3);\n        margin-bottom: 2rem;\n    }\n    \n    .feature-button-wrapper {\n        margin-top: auto;\n        padding-top: 1rem;\n        display: flex;\n        justify-content: center;\n    }\n    \n    .feature-button-wrapper .stButton {\n        width: 100%;\n        max-width: 400px;\n    }\n    \n    .feature-column {\n        display: flex;\n        flex-direction: column;\n        height: 100%;\n    }\n    \n    .feature-content-wrapper {\n        flex: 1;\n        display: flex;\n        flex-direction: column;\n    }\n    \n    div[data-testid=\"stVerticalBlockBorderWrapper\"] {\n        background: linear-gradient(135deg, rgba(59, 130, 246, 0.15) 0%, rgba(139, 92, 246, 0.15) 100%) !important;\n        border: 3px solid #60a5fa !important;\n        border-radius: 16px !important;\n        padding: 2rem !important;\n        backdrop-filter: blur(10px);\n        box-shadow: 0 8px 32px rgba(59, 130, 246, 0.2), 0 0 0 3px #60a5fa !important;\n        transition: all 0.3s ease;\n        min-height: 450px !important;\n        display: flex !important;\n        flex-direction: column !important;\n        outline: 3px solid #60a5fa !important;\n        outline-offset: -3px !important;\n    }\n    \n    div[data-testid=\"stVerticalBlockBorderWrapper\"]:hover {\n        border-color: #93c5fd !important;\n        outline-color: #93c5fd !important;\n        box-shadow: 0 12px 40px rgba(59, 130, 246, 0.3), 0 0 0 3px #93c5fd !important;\n        transform: translateY(-2px);\n    }\n    \n    div[data-testid=\"stVerticalBlockBorderWrapper\"] > div {\n        display: flex !important;\n        flex-direction: column !important;\n        height: 100% !important;\n    }\n    \n    [data-testid=\"stTooltipIcon\"] {\n        color: #60a5fa !important;\n        opacity: 1 !important;\n    }\n    \n    [data-testid=\"stTooltipIcon\"] svg {\n        fill: #60a5fa !important;\n        width: 18px !important;\n        height: 18px !important;\n    }\n    \n    .stButton {\n        display: flex;\n        justify-content: center;\n        margin-top: auto !important;\n    }\n    \n    .card-title {\n        font-size: 1.5rem;\n        font-weight: 700;\n        color: #f1f5f9;\n        margin-bottom: 1.5rem;\n        display: flex;\n        align-items: center;\n        gap: 0.5rem;\n    }\n    \n    .feature-grid {\n        display: grid;\n        grid-template-columns: repeat(auto-fit, minmax(250px, 1fr));\n        gap: 1.5rem;\n        margin: 2rem 0;\n    }\n    \n    .feature-box {\n        background: rgba(96, 165, 250, 0.1);\n        border: 1px solid rgba(96, 165, 250, 0.2);\n        border-radius: 15px;\n        padding: 1.5rem;\n        text-align: center;\n        transition: all 0.3s ease;\n    }\n    \n    .feature-box:hover {\n        background: rgba(96, 165, 250, 0.15);\n        transform: translateY(-5px);\n        box-shadow: 0 10px 30px rgba(96, 165, 250, 0.2);\n    }\n    \n    .feature-icon {\n        font-size: 2.5rem;\n        margin-bottom: 1rem;\n    }\n    \n    .feature-title {\n        font-size: 1.1rem;\n        font-weight: 600;\n        color: #f1f5f9;\n        margin-bottom: 0.5rem;\n    }\n    \n    .feature-desc {\n        font-size: 0.9rem;\n        color: #94a3b8;\n        line-height: 1.5;\n    }\n    \n    .stat-box {\n        background: linear-gradient(135deg, #3b82f6 0%, #8b5cf6 100%);\n        color: white;\n        padding: 1.5rem;\n        border-radius: 12px;\n        text-align: center;\n        box-shadow: 0 8px 20px rgba(59, 130, 246, 0.3);\n    }\n    \n    .stat-number {\n        font-size: 2rem;\n        font-weight: 800;\n        display: block;\n        margin-bottom: 0.3rem;\n    }\n    \n    .stat-label {\n        font-size: 0.85rem;\n        opacity: 0.9;\n        font-weight: 500;\n    }\n    \n    .stButton>button {\n        width: 100%;\n        background: linear-gradient(135deg, #3b82f6 0%, #8b5cf6 100%);\n        color: white;\n        border: none;\n        padding: 1rem 2rem;\n        font-size: 1.1rem;\n        font-weight: 600;\n        border-radius: 12px;\n        box-shadow: 0 8px 25px rgba(59, 130, 246, 0.4);\n        transition: all 0.3s ease;\n    }\n    \n    .stButton>button:hover {\n        transform: translateY(-3px);\n        box-shadow: 0 12px 35px rgba(59, 130, 246, 0.6);\n    }\n    \n    .stSelectbox label {\n        font-weight: 700 !important;\n        color: #ffffff !important;\n        font-size: 1.1rem !important;\n        margin-bottom: 0.8rem !important;\n        text-shadow: 0 2px 4px rgba(0, 0, 0, 0.3);\n    }\n    \n    .stSelectbox > div > div {\n        background: rgba(30, 41, 59, 0.95) !important;\n        border: 2px solid rgba(96, 165, 250, 0.5) !important;\n        color: #ffffff !important;\n    }\n    \n    .stSelectbox [data-baseweb=\"select\"] {\n        background: rgba(30, 41, 59, 0.95) !important;\n    }\n    \n    .stSelectbox [data-baseweb=\"select\"] > div {\n        background: rgba(30, 41, 59, 0.95) !important;\n        color: #ffffff !important;\n        font-weight: 600 !important;\n        font-size: 1.05rem !important;\n    }\n    \n    .stSelectbox svg {\n        fill: #60a5fa !important;\n        width: 24px !important;\n        height: 24px !important;\n    }\n    \n    .stSelectbox [aria-expanded=\"true\"] svg {\n        fill: #a78bfa !important;\n    }\n    \n    .stProgress > div > div > div {\n        background: linear-gradient(90deg, #3b82f6 0%, #8b5cf6 100%);\n    }\n    \n    .upload-zone {\n        border: 2px dashed rgba(96, 165, 250, 0.4);\n        border-radius: 15px;\n        padding: 2rem;\n        text-align: center;\n        background: rgba(96, 165, 250, 0.05);\n        transition: all 0.3s ease;\n    }\n    \n    .upload-zone:hover {\n        border-color: rgba(96, 165, 250, 0.6);\n        background: rgba(96, 165, 250, 0.1);\n    }\n    \n    .info-box {\n        background: rgba(59, 130, 246, 0.1);\n        border-left: 4px solid #3b82f6;\n        padding: 1rem 1.5rem;\n        border-radius: 8px;\n        color: #cbd5e1;\n        margin: 1rem 0;\n    }\n    \n    .success-box {\n        background: rgba(34, 197, 94, 0.1);\n        border-left: 4px solid #22c55e;\n        padding: 1rem 1.5rem;\n        border-radius: 8px;\n        color: #86efac;\n        margin: 1rem 0;\n    }\n    \n    .footer {\n        text-align: center;\n        padding: 2rem;\n        color: #64748b;\n        border-top: 1px solid rgba(148, 163, 184, 0.1);\n        margin-top: 3rem;\n    }\n    \n    h1, h2, h3, h4, h5, h6 {\n        color: #f1f5f9 !important;\n    }\n    \n    h5 {\n        color: #e0e7ff !important;\n        font-weight: 700 !important;\n        margin-bottom: 0.8rem !important;\n    }\n    \n    p {\n        color: #cbd5e1;\n    }\n    \n    .stVideo {\n        border-radius: 12px;\n        overflow: hidden;\n        box-shadow: 0 10px 30px rgba(0, 0, 0, 0.3);\n    }\n    </style>\n\"\"\", unsafe_allow_html=True)\n\ndef render_text_to_speech(elevenlabs_client):\n    st.markdown(\"### ğŸ—£ï¸ Text to Speech\")\n    \n    text_input = st.text_area(\n        \"Enter text to convert to speech\",\n        placeholder=\"Type or paste your text here...\",\n        height=150,\n        key=\"tts_input\"\n    )\n    \n    voice_id = st.selectbox(\n        \"Select Voice\",\n        [\"Rachel\", \"Adam\", \"Antoni\", \"Arnold\", \"Bella\", \"Domi\", \"Elli\", \"Josh\", \"Sam\"],\n        key=\"tts_voice\"\n    )\n    \n    voice_map = {\n        \"Rachel\": \"21m00Tcm4TlvDq8ikWAM\",\n        \"Adam\": \"pNInz6obpgDQGcFmaJgB\",\n        \"Antoni\": \"ErXwobaYiN019PkySvjV\",\n        \"Arnold\": \"VR6AewLTigWG4xSOukaG\",\n        \"Bella\": \"EXAVITQu4vr4xnSDxMaL\",\n        \"Domi\": \"AZnzlk1XvdvUeBnXmlld\",\n        \"Elli\": \"MF3mGyEYCl7XYWbV9V6O\",\n        \"Josh\": \"TxGEqnHWrfWFTfGW9XjX\",\n        \"Sam\": \"yoZ06aMxZJJ28mfd3POQ\"\n    }\n    \n    if st.button(\"ğŸµ Generate Speech\", key=\"tts_btn\", use_container_width=True):\n        if not text_input.strip():\n            st.error(\"Please enter some text first\")\n        else:\n            try:\n                with st.spinner(\"Generating speech...\"):\n                    audio_generator = elevenlabs_client.text_to_speech.convert(\n                        text=text_input,\n                        voice_id=voice_map[voice_id],\n                        model_id=\"eleven_multilingual_v2\",\n                        output_format=\"mp3_44100_128\"\n                    )\n                    \n                    audio_bytes = b''.join(audio_generator)\n                    \n                    st.audio(audio_bytes, format='audio/mpeg')\n                    \n                    st.download_button(\n                        label=\"ğŸ“¥ Download Audio\",\n                        data=audio_bytes,\n                        file_name=f\"tts_{int(time.time())}.mp3\",\n                        mime=\"audio/mpeg\",\n                        key=\"tts_download\"\n                    )\n                    \n                    st.success(\"âœ… Speech generated successfully!\")\n            except Exception as e:\n                st.error(f\"âŒ Failed to generate speech: {str(e)}\")\n\ndef render_speech_to_text():\n    st.markdown(\"### ğŸ¤ Speech to Text\")\n    \n    uploaded_audio = st.file_uploader(\n        \"Upload audio file\",\n        type=['wav', 'mp3', 'ogg', 'flac', 'm4a'],\n        key=\"stt_upload\"\n    )\n    \n    if uploaded_audio:\n        st.audio(uploaded_audio)\n    else:\n        st.markdown('<div style=\"height: 60px;\"></div>', unsafe_allow_html=True)\n    \n    if st.button(\"ğŸ“ Transcribe\", key=\"stt_btn\", use_container_width=True, disabled=uploaded_audio is None):\n        if uploaded_audio:\n            try:\n                with st.spinner(\"Transcribing audio...\"):\n                    file_extension = uploaded_audio.name.split('.')[-1].lower()\n                    \n                    with tempfile.NamedTemporaryFile(delete=False, suffix=f'.{file_extension}') as tmp_input:\n                        tmp_input.write(uploaded_audio.read())\n                        input_path = tmp_input.name\n                    \n                    wav_path = tempfile.mktemp(suffix='.wav')\n                    \n                    try:\n                        audio = AudioSegment.from_file(input_path, format=file_extension)\n                        audio.export(wav_path, format='wav')\n                    except Exception as e:\n                        os.unlink(input_path)\n                        raise Exception(f\"Failed to convert audio: {str(e)}\")\n                    \n                    recognizer = sr.Recognizer()\n                    \n                    try:\n                        with sr.AudioFile(wav_path) as source:\n                            audio_data = recognizer.record(source)\n                            text = recognizer.recognize_google(audio_data)\n                    finally:\n                        os.unlink(input_path)\n                        if os.path.exists(wav_path):\n                            os.unlink(wav_path)\n                    \n                    st.markdown(\"##### ğŸ“„ Transcription:\")\n                    st.text_area(\"Result\", value=text, height=150, key=\"stt_result\")\n                    \n                    st.success(\"âœ… Transcription completed!\")\n            except sr.UnknownValueError:\n                st.error(\"âŒ Could not understand audio. Please try with clearer audio.\")\n            except Exception as e:\n                st.error(f\"âŒ Transcription failed: {str(e)}\")\n\ndef render_text_translation(gemini_client):\n    st.markdown(\"### ğŸŒ Text Translation\")\n    \n    source_text = st.text_area(\n        \"Enter text to translate\",\n        placeholder=\"Type or paste text here...\",\n        height=120,\n        key=\"trans_input\"\n    )\n    \n    trans_col1, trans_col2 = st.columns(2)\n    \n    with trans_col1:\n        from_lang = st.selectbox(\n            \"From\",\n            [\"English\", \"Hindi\", \"Spanish\", \"French\", \"German\", \"Italian\", \"Portuguese\", \"Japanese\", \"Korean\", \"Chinese\"],\n            key=\"trans_from\"\n        )\n    \n    with trans_col2:\n        to_lang = st.selectbox(\n            \"To\",\n            [\"Hindi\", \"English\", \"Spanish\", \"French\", \"German\", \"Italian\", \"Portuguese\", \"Japanese\", \"Korean\", \"Chinese\"],\n            key=\"trans_to\"\n        )\n    \n    if st.button(\"ğŸ”„ Translate\", key=\"trans_btn\", use_container_width=True):\n        if not source_text.strip():\n            st.error(\"Please enter some text to translate\")\n        elif from_lang == to_lang:\n            st.error(\"Source and target languages must be different\")\n        else:\n            try:\n                with st.spinner(\"Translating...\"):\n                    prompt = f\"Translate the following text from {from_lang} to {to_lang}. Only provide the translation, no explanations:\\n\\n{source_text}\"\n                    \n                    response = gemini_client.models.generate_content(\n                        model=\"gemini-2.0-flash-exp\",\n                        contents=prompt\n                    )\n                    translated_text = response.text\n                    \n                    st.markdown(\"##### ğŸ“ Translation:\")\n                    st.text_area(\"Result\", value=translated_text, height=120, key=\"trans_result\")\n                    \n                    st.success(f\"âœ… Translated from {from_lang} to {to_lang}\")\n            except Exception as e:\n                st.error(f\"âŒ Translation failed: {str(e)}\")\n\ndef render_youtube_summarizer(youtube_summarizer):\n    youtube_url = st.text_input(\n        \"Enter YouTube URL\",\n        placeholder=\"https://www.youtube.com/watch?v=...\",\n        key=\"youtube_url\"\n    )\n    \n    word_count = st.slider(\n        \"Summary word count\",\n        min_value=50,\n        max_value=500,\n        value=200,\n        step=50,\n        key=\"summary_words\"\n    )\n    \n    st.markdown('<div class=\"feature-button-wrapper\">', unsafe_allow_html=True)\n    if st.button(\"ğŸ“ Summarize Video\", key=\"youtube_btn\", use_container_width=True):\n        if not youtube_url.strip():\n            st.error(\"Please enter a YouTube URL\")\n        else:\n            try:\n                with st.spinner(\"Processing... This may take a few minutes\"):\n                    status = st.empty()\n                    \n                    status.info(\"â¬‡ï¸ Downloading video...\")\n                    time.sleep(0.5)\n                    \n                    status.info(\"ğŸ¤ Transcribing audio...\")\n                    time.sleep(0.5)\n                    \n                    status.info(\"ğŸ“„ Generating summary...\")\n                    \n                    result = youtube_summarizer.process_youtube_video(youtube_url, word_count)\n                    \n                    if result:\n                        status.empty()\n                        \n                        st.markdown(f\"##### ğŸ“¹ Video: {result['title']}\")\n                        st.markdown(\"##### ğŸ“ Summary:\")\n                        st.text_area(\"Summary\", value=result['summary'], height=200, key=\"youtube_summary\")\n                        \n                        st.success(\"âœ… Summary generated successfully!\")\n                    else:\n                        st.error(\"âŒ Failed to process video. Please check the URL and try again.\")\n                        \n            except Exception as e:\n                st.error(f\"âŒ Processing failed: {str(e)}\")\n    st.markdown('</div>', unsafe_allow_html=True)\n\ndef render_word_to_story(story_generator):\n    words_input = st.text_input(\n        \"Enter words (comma-separated)\",\n        placeholder=\"adventure, forest, mystery, courage\",\n        key=\"story_words\"\n    )\n    \n    theme = st.text_input(\n        \"Story theme\",\n        placeholder=\"Fantasy adventure, Mystery thriller, etc.\",\n        key=\"story_theme\"\n    )\n    \n    story_col1, story_col2 = st.columns(2)\n    \n    with story_col1:\n        word_count = st.slider(\n            \"Story word count\",\n            min_value=100,\n            max_value=1000,\n            value=300,\n            step=50,\n            key=\"story_word_count\"\n        )\n    \n    with story_col2:\n        language = st.selectbox(\n            \"Language\",\n            [\"English\", \"Hindi\"],\n            key=\"story_language\"\n        )\n    \n    st.markdown('<div class=\"feature-button-wrapper\">', unsafe_allow_html=True)\n    if st.button(\"âœ¨ Generate Story\", key=\"story_btn\", use_container_width=True):\n        if not words_input.strip():\n            st.error(\"Please enter some words\")\n        elif not theme.strip():\n            st.error(\"Please enter a theme\")\n        else:\n            try:\n                with st.spinner(\"Creating your story... This may take a moment\"):\n                    words_list = [word.strip() for word in words_input.split(',')]\n                    \n                    status = st.empty()\n                    status.info(\"âœï¸ Writing story...\")\n                    time.sleep(0.5)\n                    \n                    status.info(\"ğŸ™ï¸ Generating emotional narration...\")\n                    \n                    result = story_generator.create_story_with_audio(\n                        words=words_list,\n                        theme=theme,\n                        word_count=word_count,\n                        language=language.lower()\n                    )\n                    \n                    if result and result['story']:\n                        status.empty()\n                        \n                        st.markdown(\"##### ğŸ“ Your Story:\")\n                        st.text_area(\"Story\", value=result['story'], height=300, key=\"generated_story\")\n                        \n                        if result['audio_bytes']:\n                            st.markdown(\"##### ğŸ§ Audio Narration:\")\n                            st.audio(result['audio_bytes'], format='audio/mpeg')\n                            \n                            st.download_button(\n                                label=\"ğŸ“¥ Download Audio\",\n                                data=result['audio_bytes'],\n                                file_name=f\"story_{int(time.time())}.mp3\",\n                                mime=\"audio/mpeg\",\n                                key=\"story_audio_download\"\n                            )\n                            st.success(\"âœ… Story created successfully with audio narration!\")\n                        else:\n                            st.warning(\"âš ï¸ Story created, but audio generation failed. This may be due to API limitations.\")\n                            st.info(\"ğŸ’¡ You can still read the story above. Audio narration will be available when API access is restored.\")\n                    else:\n                        st.error(\"âŒ Failed to generate story. Please try again.\")\n                        \n            except Exception as e:\n                st.error(f\"âŒ Story generation failed: {str(e)}\")\n    st.markdown('</div>', unsafe_allow_html=True)\n\ndef render_video_dubbing(video_processor, dubbing_service):\n    input_video_path = None\n    video_uploaded = False\n    source_lang = \"en\"\n    target_lang = \"hi\"\n    \n    uploaded_file = st.file_uploader(\n        \"Choose your video file\",\n        type=['mp4', 'avi', 'mov', 'mkv'],\n        help=\"Supported formats: MP4, AVI, MOV, MKV | Max size: 100MB\",\n        key=\"video_dubbing_uploader\"\n    )\n    \n    if uploaded_file is not None:\n        if not validate_video_file(uploaded_file):\n            st.error(\"âŒ Invalid video file or file too large (max 100MB)\")\n        else:\n            st.success(f\"âœ… Uploaded: {uploaded_file.name}\")\n            video_uploaded = True\n            \n            with tempfile.NamedTemporaryFile(delete=False, suffix='.mp4') as tmp_file:\n                tmp_file.write(uploaded_file.read())\n                input_video_path = tmp_file.name\n            \n            col1, col2 = st.columns([1, 1])\n            \n            with col1:\n                st.markdown(\"**ğŸŒ Source Language**\")\n                source_lang = st.selectbox(\n                    \"From\",\n                    [\"en\", \"hi\", \"es\", \"fr\", \"de\", \"it\", \"pt\", \"ja\", \"ko\", \"zh\"],\n                    format_func=lambda x: {\n                        \"en\": \"ğŸ‡¬ğŸ‡§ English\", \"hi\": \"ğŸ‡®ğŸ‡³ Hindi\", \"es\": \"ğŸ‡ªğŸ‡¸ Spanish\", \n                        \"fr\": \"ğŸ‡«ğŸ‡· French\", \"de\": \"ğŸ‡©ğŸ‡ª German\", \"it\": \"ğŸ‡®ğŸ‡¹ Italian\",\n                        \"pt\": \"ğŸ‡µğŸ‡¹ Portuguese\", \"ja\": \"ğŸ‡¯ğŸ‡µ Japanese\", \n                        \"ko\": \"ğŸ‡°ğŸ‡· Korean\", \"zh\": \"ğŸ‡¨ğŸ‡³ Chinese\"\n                    }.get(x, x),\n                    label_visibility=\"collapsed\",\n                    key=\"dubbing_source_lang\"\n                )\n            \n            with col2:\n                st.markdown(\"**ğŸ¯ Target Language**\")\n                target_lang = st.selectbox(\n                    \"To\",\n                    [\"hi\", \"en\", \"es\", \"fr\", \"de\", \"it\", \"pt\", \"ja\", \"ko\", \"zh\"],\n                    format_func=lambda x: {\n                        \"en\": \"ğŸ‡¬ğŸ‡§ English\", \"hi\": \"ğŸ‡®ğŸ‡³ Hindi\", \"es\": \"ğŸ‡ªğŸ‡¸ Spanish\", \n                        \"fr\": \"ğŸ‡«ğŸ‡· French\", \"de\": \"ğŸ‡©ğŸ‡ª German\", \"it\": \"ğŸ‡®ğŸ‡¹ Italian\",\n                        \"pt\": \"ğŸ‡µğŸ‡¹ Portuguese\", \"ja\": \"ğŸ‡¯ğŸ‡µ Japanese\", \n                        \"ko\": \"ğŸ‡°ğŸ‡· Korean\", \"zh\": \"ğŸ‡¨ğŸ‡³ Chinese\"\n                    }.get(x, x),\n                    label_visibility=\"collapsed\",\n                    key=\"dubbing_target_lang\"\n                )\n    else:\n        st.info(\"ğŸ“¤ Upload a video file to start dubbing\")\n    \n    st.markdown('<div class=\"feature-button-wrapper\">', unsafe_allow_html=True)\n    if st.button(\"ğŸ¬ Dub Video\", use_container_width=True, key=\"dub_video_btn\", disabled=not video_uploaded):\n        if video_uploaded and input_video_path:\n            process_video_with_elevenlabs(\n                input_video_path, \n                source_lang, \n                target_lang,\n                video_processor,\n                dubbing_service\n            )\n    st.markdown('</div>', unsafe_allow_html=True)\n\ndef render_article_to_podcast(article_podcast):\n    article_text = st.text_area(\n        \"Enter news article or text\",\n        placeholder=\"Paste your news article or any text here...\",\n        height=200,\n        key=\"article_input\"\n    )\n    \n    script_word_count = st.slider(\n        \"Podcast script length (words)\",\n        min_value=100,\n        max_value=500,\n        value=300,\n        step=50,\n        key=\"podcast_script_length\"\n    )\n    \n    st.markdown('<div class=\"feature-button-wrapper\">', unsafe_allow_html=True)\n    if st.button(\"ğŸ§ Generate Podcast\", key=\"podcast_btn\", use_container_width=True):\n        if not article_text.strip():\n            st.error(\"Please enter an article or text\")\n        else:\n            try:\n                with st.spinner(\"Creating your podcast... This may take a moment\"):\n                    status = st.empty()\n                    \n                    def progress_callback(message, percent):\n                        status.info(f\"{message}\")\n                    \n                    audio_bytes = article_podcast.create_podcast_from_article(\n                        article_text=article_text,\n                        script_word_count=script_word_count,\n                        progress_callback=progress_callback\n                    )\n                    \n                    if audio_bytes:\n                        status.empty()\n                        \n                        st.markdown(\"##### ğŸ§ Your Podcast:\")\n                        st.audio(audio_bytes, format='audio/mpeg')\n                        \n                        st.download_button(\n                            label=\"ğŸ“¥ Download Podcast\",\n                            data=audio_bytes,\n                            file_name=f\"podcast_{int(time.time())}.mp3\",\n                            mime=\"audio/mpeg\",\n                            key=\"podcast_download\"\n                        )\n                        \n                        st.success(\"âœ… Podcast created successfully!\")\n                        st.info(\"ğŸ’¡ This podcast features a conversational format with a Host and an Expert discussing your article.\")\n                    else:\n                        st.error(\"âŒ Failed to generate podcast. Please try again.\")\n                        \n            except Exception as e:\n                st.error(f\"âŒ Podcast generation failed: {str(e)}\")\n    st.markdown('</div>', unsafe_allow_html=True)\n\n@st.cache_resource\ndef initialize_services():\n    try:\n        elevenlabs_api_key = os.environ.get('ELEVENLABS_API_KEY')\n        gemini_api_key = os.environ.get('GEMINI_API_KEY')\n        \n        if not elevenlabs_api_key:\n            st.error(\"ğŸ”‘ API key environment variable not set\")\n            return None, None, None, None, None, None, None\n        \n        if not gemini_api_key:\n            st.error(\"ğŸ”‘ GEMINI_API_KEY environment variable not set\")\n            return None, None, None, None, None, None, None\n        \n        video_processor = VideoProcessor()\n        dubbing_service = ElevenLabsDubbing(api_key=elevenlabs_api_key)\n        elevenlabs_client = ElevenLabs(api_key=elevenlabs_api_key)\n        gemini_client = genai.Client(api_key=gemini_api_key)\n        youtube_summarizer = YouTubeSummarizer(gemini_api_key=gemini_api_key)\n        story_generator = StoryGenerator(gemini_api_key=gemini_api_key, elevenlabs_api_key=elevenlabs_api_key)\n        article_podcast = ArticleToPodcast(gemini_api_key=gemini_api_key, elevenlabs_api_key=elevenlabs_api_key)\n        \n        return video_processor, dubbing_service, elevenlabs_client, gemini_client, youtube_summarizer, story_generator, article_podcast\n    except Exception as e:\n        st.error(f\"âŒ Failed to initialize services: {str(e)}\")\n        return None, None, None, None, None, None, None\n\ndef main():\n    st.markdown(\"\"\"\n        <div class=\"header\">\n            <span class=\"logo\">ğŸŒ Anuvaad AI</span>\n        </div>\n    \"\"\", unsafe_allow_html=True)\n    \n    st.markdown(\"\"\"\n        <div class=\"hero\">\n            <h1>Transform Content<br>Across Languages</h1>\n            <p>AI-powered video dubbing, text-to-speech, speech-to-text, and translation</p>\n            <span class=\"badge\">âœ¨ Powered by Advanced AI Technology</span>\n        </div>\n    \"\"\", unsafe_allow_html=True)\n    \n    services = initialize_services()\n    if None in services:\n        st.markdown('<div class=\"content-section\">', unsafe_allow_html=True)\n        st.error(\"âš ï¸ Failed to initialize application services. Please check API keys.\")\n        st.markdown('</div>', unsafe_allow_html=True)\n        return\n    \n    video_processor, dubbing_service, elevenlabs_client, gemini_client, youtube_summarizer, story_generator, article_podcast = services\n    \n    st.markdown('<div class=\"content-section\">', unsafe_allow_html=True)\n    \n    st.markdown('<h2 style=\"text-align: center; margin: 1rem 0 2rem 0;\">Main Features</h2>', unsafe_allow_html=True)\n    \n    row1_col1, row1_col2 = st.columns(2, gap=\"large\")\n    \n    with row1_col1:\n        with st.container(border=True):\n            st.markdown('<h3 style=\"text-align: center; margin: 0 0 1rem 0;\">ğŸ¬ Video Dubbing</h3>', unsafe_allow_html=True)\n            render_video_dubbing(video_processor, dubbing_service)\n    \n    with row1_col2:\n        with st.container(border=True):\n            st.markdown('<h3 style=\"text-align: center; margin: 0 0 1rem 0;\">ğŸ“º YouTube Summarizer</h3>', unsafe_allow_html=True)\n            render_youtube_summarizer(youtube_summarizer)\n    \n    row2_col1, row2_col2 = st.columns(2, gap=\"large\")\n    \n    with row2_col1:\n        with st.container(border=True):\n            st.markdown('<h3 style=\"text-align: center; margin: 0 0 1rem 0;\">ğŸ“– Word to Story</h3>', unsafe_allow_html=True)\n            render_word_to_story(story_generator)\n    \n    with row2_col2:\n        with st.container(border=True):\n            st.markdown('<h3 style=\"text-align: center; margin: 0 0 1rem 0;\">ğŸ™ï¸ Article to Podcast</h3>', unsafe_allow_html=True)\n            render_article_to_podcast(article_podcast)\n    \n    st.markdown(\"---\")\n    st.markdown('<h2 style=\"text-align: center; margin: 2rem 0;\">Additional Tools</h2>', unsafe_allow_html=True)\n    \n    btn_col1, btn_col2, btn_col3 = st.columns(3, gap=\"medium\")\n    \n    with btn_col1:\n        if st.button(\"ğŸ—£ï¸ Text to Speech\", key=\"tts_feature_btn\", use_container_width=True):\n            st.session_state.active_feature = \"tts\"\n    \n    with btn_col2:\n        if st.button(\"ğŸ¤ Speech to Text\", key=\"stt_feature_btn\", use_container_width=True):\n            st.session_state.active_feature = \"stt\"\n    \n    with btn_col3:\n        if st.button(\"ğŸŒ Text Translation\", key=\"trans_feature_btn\", use_container_width=True):\n            st.session_state.active_feature = \"trans\"\n    \n    if \"active_feature\" not in st.session_state:\n        st.session_state.active_feature = None\n    \n    if st.session_state.active_feature == \"tts\":\n        render_text_to_speech(elevenlabs_client)\n    elif st.session_state.active_feature == \"stt\":\n        render_speech_to_text()\n    elif st.session_state.active_feature == \"trans\":\n        render_text_translation(gemini_client)\n    \n    st.markdown('</div>', unsafe_allow_html=True)\n    \n    st.markdown(\"\"\"\n        <div class=\"footer\">\n            <p>Â© 2025 Anuvaad AI - Advanced AI Technology</p>\n            <p style=\"font-size: 0.85rem; margin-top: 0.5rem;\">Breaking language barriers with artificial intelligence</p>\n        </div>\n    \"\"\", unsafe_allow_html=True)\n\ndef process_video_with_elevenlabs(input_path, source_lang, target_lang, video_proc, dub_svc):\n    st.markdown('<div class=\"card\">', unsafe_allow_html=True)\n    st.markdown('<div class=\"card-title\">ğŸ”„ Processing</div>', unsafe_allow_html=True)\n    \n    progress_bar = st.progress(0)\n    status_text = st.empty()\n    \n    try:\n        status_text.markdown(\"**ğŸ“¤ Uploading video...**\")\n        progress_bar.progress(10)\n        time.sleep(0.5)\n        \n        status_text.markdown(\"**ğŸ—£ï¸ Transcribing speech...**\")\n        progress_bar.progress(25)\n        time.sleep(0.5)\n        \n        status_text.markdown(f\"**ğŸŒ Translating {source_lang.upper()} â†’ {target_lang.upper()}...**\")\n        progress_bar.progress(40)\n        \n        status_text.markdown(\"**ğŸ™ï¸ Generating AI voice...**\")\n        progress_bar.progress(50)\n        \n        def progress_callback(message, percent):\n            status_text.markdown(f\"**{message}**\")\n            progress_bar.progress(percent)\n        \n        dubbed_video_path = dub_svc.dub_video_complete(\n            input_path,\n            source_lang,\n            target_lang,\n            progress_callback=progress_callback\n        )\n        \n        if not dubbed_video_path:\n            st.error(\"âŒ Failed to dub video. Please check your API configuration.\")\n            st.markdown('</div>', unsafe_allow_html=True)\n            return\n        \n        progress_bar.progress(100)\n        status_text.markdown(\"**âœ… Dubbing completed!**\")\n        time.sleep(0.5)\n        \n        st.balloons()\n        st.success(\"ğŸ‰ Your video is ready!\")\n        \n        st.markdown(\"---\")\n        \n        col1, col2 = st.columns(2)\n        \n        with col1:\n            st.markdown(\"##### ğŸ“¹ Original\")\n            st.video(input_path)\n        \n        with col2:\n            st.markdown(\"##### ğŸ¬ Dubbed\")\n            st.video(dubbed_video_path)\n        \n        st.markdown(\"---\")\n        \n        with open(dubbed_video_path, 'rb') as f:\n            st.download_button(\n                label=\"ğŸ“¥ Download Dubbed Video\",\n                data=f.read(),\n                file_name=f\"anuvaad_ai_{source_lang}_to_{target_lang}_{int(time.time())}.mp4\",\n                mime=\"video/mp4\",\n                type=\"primary\",\n                use_container_width=True\n            )\n        \n        st.markdown('</div>', unsafe_allow_html=True)\n            \n    except Exception as e:\n        st.error(f\"âŒ Processing failed: {str(e)}\")\n        progress_bar.progress(0)\n        status_text.markdown(\"**âŒ Processing failed**\")\n        st.markdown('</div>', unsafe_allow_html=True)\n\nif __name__ == \"__main__\":\n    main()\n","size_bytes":36848},"translation_service.py":{"content":"import os\nfrom google import genai\nfrom google.genai import types\nfrom typing import Optional\n\nclass TranslationService:\n    \"\"\"Handles text translation using Gemini AI\"\"\"\n    \n    def __init__(self, api_key: str):\n        \"\"\"Initialize translation service with Gemini API\"\"\"\n        self.client = genai.Client(api_key=api_key)\n        \n        # Language mappings\n        self.language_names = {\n            'en': 'English',\n            'hi': 'Hindi'\n        }\n    \n    def translate_text(self, text: str, source_lang: str, target_lang: str) -> Optional[str]:\n        \"\"\"\n        Translate text from source language to target language\n        \"\"\"\n        try:\n            # Skip translation if source and target are the same\n            if source_lang == target_lang:\n                return text\n            \n            source_name = self.language_names.get(source_lang, source_lang)\n            target_name = self.language_names.get(target_lang, target_lang)\n            \n            # Create translation prompt\n            prompt = f\"\"\"\n            You are a professional translator specializing in video dubbing translation.\n            \n            Translate the following text from {source_name} to {target_name}.\n            \n            Guidelines:\n            - Maintain the natural flow and timing suitable for dubbing\n            - Preserve emotional tone and context\n            - Keep sentence structure appropriate for spoken dialogue\n            - Ensure cultural appropriateness\n            - Maintain roughly similar length for timing synchronization\n            \n            Text to translate:\n            {text}\n            \n            Provide only the translation without any additional comments or explanations.\n            \"\"\"\n            \n            response = self.client.models.generate_content(\n                model=\"gemini-2.5-flash\",\n                contents=prompt\n            )\n            \n            if response and response.text:\n                return response.text.strip()\n            else:\n                return None\n                \n        except Exception as e:\n            print(f\"Translation error: {e}\")\n            return None\n    \n    def translate_with_context(self, text: str, source_lang: str, target_lang: str, \n                              context: str = \"\") -> Optional[str]:\n        \"\"\"\n        Translate text with additional context for better accuracy\n        \"\"\"\n        try:\n            source_name = self.language_names.get(source_lang, source_lang)\n            target_name = self.language_names.get(target_lang, target_lang)\n            \n            context_info = f\"\\nContext: {context}\" if context else \"\"\n            \n            prompt = f\"\"\"\n            You are a professional translator for video dubbing.\n            \n            Translate from {source_name} to {target_name}.{context_info}\n            \n            Requirements:\n            - Natural dubbing-appropriate translation\n            - Preserve emotional tone and meaning\n            - Maintain similar timing/length\n            - Cultural sensitivity\n            \n            Text: {text}\n            \n            Translation:\n            \"\"\"\n            \n            response = self.client.models.generate_content(\n                model=\"gemini-2.5-flash\",\n                contents=prompt\n            )\n            \n            return response.text.strip() if response and response.text else None\n            \n        except Exception as e:\n            print(f\"Contextual translation error: {e}\")\n            return None\n    \n    def translate_segments(self, segments: list, source_lang: str, target_lang: str) -> list:\n        \"\"\"\n        Translate multiple text segments while preserving timing information\n        \"\"\"\n        translated_segments = []\n        \n        for segment in segments:\n            original_text = segment.get('text', '')\n            \n            if not original_text.strip():\n                # Keep empty segments\n                translated_segments.append(segment.copy())\n                continue\n            \n            translated_text = self.translate_text(original_text, source_lang, target_lang)\n            \n            if translated_text:\n                translated_segment = segment.copy()\n                translated_segment['text'] = translated_text\n                translated_segment['original_text'] = original_text\n                translated_segments.append(translated_segment)\n            else:\n                # Fallback to original text if translation fails\n                fallback_segment = segment.copy()\n                fallback_segment['translation_failed'] = True\n                translated_segments.append(fallback_segment)\n        \n        return translated_segments\n    \n    def improve_translation_for_dubbing(self, text: str, target_lang: str) -> Optional[str]:\n        \"\"\"\n        Improve translation specifically for dubbing requirements\n        \"\"\"\n        try:\n            target_name = self.language_names.get(target_lang, target_lang)\n            \n            prompt = f\"\"\"\n            You are a dubbing specialist. Improve this {target_name} translation for video dubbing:\n            \n            \"{text}\"\n            \n            Make it more natural for speaking while maintaining:\n            - Original meaning and emotion\n            - Similar length/timing\n            - Natural speech patterns\n            - Cultural appropriateness\n            \n            Provide only the improved translation:\n            \"\"\"\n            \n            response = self.client.models.generate_content(\n                model=\"gemini-2.5-flash\",\n                contents=prompt\n            )\n            \n            return response.text.strip() if response and response.text else text\n            \n        except Exception as e:\n            print(f\"Translation improvement error: {e}\")\n            return text\n    \n    def detect_language(self, text: str) -> Optional[str]:\n        \"\"\"\n        Detect the language of input text\n        \"\"\"\n        try:\n            prompt = f\"\"\"\n            Detect the language of this text and respond with just the language code (en for English, hi for Hindi):\n            \n            \"{text[:200]}\"  # First 200 chars\n            \n            Language code:\n            \"\"\"\n            \n            response = self.client.models.generate_content(\n                model=\"gemini-2.5-flash\",\n                contents=prompt\n            )\n            \n            if response and response.text:\n                detected = response.text.strip().lower()\n                return detected if detected in ['en', 'hi'] else None\n            \n            return None\n            \n        except Exception as e:\n            print(f\"Language detection error: {e}\")\n            return None\n    \n    def get_translation_quality_score(self, original: str, translation: str) -> float:\n        \"\"\"\n        Get a quality score for the translation (0-1)\n        \"\"\"\n        try:\n            prompt = f\"\"\"\n            Rate the translation quality on a scale of 0-1 (where 1 is perfect).\n            \n            Original: \"{original}\"\n            Translation: \"{translation}\"\n            \n            Consider accuracy, naturalness, and appropriateness for dubbing.\n            Respond with only a decimal number between 0 and 1:\n            \"\"\"\n            \n            response = self.client.models.generate_content(\n                model=\"gemini-2.5-flash\",\n                contents=prompt\n            )\n            \n            if response and response.text:\n                try:\n                    score = float(response.text.strip())\n                    return max(0.0, min(1.0, score))  # Clamp between 0 and 1\n                except ValueError:\n                    return 0.7  # Default score\n            \n            return 0.7\n            \n        except Exception as e:\n            print(f\"Quality scoring error: {e}\")\n            return 0.7\n","size_bytes":7956},"video_processor.py":{"content":"import os\nimport tempfile\ntry:\n    from moviepy import VideoFileClip, CompositeVideoClip, AudioFileClip\nexcept ImportError:\n    from moviepy.editor import VideoFileClip, CompositeVideoClip, AudioFileClip\nimport ffmpeg\nfrom typing import Optional, Dict\n\nclass VideoProcessor:\n    \"\"\"Handles video processing operations\"\"\"\n    \n    def __init__(self):\n        \"\"\"Initialize video processor\"\"\"\n        pass\n    \n    def get_video_info(self, video_path: str) -> Optional[Dict]:\n        \"\"\"\n        Get basic information about the video file\n        \"\"\"\n        try:\n            with VideoFileClip(video_path) as video:\n                return {\n                    'duration': video.duration,\n                    'fps': video.fps,\n                    'width': video.w,\n                    'height': video.h,\n                    'has_audio': video.audio is not None\n                }\n        except Exception as e:\n            print(f\"Error getting video info: {e}\")\n            return None\n    \n    def extract_audio(self, video_path: str) -> Optional[str]:\n        \"\"\"\n        Extract audio track from video file\n        \"\"\"\n        try:\n            output_path = tempfile.mktemp(suffix='.wav')\n            \n            # Use moviepy to extract audio\n            with VideoFileClip(video_path) as video:\n                if video.audio is None:\n                    print(\"No audio track found in video\")\n                    return None\n                \n                # Extract audio and save as WAV\n                video.audio.write_audiofile(\n                    output_path,\n                    logger=None  # Suppress output\n                )\n            \n            return output_path\n            \n        except Exception as e:\n            print(f\"Error extracting audio: {e}\")\n            return None\n    \n    def create_dubbed_video(self, original_video_path: str, dubbed_audio_path: str) -> Optional[str]:\n        \"\"\"\n        Create final dubbed video by combining original video with new audio\n        \"\"\"\n        try:\n            output_path = tempfile.mktemp(suffix='.mp4')\n            \n            # Load original video (without audio)\n            with VideoFileClip(original_video_path) as original_video:\n                # Load dubbed audio\n                with AudioFileClip(dubbed_audio_path) as dubbed_audio:\n                    \n                    # Get video duration\n                    video_duration = original_video.duration\n                    audio_duration = dubbed_audio.duration\n                    \n                    # Ensure audio matches video duration exactly\n                    if abs(audio_duration - video_duration) > 0.1:  # 100ms tolerance\n                        # Adjust audio duration to match video\n                        if audio_duration > video_duration:\n                            dubbed_audio = dubbed_audio.subclip(0, video_duration)\n                        else:\n                            # Extend audio with silence if needed\n                            from moviepy.audio.AudioClip import AudioClip\n                            silence_duration = video_duration - audio_duration\n                            silence = AudioClip(\n                                make_frame=lambda t: 0,\n                                duration=silence_duration\n                            ).set_fps(dubbed_audio.fps)\n                            dubbed_audio = dubbed_audio.concatenate_audioclips([dubbed_audio, silence])\n                    \n                    # Remove original audio and add dubbed audio\n                    final_video = original_video.without_audio().set_audio(dubbed_audio)\n                    \n                    # Write final video\n                    final_video.write_videofile(\n                        output_path,\n                        codec='libx264',\n                        audio_codec='aac',\n                        temp_audiofile='temp-audio.m4a',\n                        remove_temp=True,\n                        logger=None\n                    )\n            \n            return output_path\n            \n        except Exception as e:\n            print(f\"Error creating dubbed video: {e}\")\n            return None\n    \n    def create_side_by_side_comparison(self, original_path: str, dubbed_path: str) -> Optional[str]:\n        \"\"\"\n        Create a side-by-side comparison video\n        \"\"\"\n        try:\n            output_path = tempfile.mktemp(suffix='.mp4')\n            \n            with VideoFileClip(original_path) as original, VideoFileClip(dubbed_path) as dubbed:\n                # Resize videos to half width\n                original_resized = original.resize(width=original.w // 2)\n                dubbed_resized = dubbed.resize(width=dubbed.w // 2)\n                \n                # Position videos side by side\n                original_positioned = original_resized.set_position(('left', 'center'))\n                dubbed_positioned = dubbed_resized.set_position(('right', 'center'))\n                \n                # Composite videos\n                comparison = CompositeVideoClip([\n                    original_positioned,\n                    dubbed_positioned\n                ], size=(original.w, original.h))\n                \n                # Write comparison video\n                comparison.write_videofile(\n                    output_path,\n                    codec='libx264',\n                    audio_codec='aac',\n                    logger=None\n                )\n            \n            return output_path\n            \n        except Exception as e:\n            print(f\"Error creating comparison video: {e}\")\n            return None\n    \n    def extract_video_frames_at_timestamps(self, video_path: str, timestamps: list) -> list:\n        \"\"\"\n        Extract video frames at specific timestamps for analysis\n        \"\"\"\n        try:\n            frames = []\n            \n            with VideoFileClip(video_path) as video:\n                for timestamp in timestamps:\n                    try:\n                        # Extract frame at timestamp\n                        frame = video.get_frame(timestamp)\n                        frames.append({\n                            'timestamp': timestamp,\n                            'frame': frame\n                        })\n                    except:\n                        continue\n            \n            return frames\n            \n        except Exception as e:\n            print(f\"Error extracting frames: {e}\")\n            return []\n    \n    def optimize_video_for_web(self, video_path: str) -> Optional[str]:\n        \"\"\"\n        Optimize video for web playback\n        \"\"\"\n        try:\n            output_path = tempfile.mktemp(suffix='.mp4')\n            \n            # Use ffmpeg for optimization\n            (\n                ffmpeg\n                .input(video_path)\n                .output(\n                    output_path,\n                    vcodec='libx264',\n                    acodec='aac',\n                    preset='fast',\n                    crf=23,\n                    movflags='faststart'  # Enable streaming\n                )\n                .overwrite_output()\n                .run(quiet=True)\n            )\n            \n            return output_path\n            \n        except Exception as e:\n            print(f\"Error optimizing video: {e}\")\n            return video_path\n    \n    def validate_video_format(self, video_path: str) -> bool:\n        \"\"\"\n        Validate if video format is supported\n        \"\"\"\n        try:\n            with VideoFileClip(video_path) as video:\n                # Basic validation - check if video can be loaded\n                return video.duration > 0 and video.fps > 0\n        except:\n            return False\n    \n    def get_video_metadata(self, video_path: str) -> Dict:\n        \"\"\"\n        Get comprehensive video metadata\n        \"\"\"\n        try:\n            probe = ffmpeg.probe(video_path)\n            video_stream = next((stream for stream in probe['streams'] if stream['codec_type'] == 'video'), None)\n            audio_stream = next((stream for stream in probe['streams'] if stream['codec_type'] == 'audio'), None)\n            \n            metadata = {\n                'format': probe['format']['format_name'],\n                'duration': float(probe['format']['duration']),\n                'size': int(probe['format']['size']),\n                'bitrate': int(probe['format']['bit_rate'])\n            }\n            \n            if video_stream:\n                metadata.update({\n                    'video_codec': video_stream['codec_name'],\n                    'width': int(video_stream['width']),\n                    'height': int(video_stream['height']),\n                    'fps': eval(video_stream['r_frame_rate'])\n                })\n            \n            if audio_stream:\n                metadata.update({\n                    'audio_codec': audio_stream['codec_name'],\n                    'sample_rate': int(audio_stream['sample_rate']),\n                    'channels': int(audio_stream['channels'])\n                })\n            \n            return metadata\n            \n        except Exception as e:\n            print(f\"Error getting metadata: {e}\")\n            return {}\n","size_bytes":9199},"pyproject.toml":{"content":"[project]\nname = \"repl-nix-workspace\"\nversion = \"0.1.0\"\ndescription = \"Add your description here\"\nrequires-python = \">=3.11\"\ndependencies = [\n    \"elevenlabs>=2.16.0\",\n    \"ffmpeg>=1.4\",\n    \"ffmpeg-python>=0.2.0\",\n    \"flask-sqlalchemy>=3.1.1\",\n    \"flask>=3.1.2\",\n    \"flask-cors>=6.0.1\",\n    \"google-genai>=1.41.0\",\n    \"librosa>=0.11.0\",\n    \"moviepy>=2.2.1\",\n    \"numpy>=2.3.3\",\n    \"pydub>=0.25.1\",\n    \"requests>=2.32.5\",\n    \"sift-stack-py>=0.9.1\",\n    \"soundfile>=0.13.1\",\n    \"speechrecognition>=3.14.3\",\n    \"streamlit>=1.50.0\",\n    \"whisper>=1.1.10\",\n    \"yt-dlp>=2025.9.26\",\n    \"flask-bcrypt>=1.0.1\",\n    \"flask-jwt-extended>=4.7.1\",\n    \"psycopg2-binary>=2.9.11\",\n    \"werkzeug>=3.1.3\",\n]\n","size_bytes":704},"audio_processor.py":{"content":"import os\nimport tempfile\nimport speech_recognition as sr\nimport numpy as np\nfrom pydub import AudioSegment\nfrom pydub.silence import split_on_silence, detect_nonsilent\ntry:\n    import librosa\n    import soundfile as sf\n    LIBROSA_AVAILABLE = True\nexcept ImportError:\n    LIBROSA_AVAILABLE = False\nfrom typing import Tuple, Optional, List, Dict\n\nclass AudioProcessor:\n    \"\"\"Handles audio processing, separation, and speech recognition\"\"\"\n    \n    def __init__(self):\n        \"\"\"Initialize audio processor with speech recognizer\"\"\"\n        self.recognizer = sr.Recognizer()\n        \n    def separate_audio_components(self, audio_path: str, preserve_background: bool = True) -> Tuple[str, Optional[str], List]:\n        \"\"\"\n        Separate speech from background audio using advanced techniques\n        Returns: (speech_audio_path, background_audio_path, timestamps)\n        \"\"\"\n        try:\n            # Load audio with pydub\n            audio = AudioSegment.from_file(audio_path)\n            \n            # Convert to mono for processing\n            mono_audio = audio.set_channels(1)\n            \n            # Detect speech segments using silence detection\n            speech_segments = detect_nonsilent(\n                mono_audio,\n                min_silence_len=100,  # 100ms minimum silence\n                silence_thresh=mono_audio.dBFS - 16,  # 16dB below average\n                seek_step=10\n            )\n            \n            # Create speech-only audio\n            speech_audio = AudioSegment.empty()\n            silence_audio = AudioSegment.empty()\n            timestamps = []\n            \n            last_end = 0\n            for start_ms, end_ms in speech_segments:\n                # Add silence before speech if exists\n                if start_ms > last_end:\n                    silence_duration = start_ms - last_end\n                    silence_audio += AudioSegment.silent(duration=silence_duration)\n                    speech_audio += AudioSegment.silent(duration=silence_duration)\n                \n                # Add speech segment\n                speech_segment = mono_audio[start_ms:end_ms]\n                speech_audio += speech_segment\n                \n                # Store timestamp info\n                timestamps.append({\n                    'start': start_ms / 1000.0,\n                    'end': end_ms / 1000.0,\n                    'duration': (end_ms - start_ms) / 1000.0\n                })\n                \n                last_end = end_ms\n            \n            # Add final silence if needed\n            if last_end < len(mono_audio):\n                silence_duration = len(mono_audio) - last_end\n                speech_audio += AudioSegment.silent(duration=silence_duration)\n            \n            # Save speech audio\n            speech_path = tempfile.mktemp(suffix='.wav')\n            speech_audio.export(speech_path, format='wav')\n            \n            background_path = None\n            if preserve_background:\n                # Create background audio by inverting speech segments\n                background_audio = mono_audio.overlay(speech_audio.invert_phase())\n                background_path = tempfile.mktemp(suffix='.wav')\n                background_audio.export(background_path, format='wav')\n            \n            return speech_path, background_path, timestamps\n            \n        except Exception as e:\n            print(f\"Error in audio separation: {e}\")\n            # Fallback: return original audio as speech\n            return audio_path, None, []\n    \n    def speech_to_text(self, audio_path: str, language: str) -> Optional[Dict]:\n        \"\"\"\n        Convert speech to text with detailed timing information using Google Speech Recognition\n        \"\"\"\n        try:\n            # Set language code\n            lang_code = \"en-US\" if language == \"en\" else \"hi-IN\"\n            \n            # Load audio file\n            audio = AudioSegment.from_file(audio_path)\n            \n            # Convert to WAV format if needed\n            wav_path = tempfile.mktemp(suffix='.wav')\n            audio.export(wav_path, format='wav')\n            \n            # Recognize speech using Google Speech Recognition\n            with sr.AudioFile(wav_path) as source:\n                audio_data = self.recognizer.record(source)\n                \n                try:\n                    # Use Google Speech Recognition (free tier)\n                    text = self.recognizer.recognize_google(audio_data, language=lang_code)\n                    \n                    # Create basic segments (Google API doesn't provide word-level timestamps in free tier)\n                    duration = len(audio) / 1000.0  # Convert to seconds\n                    segments = [{\n                        'start': 0.0,\n                        'end': duration,\n                        'text': text,\n                        'words': []\n                    }]\n                    \n                    # Clean up temp file\n                    if os.path.exists(wav_path):\n                        os.unlink(wav_path)\n                    \n                    return {\n                        'text': text,\n                        'language': language,\n                        'segments': segments\n                    }\n                    \n                except sr.UnknownValueError:\n                    print(\"Google Speech Recognition could not understand audio\")\n                    return None\n                except sr.RequestError as e:\n                    print(f\"Could not request results from Google Speech Recognition; {e}\")\n                    return None\n            \n        except Exception as e:\n            print(f\"Error in speech recognition: {e}\")\n            return None\n    \n    def enhance_audio_quality(self, audio_path: str) -> str:\n        \"\"\"\n        Enhance audio quality through noise reduction and normalization\n        \"\"\"\n        try:\n            # Load audio\n            audio = AudioSegment.from_file(audio_path)\n            \n            # Normalize audio\n            normalized = audio.normalize()\n            \n            # Apply basic noise reduction by removing very quiet parts\n            # This is a simple approach - more sophisticated methods would use spectral subtraction\n            threshold = normalized.dBFS - 20\n            enhanced = normalized.compress_dynamic_range(\n                threshold=threshold,\n                ratio=4.0,\n                attack=5.0,\n                release=50.0\n            )\n            \n            # Export enhanced audio\n            enhanced_path = tempfile.mktemp(suffix='.wav')\n            enhanced.export(enhanced_path, format='wav')\n            \n            return enhanced_path\n            \n        except Exception as e:\n            print(f\"Error in audio enhancement: {e}\")\n            return audio_path\n    \n    def mix_audio_tracks(self, speech_path: str, background_path: str, \n                        speech_volume: float = 1.0, background_volume: float = 0.3) -> str:\n        \"\"\"\n        Mix speech and background audio with specified volume levels\n        \"\"\"\n        try:\n            # Load audio files\n            speech = AudioSegment.from_file(speech_path)\n            background = AudioSegment.from_file(background_path)\n            \n            # Adjust volumes\n            speech = speech + (20 * np.log10(speech_volume))  # Convert to dB\n            background = background + (20 * np.log10(background_volume))\n            \n            # Ensure both tracks have the same length\n            max_length = max(len(speech), len(background))\n            \n            if len(speech) < max_length:\n                speech = speech + AudioSegment.silent(duration=max_length - len(speech))\n            \n            if len(background) < max_length:\n                background = background + AudioSegment.silent(duration=max_length - len(background))\n            \n            # Mix the tracks\n            mixed = speech.overlay(background)\n            \n            # Export mixed audio\n            mixed_path = tempfile.mktemp(suffix='.wav')\n            mixed.export(mixed_path, format='wav')\n            \n            return mixed_path\n            \n        except Exception as e:\n            print(f\"Error in audio mixing: {e}\")\n            return speech_path\n    \n    def analyze_audio_gaps(self, audio_path: str) -> List[Dict]:\n        \"\"\"\n        Analyze gaps and pauses in audio for better synchronization\n        \"\"\"\n        try:\n            # Load audio\n            audio = AudioSegment.from_file(audio_path)\n            \n            # Detect silent segments\n            silence_segments = []\n            silent_parts = split_on_silence(\n                audio,\n                min_silence_len=200,  # 200ms minimum silence\n                silence_thresh=audio.dBFS - 14,\n                keep_silence=100  # Keep 100ms of silence\n            )\n            \n            current_time = 0\n            for i, segment in enumerate(silent_parts):\n                if i > 0:\n                    # Calculate silence duration between segments\n                    silence_start = current_time\n                    silence_end = current_time + len(segment)\n                    \n                    silence_segments.append({\n                        'start': silence_start / 1000.0,\n                        'end': silence_end / 1000.0,\n                        'duration': len(segment) / 1000.0\n                    })\n                \n                current_time += len(segment)\n            \n            return silence_segments\n            \n        except Exception as e:\n            print(f\"Error in gap analysis: {e}\")\n            return []\n    \n    def adjust_speech_timing(self, audio_path: str, target_duration: float, \n                           preserve_pitch: bool = True) -> str:\n        \"\"\"\n        Adjust speech timing to match target duration while preserving quality\n        \"\"\"\n        try:\n            if LIBROSA_AVAILABLE:\n                # Load audio with librosa for better time-stretching\n                y, sr = librosa.load(audio_path)\n                \n                # Calculate current duration and stretch ratio\n                current_duration = len(y) / sr\n                stretch_ratio = target_duration / current_duration\n                \n                # Time-stretch the audio\n                if preserve_pitch:\n                    # Use phase vocoder for pitch preservation\n                    y_stretched = librosa.effects.time_stretch(y, rate=1/stretch_ratio)\n                else:\n                    # Simple resampling (changes pitch)\n                    y_stretched = librosa.resample(y, orig_sr=sr, target_sr=int(sr*stretch_ratio))\n                \n                # Save stretched audio\n                stretched_path = tempfile.mktemp(suffix='.wav')\n                sf.write(stretched_path, y_stretched, sr)\n                \n                return stretched_path\n            else:\n                # Fallback: use pydub for basic speed adjustment\n                audio = AudioSegment.from_file(audio_path)\n                current_duration = len(audio) / 1000.0\n                speed_ratio = current_duration / target_duration\n                \n                # Adjust frame rate for speed change\n                new_frame_rate = int(audio.frame_rate * speed_ratio)\n                adjusted = audio._spawn(audio.raw_data, overrides={\"frame_rate\": new_frame_rate})\n                adjusted = adjusted.set_frame_rate(audio.frame_rate)\n                \n                stretched_path = tempfile.mktemp(suffix='.wav')\n                adjusted.export(stretched_path, format='wav')\n                return stretched_path\n            \n        except Exception as e:\n            print(f\"Error in timing adjustment: {e}\")\n            return audio_path\n","size_bytes":11791},"utils.py":{"content":"import os\nimport tempfile\nfrom typing import Optional\n\ndef format_time(seconds: float) -> str:\n    \"\"\"\n    Format time in seconds to MM:SS format\n    \"\"\"\n    try:\n        minutes = int(seconds // 60)\n        seconds = int(seconds % 60)\n        return f\"{minutes:02d}:{seconds:02d}\"\n    except:\n        return \"00:00\"\n\ndef validate_video_file(uploaded_file) -> bool:\n    \"\"\"\n    Validate uploaded video file\n    \"\"\"\n    try:\n        # Check file size (max 100MB)\n        if uploaded_file.size > 100 * 1024 * 1024:\n            return False\n        \n        # Check file extension\n        allowed_extensions = ['.mp4', '.avi', '.mov', '.mkv']\n        file_extension = os.path.splitext(uploaded_file.name)[1].lower()\n        \n        return file_extension in allowed_extensions\n    except:\n        return False\n\ndef create_temp_file(suffix: str = '.tmp') -> str:\n    \"\"\"\n    Create a temporary file and return its path\n    \"\"\"\n    return tempfile.mktemp(suffix=suffix)\n\ndef cleanup_temp_file(file_path: str):\n    \"\"\"\n    Safely delete a temporary file\n    \"\"\"\n    try:\n        if file_path and os.path.exists(file_path):\n            os.unlink(file_path)\n    except:\n        pass\n\ndef get_file_size_mb(file_path: str) -> float:\n    \"\"\"\n    Get file size in megabytes\n    \"\"\"\n    try:\n        return os.path.getsize(file_path) / (1024 * 1024)\n    except:\n        return 0.0\n\ndef validate_audio_format(audio_path: str) -> bool:\n    \"\"\"\n    Validate audio file format\n    \"\"\"\n    try:\n        from pydub import AudioSegment\n        audio = AudioSegment.from_file(audio_path)\n        return len(audio) > 0\n    except:\n        return False\n\ndef ensure_directory_exists(directory_path: str):\n    \"\"\"\n    Ensure a directory exists, create if it doesn't\n    \"\"\"\n    try:\n        os.makedirs(directory_path, exist_ok=True)\n    except:\n        pass\n\ndef get_supported_formats():\n    \"\"\"\n    Get list of supported video and audio formats\n    \"\"\"\n    return {\n        'video': ['mp4', 'avi', 'mov', 'mkv'],\n        'audio': ['wav', 'mp3', 'aac', 'm4a']\n    }\n\ndef calculate_processing_time_estimate(video_duration: float) -> str:\n    \"\"\"\n    Estimate processing time based on video duration\n    \"\"\"\n    try:\n        # Rough estimate: 3-5x the video duration for processing\n        estimated_seconds = video_duration * 4\n        \n        if estimated_seconds < 60:\n            return f\"~{int(estimated_seconds)} seconds\"\n        elif estimated_seconds < 3600:\n            minutes = int(estimated_seconds / 60)\n            return f\"~{minutes} minutes\"\n        else:\n            hours = int(estimated_seconds / 3600)\n            minutes = int((estimated_seconds % 3600) / 60)\n            return f\"~{hours}h {minutes}m\"\n    except:\n        return \"Unknown\"\n\ndef validate_api_keys():\n    \"\"\"\n    Validate that required API keys are available from environment\n    \"\"\"\n    import os\n    \n    required_keys = {\n        'elevenlabs': os.environ.get('ELEVENLABS_API_KEY', ''),\n        'gemini': os.environ.get('GEMINI_API_KEY', '')\n    }\n    \n    validation_results = {}\n    for service, key in required_keys.items():\n        validation_results[service] = bool(key and len(key) > 10)\n    \n    return validation_results\n\ndef format_file_size(size_bytes: int) -> str:\n    \"\"\"\n    Format file size in human readable format\n    \"\"\"\n    try:\n        if size_bytes < 1024:\n            return f\"{size_bytes} B\"\n        elif size_bytes < 1024**2:\n            return f\"{size_bytes/1024:.1f} KB\"\n        elif size_bytes < 1024**3:\n            return f\"{size_bytes/(1024**2):.1f} MB\"\n        else:\n            return f\"{size_bytes/(1024**3):.1f} GB\"\n    except:\n        return \"Unknown\"\n\ndef extract_filename_without_extension(file_path: str) -> str:\n    \"\"\"\n    Extract filename without extension from file path\n    \"\"\"\n    try:\n        return os.path.splitext(os.path.basename(file_path))[0]\n    except:\n        return \"unknown\"\n\ndef is_valid_language_code(lang_code: str) -> bool:\n    \"\"\"\n    Validate language code\n    \"\"\"\n    supported_languages = ['en', 'hi']\n    return lang_code in supported_languages\n\ndef create_progress_callback(progress_bar, status_text):\n    \"\"\"\n    Create a callback function for progress updates\n    \"\"\"\n    def update_progress(stage: str, percentage: int):\n        try:\n            progress_bar.progress(percentage / 100)\n            status_text.text(stage)\n        except:\n            pass\n    \n    return update_progress\n\ndef safe_float_conversion(value, default: float = 0.0) -> float:\n    \"\"\"\n    Safely convert value to float with default fallback\n    \"\"\"\n    try:\n        return float(value)\n    except (ValueError, TypeError):\n        return default\n\ndef safe_int_conversion(value, default: int = 0) -> int:\n    \"\"\"\n    Safely convert value to integer with default fallback\n    \"\"\"\n    try:\n        return int(value)\n    except (ValueError, TypeError):\n        return default\n\ndef generate_unique_filename(base_name: str, extension: str) -> str:\n    \"\"\"\n    Generate unique filename with timestamp\n    \"\"\"\n    import time\n    timestamp = int(time.time())\n    return f\"{base_name}_{timestamp}.{extension.lstrip('.')}\"\n","size_bytes":5119},"dubbing_service.py":{"content":"import os\nimport tempfile\nimport requests\nfrom elevenlabs.client import ElevenLabs\nfrom elevenlabs.types.voice_settings import VoiceSettings\nfrom typing import Optional\n\nclass DubbingService:\n    \"\"\"Handles AI voice generation using ElevenLabs\"\"\"\n    \n    def __init__(self, api_key: str):\n        \"\"\"Initialize dubbing service with ElevenLabs API\"\"\"\n        self.client = ElevenLabs(api_key=api_key)\n        \n        # Default voice IDs for different languages\n        self.default_voices = {\n            'en': {\n                'male': 'pNInz6obpgDQGcFmaJgB',  # Adam\n                'female': 'EXAVITQu4vr4xnSDxMaL'  # Bella\n            },\n            'hi': {\n                'male': 'pNInz6obpgDQGcFmaJgB',  # Use English voice - can be changed\n                'female': 'EXAVITQu4vr4xnSDxMaL'  # Use English voice - can be changed\n            }\n        }\n        \n        self.current_voice_id = None\n    \n    def get_available_voices(self) -> list:\n        \"\"\"Get list of available voices\"\"\"\n        try:\n            voices = self.client.voices.get_all()\n            return [\n                {\n                    'voice_id': voice.voice_id,\n                    'name': voice.name,\n                    'category': voice.category,\n                    'labels': voice.labels\n                }\n                for voice in voices.voices\n            ]\n        except Exception as e:\n            print(f\"Error fetching voices: {e}\")\n            return []\n    \n    def select_voice(self, language: str, gender: str = 'female') -> str:\n        \"\"\"Select appropriate voice for language and gender\"\"\"\n        try:\n            # Use default voice mapping\n            voices = self.default_voices.get(language, self.default_voices['en'])\n            voice_id = voices.get(gender, voices['female'])\n            self.current_voice_id = voice_id\n            return voice_id\n        except:\n            # Fallback to default English female voice\n            self.current_voice_id = self.default_voices['en']['female']\n            return self.current_voice_id\n    \n    def generate_speech(self, text: str, language: str, \n                       stability: float = 0.75, clarity: float = 0.75, \n                       style: float = 0.0, enhance: bool = True) -> Optional[str]:\n        \"\"\"\n        Generate speech from text using ElevenLabs\n        \"\"\"\n        try:\n            # Select appropriate voice if not already set\n            if not self.current_voice_id:\n                self.select_voice(language)\n            \n            # Ensure voice_id is set\n            voice_id = self.current_voice_id\n            if not voice_id:\n                return None\n            \n            # Generate speech with voice settings\n            voice_settings_obj = VoiceSettings(\n                stability=stability,\n                similarity_boost=clarity,\n                style=style,\n                use_speaker_boost=enhance\n            )\n            \n            # Use appropriate model based on language\n            model = \"eleven_multilingual_v2\"\n            \n            response = self.client.text_to_speech.convert(\n                voice_id=voice_id,\n                text=text,\n                output_format=\"mp3_44100_128\",\n                model_id=model,\n                voice_settings=voice_settings_obj\n            )\n            \n            # Save audio to temporary file\n            output_path = tempfile.mktemp(suffix='.mp3')\n            \n            with open(output_path, 'wb') as f:\n                for chunk in response:\n                    f.write(chunk)\n            \n            return output_path\n            \n        except Exception as e:\n            print(f\"Error generating speech: {e}\")\n            return None\n    \n    def generate_speech_with_timing(self, segments: list, language: str,\n                                   stability: float = 0.75, clarity: float = 0.75,\n                                   style: float = 0.0) -> Optional[str]:\n        \"\"\"\n        Generate speech for multiple segments with timing information\n        \"\"\"\n        try:\n            from pydub import AudioSegment\n            import math\n            \n            # Select voice\n            if not self.current_voice_id:\n                self.select_voice(language)\n            \n            # Ensure voice_id is set\n            voice_id = self.current_voice_id\n            if not voice_id:\n                return None\n            \n            # Generate complete audio\n            complete_audio = AudioSegment.empty()\n            last_end_time = 0\n            \n            for segment in segments:\n                text = segment.get('text', '').strip()\n                start_time = segment.get('start', last_end_time)\n                end_time = segment.get('end', start_time + 1)\n                \n                if not text:\n                    # Add silence for empty segments\n                    silence_duration = int((end_time - start_time) * 1000)\n                    complete_audio += AudioSegment.silent(duration=silence_duration)\n                    last_end_time = end_time\n                    continue\n                \n                # Add silence before segment if needed\n                if start_time > last_end_time:\n                    silence_duration = int((start_time - last_end_time) * 1000)\n                    complete_audio += AudioSegment.silent(duration=silence_duration)\n                \n                # Generate speech for this segment\n                voice_settings_obj = VoiceSettings(\n                    stability=stability,\n                    similarity_boost=clarity,\n                    style=style,\n                    use_speaker_boost=True\n                )\n                \n                response = self.client.text_to_speech.convert(\n                    voice_id=voice_id,\n                    text=text,\n                    output_format=\"mp3_44100_128\",\n                    model_id=\"eleven_multilingual_v2\",\n                    voice_settings=voice_settings_obj\n                )\n                \n                # Save segment audio\n                segment_path = tempfile.mktemp(suffix='.mp3')\n                with open(segment_path, 'wb') as f:\n                    for chunk in response:\n                        f.write(chunk)\n                \n                # Load and adjust timing\n                segment_audio = AudioSegment.from_mp3(segment_path)\n                target_duration = int((end_time - start_time) * 1000)\n                \n                if len(segment_audio) != target_duration:\n                    # Adjust speed to match target duration\n                    speed_ratio = len(segment_audio) / target_duration\n                    if speed_ratio > 1.5 or speed_ratio < 0.5:\n                        # If speed change is too dramatic, just truncate or pad\n                        if len(segment_audio) > target_duration:\n                            segment_audio = segment_audio[:target_duration]\n                        else:\n                            segment_audio += AudioSegment.silent(\n                                duration=target_duration - len(segment_audio)\n                            )\n                    else:\n                        # Use speed change\n                        new_sample_rate = int(segment_audio.frame_rate * speed_ratio)\n                        segment_audio = segment_audio._spawn(\n                            segment_audio.raw_data,\n                            overrides={\"frame_rate\": new_sample_rate}\n                        ).set_frame_rate(segment_audio.frame_rate)\n                \n                complete_audio += segment_audio\n                last_end_time = end_time\n                \n                # Cleanup\n                os.unlink(segment_path)\n            \n            # Export final audio\n            output_path = tempfile.mktemp(suffix='.wav')\n            complete_audio.export(output_path, format='wav')\n            \n            return output_path\n            \n        except Exception as e:\n            print(f\"Error generating timed speech: {e}\")\n            return None\n    \n    def clone_voice_from_sample(self, audio_path: str, voice_name: str) -> Optional[str]:\n        \"\"\"\n        Clone a voice from an audio sample (if ElevenLabs supports it)\n        \"\"\"\n        try:\n            # This would require ElevenLabs voice cloning API\n            # For now, return None as this is a premium feature\n            print(\"Voice cloning requires premium ElevenLabs subscription\")\n            return None\n            \n        except Exception as e:\n            print(f\"Error cloning voice: {e}\")\n            return None\n    \n    def adjust_voice_emotion(self, text: str, emotion: str, language: str) -> Optional[str]:\n        \"\"\"\n        Generate speech with specific emotional tone\n        \"\"\"\n        try:\n            # Modify text to convey emotion\n            emotional_prompts = {\n                'happy': \"Say this with joy and enthusiasm: \",\n                'sad': \"Say this with sadness and melancholy: \",\n                'excited': \"Say this with high energy and excitement: \",\n                'calm': \"Say this in a calm and peaceful manner: \",\n                'serious': \"Say this seriously and professionally: \"\n            }\n            \n            prompt = emotional_prompts.get(emotion, \"\")\n            enhanced_text = f\"{prompt}{text}\" if prompt else text\n            \n            return self.generate_speech(enhanced_text, language)\n            \n        except Exception as e:\n            print(f\"Error adjusting emotion: {e}\")\n            return self.generate_speech(text, language)\n    \n    def get_voice_info(self, voice_id: str) -> dict:\n        \"\"\"Get information about a specific voice\"\"\"\n        try:\n            voice = self.client.voices.get(voice_id)\n            return {\n                'voice_id': voice.voice_id,\n                'name': voice.name,\n                'category': voice.category,\n                'description': voice.description,\n                'labels': voice.labels,\n                'preview_url': voice.preview_url\n            }\n        except Exception as e:\n            print(f\"Error getting voice info: {e}\")\n            return {}\n","size_bytes":10194},"replit.md":{"content":"# Anuvaad AI - Professional Video Dubbing\n\n## Overview\n\nAnuvaad AI is a professional video dubbing application that translates and dubs videos from one language to another while maintaining synchronization and audio quality. The application uses ElevenLabs for AI voice generation, Google Gemini for translation, and various audio/video processing libraries to create seamless dubbed content.\n\n**Core Purpose**: \n- **Video Dubbing**: Transform videos into different languages by extracting audio, translating, generating dubbed audio, and synchronizing with original video timing\n- **YouTube Summarizer**: Download and transcribe YouTube videos, then generate AI-powered summaries with customizable word count\n- **Word to Story**: Create engaging stories from input words with themes, supporting English and Hindi with emotional audio narration\n- **Additional Tools**: Text-to-speech, speech-to-text, and text translation utilities\n\n**Technology Stack**:\n- **Frontend**: React (Vite) with Netflix-style UI - modern, responsive interface with hero section, feature cards, and modal previews\n- **Backend**: Flask REST API with JWT authentication\n- **Database**: SQLite for user accounts and history tracking\n- **Video Processing**: MoviePy, FFmpeg\n- **Audio Processing**: Pydub, SpeechRecognition, Librosa (optional)\n- **AI Services**: ElevenLabs (voice generation), Google Gemini (translation)\n- **Language**: Python 3.x (backend), JavaScript/React (frontend)\n\n## User Preferences\n\nPreferred communication style: Simple, everyday language.\n\n## System Architecture\n\n### 1. Frontend Architecture (React-based Netflix-style UI)\n\n**Design Pattern**: Single-page application with component-based architecture\n\nThe application uses React with Vite for a modern, Netflix-inspired interface:\n- **Styling Approach**: Custom CSS with Netflix-like design patterns\n- **Hero Section**: Full-width background image with gradient overlay and call-to-action\n- **Feature Cards**: Image-based cards with hover effects that open detailed modal previews\n- **Layout**: Responsive design with sticky header navigation\n- **Theme**: Dark theme with Netflix-style red accents (#E50914)\n- **Typography**: Inter font family for modern, clean appearance\n- **Authentication**: Modal-based login/signup flows with JWT token management\n\n**Key Components**:\n- `Hero.jsx` - Full-width hero section with background image\n- `FeatureCard.jsx` - Individual feature cards with images\n- `FeatureModal.jsx` - Detailed modal view with feature images and information\n- `Header.jsx` - Sticky navigation with login/signup buttons\n- `AuthContext.jsx` - Authentication state management\n\n**Key Decision**: React was chosen for its component reusability and ability to create a polished Netflix-style UI with smooth interactions.\n\n### 2. Backend Architecture (Modular Service Layer)\n\n**Design Pattern**: Service-oriented architecture with single-responsibility modules\n\nThe application is organized into specialized service modules:\n\n#### Video Processing Service (`video_processor.py`)\n- **Purpose**: Handle video file operations and metadata extraction\n- **Key Capabilities**:\n  - Extract video information (duration, fps, dimensions, audio presence)\n  - Extract audio tracks from video files using MoviePy/FFmpeg\n- **Technology**: MoviePy with FFmpeg backend for cross-format compatibility\n\n#### Audio Processing Service (`audio_processor.py`)\n- **Purpose**: Separate speech from background audio and process audio segments\n- **Key Capabilities**:\n  - Speech/background separation using silence detection\n  - Audio segment timestamp tracking\n  - Optional advanced processing with Librosa (graceful degradation if unavailable)\n- **Technology**: Pydub for basic processing, SpeechRecognition for transcription, Librosa for advanced features\n- **Design Decision**: Librosa is optional dependency with fallback functionality to maintain compatibility\n\n#### Translation Service (`translation_service.py`)\n- **Purpose**: Translate transcribed text between languages\n- **Key Capabilities**:\n  - Context-aware translation optimized for video dubbing\n  - Maintains emotional tone and timing considerations\n  - Cultural appropriateness handling\n- **Technology**: Google Gemini AI with specialized prompting\n- **Design Decision**: Uses AI model for natural, context-aware translations rather than literal word-for-word translation\n\n#### Dubbing Service (`dubbing_service.py`)\n- **Purpose**: Generate AI voice audio for translated text\n- **Key Capabilities**:\n  - Voice selection by language and gender\n  - Access to ElevenLabs voice library\n  - Voice configuration management\n- **Technology**: ElevenLabs API with voice settings customization\n- **Design Decision**: Default voice mapping per language with extensibility for custom voice selection\n\n#### ElevenLabs Dubbing Integration (`elevenlabs_dubbing.py`)\n- **Purpose**: Use ElevenLabs' complete dubbing API for automated dubbing\n- **Key Capabilities**:\n  - Upload video and create dubbing projects\n  - Monitor dubbing progress with polling\n  - Download completed dubbed content\n- **Technology**: ElevenLabs Dubbing API in \"automatic\" mode\n- **Design Decision**: Provides alternative to manual pipeline using ElevenLabs' end-to-end dubbing service\n\n#### Synchronization Engine (`sync_engine.py`)\n- **Purpose**: Align dubbed audio timing with original video\n- **Key Capabilities**:\n  - Proportional timing allocation across segments\n  - Audio stretching/compression for timing match\n  - Segment-based synchronization\n- **Technology**: Pydub for audio manipulation, optional Librosa for time-stretching\n- **Design Decision**: Segment-based approach allows precise control over timing while maintaining audio quality\n\n#### YouTube Summarizer Service (`youtube_summarizer.py`) - Added Oct 2025\n- **Purpose**: Download, transcribe, and summarize YouTube videos\n- **Key Capabilities**:\n  - Download YouTube videos using yt-dlp\n  - Extract and transcribe audio in chunks for long videos\n  - Generate AI-powered summaries with configurable word count\n- **Technology**: yt-dlp for downloading, SpeechRecognition for transcription, Google Gemini for summarization\n- **Design Decision**: Chunk-based transcription handles long videos efficiently; summary-only output (no full transcript) keeps UI focused\n\n#### Story Generator Service (`story_generator.py`) - Added Oct 2025\n- **Purpose**: Generate creative stories from input words with emotional audio narration\n- **Key Capabilities**:\n  - AI story generation based on user-provided words, theme, and word count\n  - Support for English and Hindi languages\n  - Emotional text-to-speech with enhanced voice settings\n  - Audio download functionality\n- **Technology**: Google Gemini for story generation, ElevenLabs for emotional TTS\n- **Design Decision**: Uses emotional voice settings (reduced stability, increased style) for engaging narration; graceful degradation when quota exceeded\n\n### 3. Processing Pipeline Architecture\n\n**Workflow**: Multi-stage processing pipeline with error handling at each stage\n\n**Primary Flow**:\n1. **Video Upload & Validation** â†’ Size limits (100MB), format validation\n2. **Audio Extraction** â†’ MoviePy extracts WAV audio from video\n3. **Speech Recognition** â†’ SpeechRecognition transcribes audio to text\n4. **Audio Separation** â†’ Pydub separates speech from background (optional)\n5. **Translation** â†’ Gemini AI translates text to target language\n6. **Voice Generation** â†’ ElevenLabs generates dubbed audio\n7. **Synchronization** â†’ Sync engine aligns dubbed audio with original timing\n8. **Final Assembly** â†’ MoviePy combines dubbed audio with original video\n\n**Alternative Flow** (ElevenLabs Direct):\n1. **Video Upload** â†’ Direct upload to ElevenLabs\n2. **Dubbing Processing** â†’ ElevenLabs handles extraction, translation, dubbing\n3. **Download Result** â†’ Retrieve completed dubbed video\n\n**Design Decision**: Dual-pipeline approach provides flexibilityâ€”use manual pipeline for fine control or ElevenLabs automatic mode for simplicity.\n\n### 4. Configuration & Utilities\n\n**Utility Functions** (`utils.py`):\n- Time formatting for UI display\n- File validation for security\n- Temporary file management\n- Safe cleanup operations\n\n**Design Decision**: Centralized utilities reduce code duplication and provide consistent error handling patterns.\n\n### 5. Error Handling & Resilience\n\n**Strategy**: Graceful degradation with optional dependencies\n\n- **Optional Dependencies**: Librosa features degrade gracefully if not installed\n- **API Failures**: Service modules return `None` or `Optional` types with try-except blocks\n- **File Operations**: Temporary file cleanup in finally blocks\n- **Validation**: Early validation prevents processing invalid inputs\n\n### 6. State Management\n\n**Approach**: Stateless processing with file-based intermediates\n\n- **Session State**: Streamlit's session state for UI state management\n- **Data Flow**: File paths passed between processing stages\n- **Temporary Storage**: System temp directory for intermediate files\n- **Cleanup**: Explicit cleanup after processing completion\n\n**Design Decision**: File-based intermediates allow inspection, debugging, and recovery while avoiding memory constraints with large videos.\n\n## External Dependencies\n\n### AI & Cloud Services\n\n**ElevenLabs API** (Primary Voice Service)\n- **Purpose**: AI voice generation and complete dubbing pipeline\n- **Authentication**: API key required (user-provided)\n- **Services Used**:\n  - Voice synthesis with customizable voice settings\n  - Complete dubbing API for automated video dubbing\n  - Voice library access for selection\n- **Rate Limits**: Service-dependent (ElevenLabs tier-based)\n\n**Google Gemini AI** (Translation Service)\n- **Purpose**: Context-aware text translation\n- **Authentication**: API key required (user-provided)\n- **Service Used**: `genai.Client` for content generation\n- **Model**: Uses Gemini models for translation tasks\n\n### Media Processing Libraries\n\n**MoviePy** (Video Processing)\n- **Purpose**: Video file manipulation and audio extraction\n- **Backend**: FFmpeg for codec support\n- **Usage**: Video clips, audio extraction, final assembly\n\n**FFmpeg** (Media Codec Engine)\n- **Purpose**: Low-level media processing and format conversion\n- **Integration**: Used via MoviePy wrapper\n- **Requirement**: Must be installed on system PATH\n\n**Pydub** (Audio Processing)\n- **Purpose**: Audio manipulation and format conversion\n- **Backend**: FFmpeg for audio codecs\n- **Usage**: Silence detection, audio concatenation, format conversion\n\n**SpeechRecognition** (Speech-to-Text)\n- **Purpose**: Audio transcription\n- **Backend**: Multiple recognizer options (Google, Sphinx, etc.)\n- **Usage**: Convert speech audio to text for translation\n\n**Librosa** (Advanced Audio Analysis) - Optional\n- **Purpose**: Advanced audio processing and time-stretching\n- **Installation**: Optional dependency with fallback behavior\n- **Usage**: Audio time-stretching for synchronization\n\n**Soundfile** (Audio I/O) - Optional\n- **Purpose**: Audio file reading/writing for Librosa\n- **Installation**: Optional dependency paired with Librosa\n\n### Frontend Framework\n\n**Streamlit** (Web Framework)\n- **Purpose**: Rapid web application development\n- **Features Used**:\n  - File upload widgets\n  - Progress indicators\n  - Custom CSS injection\n  - Video/audio playback\n  - Session state management\n\n### Python Standard Libraries\n\n**Core Dependencies**:\n- `tempfile`: Temporary file management\n- `os`: File system operations\n- `pathlib`: Path manipulation\n- `time`: Timing and delays for API polling\n\n### System Requirements\n\n**FFmpeg Installation**: Required on system PATH for MoviePy and Pydub to function\n**Python Version**: 3.x (type hints suggest 3.6+)\n**Optional C Dependencies**: Librosa requires compiled audio processing libraries","size_bytes":11824},"sync_engine.py":{"content":"import os\nimport tempfile\nfrom pydub import AudioSegment\ntry:\n    import librosa\n    import soundfile as sf\n    import numpy as np\n    LIBROSA_AVAILABLE = True\nexcept ImportError:\n    import numpy as np\n    LIBROSA_AVAILABLE = False\nfrom typing import List, Dict, Optional\n\nclass SyncEngine:\n    \"\"\"Handles audio-video synchronization and timing alignment\"\"\"\n    \n    def __init__(self):\n        \"\"\"Initialize synchronization engine\"\"\"\n        pass\n    \n    def synchronize_audio(self, dubbed_audio_path: str, original_segments: List[Dict], \n                         original_audio_path: str) -> Optional[str]:\n        \"\"\"\n        Synchronize dubbed audio with original video timing\n        \"\"\"\n        try:\n            # Load original and dubbed audio\n            original_audio = AudioSegment.from_file(original_audio_path)\n            dubbed_audio = AudioSegment.from_file(dubbed_audio_path)\n            \n            # Create synchronized audio with same length as original\n            synced_audio = AudioSegment.silent(duration=len(original_audio))\n            \n            # Process each segment\n            for segment in original_segments:\n                start_ms = int(segment['start'] * 1000)\n                end_ms = int(segment['end'] * 1000)\n                segment_duration = end_ms - start_ms\n                \n                if segment_duration <= 0:\n                    continue\n                \n                # Extract corresponding portion from dubbed audio\n                # For now, assume dubbed audio follows same segment order\n                try:\n                    # Calculate proportional position in dubbed audio\n                    total_original_duration = sum(\n                        (seg['end'] - seg['start']) for seg in original_segments\n                    )\n                    segment_ratio = (segment['end'] - segment['start']) / total_original_duration\n                    \n                    dubbed_start = int(len(dubbed_audio) * (segment['start'] / original_segments[-1]['end']))\n                    dubbed_segment_length = int(len(dubbed_audio) * segment_ratio)\n                    dubbed_end = dubbed_start + dubbed_segment_length\n                    \n                    if dubbed_end > len(dubbed_audio):\n                        dubbed_end = len(dubbed_audio)\n                    \n                    dubbed_segment = dubbed_audio[dubbed_start:dubbed_end]\n                    \n                    # Adjust dubbed segment to fit original timing\n                    if len(dubbed_segment) != segment_duration:\n                        dubbed_segment = self._adjust_segment_timing(\n                            dubbed_segment, segment_duration\n                        )\n                    \n                    # Overlay dubbed segment onto synchronized audio\n                    synced_audio = synced_audio.overlay(dubbed_segment, position=start_ms)\n                    \n                except Exception as e:\n                    print(f\"Error processing segment: {e}\")\n                    continue\n            \n            # Export synchronized audio\n            output_path = tempfile.mktemp(suffix='.wav')\n            synced_audio.export(output_path, format='wav')\n            \n            return output_path\n            \n        except Exception as e:\n            print(f\"Error in synchronization: {e}\")\n            return dubbed_audio_path\n    \n    def _adjust_segment_timing(self, audio_segment: AudioSegment, target_duration: int) -> AudioSegment:\n        \"\"\"\n        Adjust audio segment timing to match target duration\n        \"\"\"\n        try:\n            current_duration = len(audio_segment)\n            \n            if current_duration == target_duration:\n                return audio_segment\n            \n            # Calculate speed adjustment ratio\n            speed_ratio = current_duration / target_duration\n            \n            # Limit extreme speed changes\n            if speed_ratio > 2.0:\n                speed_ratio = 2.0\n            elif speed_ratio < 0.5:\n                speed_ratio = 0.5\n            \n            # Apply speed change using frame rate manipulation\n            new_frame_rate = int(audio_segment.frame_rate * speed_ratio)\n            \n            adjusted_segment = audio_segment._spawn(\n                audio_segment.raw_data,\n                overrides={\"frame_rate\": new_frame_rate}\n            ).set_frame_rate(audio_segment.frame_rate)\n            \n            # Fine-tune length if still not matching\n            if len(adjusted_segment) > target_duration:\n                adjusted_segment = adjusted_segment[:target_duration]\n            elif len(adjusted_segment) < target_duration:\n                padding = target_duration - len(adjusted_segment)\n                adjusted_segment += AudioSegment.silent(duration=padding)\n            \n            return adjusted_segment\n            \n        except Exception as e:\n            print(f\"Error adjusting segment timing: {e}\")\n            return audio_segment\n    \n    def align_with_video_frames(self, audio_path: str, video_fps: float, \n                               video_duration: float) -> Optional[str]:\n        \"\"\"\n        Align audio with video frame rate for perfect synchronization\n        \"\"\"\n        try:\n            # Load audio\n            audio = AudioSegment.from_file(audio_path)\n            \n            # Calculate target duration based on video\n            target_duration_ms = int(video_duration * 1000)\n            \n            # Adjust audio length to match video exactly\n            if len(audio) != target_duration_ms:\n                if len(audio) > target_duration_ms:\n                    # Truncate audio\n                    audio = audio[:target_duration_ms]\n                else:\n                    # Pad with silence\n                    padding = target_duration_ms - len(audio)\n                    audio += AudioSegment.silent(duration=padding)\n            \n            # Ensure audio sample rate is compatible with video frame rate\n            # Standard approach: use 48kHz for video compatibility\n            target_sample_rate = 48000\n            if audio.frame_rate != target_sample_rate:\n                audio = audio.set_frame_rate(target_sample_rate)\n            \n            # Export aligned audio\n            output_path = tempfile.mktemp(suffix='.wav')\n            audio.export(output_path, format='wav')\n            \n            return output_path\n            \n        except Exception as e:\n            print(f\"Error aligning with video frames: {e}\")\n            return audio_path\n    \n    def detect_speech_timing(self, audio_path: str) -> List[Dict]:\n        \"\"\"\n        Detect precise speech timing in audio\n        \"\"\"\n        try:\n            if not LIBROSA_AVAILABLE:\n                # Fallback: use basic pydub detection\n                from pydub.silence import detect_nonsilent\n                audio = AudioSegment.from_file(audio_path)\n                \n                nonsilent_ranges = detect_nonsilent(\n                    audio,\n                    min_silence_len=100,\n                    silence_thresh=audio.dBFS - 16\n                )\n                \n                return [{\n                    'start': start / 1000.0,\n                    'end': end / 1000.0,\n                    'duration': (end - start) / 1000.0\n                } for start, end in nonsilent_ranges]\n            \n            # Load audio with librosa for better analysis\n            y, sr = librosa.load(audio_path)\n            \n            # Detect onset times (beginning of sounds)\n            onset_frames = librosa.onset.onset_detect(\n                y=y, sr=sr, units='time', hop_length=512, backtrack=True\n            )\n            \n            # Detect speech activity using energy-based method\n            frame_length = 2048\n            hop_length = 512\n            \n            # Calculate RMS energy\n            rms = librosa.feature.rms(\n                y=y, frame_length=frame_length, hop_length=hop_length\n            )[0]\n            \n            # Convert frame indices to time\n            times = librosa.frames_to_time(\n                np.arange(len(rms)), sr=sr, hop_length=hop_length\n            )\n            \n            # Detect speech segments based on energy threshold\n            energy_threshold = np.mean(rms) * 0.1\n            speech_segments = []\n            \n            in_speech = False\n            segment_start = None\n            \n            for i, (time, energy) in enumerate(zip(times, rms)):\n                if energy > energy_threshold and not in_speech:\n                    # Start of speech\n                    in_speech = True\n                    segment_start = time\n                elif energy <= energy_threshold and in_speech:\n                    # End of speech\n                    in_speech = False\n                    if segment_start is not None:\n                        speech_segments.append({\n                            'start': segment_start,\n                            'end': time,\n                            'duration': time - segment_start\n                        })\n            \n            # Handle case where audio ends during speech\n            if in_speech and segment_start is not None:\n                speech_segments.append({\n                    'start': segment_start,\n                    'end': times[-1],\n                    'duration': times[-1] - segment_start\n                })\n            \n            return speech_segments\n            \n        except Exception as e:\n            print(f\"Error detecting speech timing: {e}\")\n            return []\n    \n    def create_timing_map(self, original_segments: List[Dict], \n                         dubbed_segments: List[Dict]) -> List[Dict]:\n        \"\"\"\n        Create timing mapping between original and dubbed segments\n        \"\"\"\n        try:\n            timing_map = []\n            \n            for i, (orig, dubbed) in enumerate(zip(original_segments, dubbed_segments)):\n                timing_map.append({\n                    'segment_id': i,\n                    'original_start': orig.get('start', 0),\n                    'original_end': orig.get('end', 0),\n                    'original_duration': orig.get('end', 0) - orig.get('start', 0),\n                    'dubbed_start': dubbed.get('start', 0),\n                    'dubbed_end': dubbed.get('end', 0),\n                    'dubbed_duration': dubbed.get('end', 0) - dubbed.get('start', 0),\n                    'speed_ratio': (dubbed.get('end', 0) - dubbed.get('start', 0)) / \n                                  max(orig.get('end', 0) - orig.get('start', 0), 0.001)\n                })\n            \n            return timing_map\n            \n        except Exception as e:\n            print(f\"Error creating timing map: {e}\")\n            return []\n    \n    def apply_dynamic_time_warping(self, original_audio_path: str, \n                                  dubbed_audio_path: str) -> Optional[str]:\n        \"\"\"\n        Apply dynamic time warping for better synchronization\n        \"\"\"\n        try:\n            if not LIBROSA_AVAILABLE:\n                # Fallback: simple duration matching\n                original = AudioSegment.from_file(original_audio_path)\n                dubbed = AudioSegment.from_file(dubbed_audio_path)\n                \n                if len(original) == len(dubbed):\n                    return dubbed_audio_path\n                \n                # Adjust speed to match duration\n                speed_ratio = len(dubbed) / len(original)\n                new_frame_rate = int(dubbed.frame_rate * speed_ratio)\n                adjusted = dubbed._spawn(dubbed.raw_data, overrides={\"frame_rate\": new_frame_rate})\n                adjusted = adjusted.set_frame_rate(dubbed.frame_rate)\n                \n                output_path = tempfile.mktemp(suffix='.wav')\n                adjusted.export(output_path, format='wav')\n                return output_path\n            \n            # Load both audio files\n            y1, sr1 = librosa.load(original_audio_path)\n            y2, sr2 = librosa.load(dubbed_audio_path)\n            \n            # Ensure same sample rate\n            if sr1 != sr2:\n                y2 = librosa.resample(y2, orig_sr=sr2, target_sr=sr1)\n                sr2 = sr1\n            \n            # Extract chroma features for alignment\n            chroma1 = librosa.feature.chroma_cqt(y=y1, sr=sr1)\n            chroma2 = librosa.feature.chroma_cqt(y=y2, sr=sr2)\n            \n            # Compute DTW alignment\n            D, wp = librosa.sequence.dtw(chroma1, chroma2, subseq=True)\n            \n            # Apply time warping to dubbed audio\n            # This is a simplified approach - full DTW implementation would be more complex\n            warped_indices = wp[:, 1]\n            warped_audio = y2[warped_indices * (len(y2) // len(warped_indices))]\n            \n            # Ensure output length matches original\n            if len(warped_audio) != len(y1):\n                if len(warped_audio) > len(y1):\n                    warped_audio = warped_audio[:len(y1)]\n                else:\n                    # Pad with zeros\n                    warped_audio = np.pad(warped_audio, (0, len(y1) - len(warped_audio)))\n            \n            # Save warped audio\n            output_path = tempfile.mktemp(suffix='.wav')\n            sf.write(output_path, warped_audio, sr1)\n            \n            return output_path\n            \n        except Exception as e:\n            print(f\"Error in dynamic time warping: {e}\")\n            return dubbed_audio_path\n    \n    def validate_synchronization(self, original_audio_path: str, \n                                dubbed_audio_path: str) -> Dict:\n        \"\"\"\n        Validate synchronization quality between original and dubbed audio\n        \"\"\"\n        try:\n            # Load both audio files\n            original = AudioSegment.from_file(original_audio_path)\n            dubbed = AudioSegment.from_file(dubbed_audio_path)\n            \n            # Basic validation metrics\n            duration_diff = abs(len(original) - len(dubbed))\n            duration_match = duration_diff < 100  # Within 100ms\n            \n            validation_result = {\n                'duration_match': duration_match,\n                'duration_difference_ms': duration_diff,\n                'original_duration': len(original),\n                'dubbed_duration': len(dubbed),\n                'sync_quality': 'Good' if duration_match else 'Needs adjustment'\n            }\n            \n            return validation_result\n            \n        except Exception as e:\n            print(f\"Error validating synchronization: {e}\")\n            return {\n                'duration_match': False,\n                'sync_quality': 'Error',\n                'error': str(e)\n            }\n","size_bytes":14801},"attached_assets/news_podcast_1760030436850.py":{"content":"import os\r\nimport requests\r\nimport subprocess\r\nimport json\r\n\r\n# ---------------------------\r\n# 1ï¸âƒ£ Set your API keys\r\n# ---------------------------\r\nGEMINI_API_KEY = \"AIzaSyBqp2PwIV4QgLIjo73mKmt7vyb-D1HnkhA\"\r\nELEVENLABS_API_KEY = \"sk_b8c6c84322c3f2e6dc4242bab781a0e46aa5dd742f1b82a5\"\r\n\r\nif not GEMINI_API_KEY or not ELEVENLABS_API_KEY:\r\n    exit(\"ğŸš¨ Please set both GEMINI_API_KEY and ELEVENLABS_API_KEY.\")\r\n\r\n# ElevenLabs Voice IDs\r\nHOST_VOICE_ID = \"pNInz6obpgDQGcFmaJgB\"  # Adam\r\nEXPERT_VOICE_ID = \"21m00Tcm4TlvDq8ikWAM\"  # Rachel\r\n\r\n# ---------------------------\r\n# 2ï¸âƒ£ Hardcoded article\r\n# ---------------------------\r\narticle = \"\"\"\r\nHaryana cadre IPS officer Y Puran Kumar, 52, left an eight-page â€œfinal noteâ€,\r\nin which he named three retired IAS and 10 IPS officers and how they allegedly\r\nâ€œtortured him mentally and administrativelyâ€ during his service-career.\r\nThe wife of the 2001-batch officer, Amneet P Kumar, meanwhile, returned from\r\nJapan Wednesday and allegedly refused to get the post mortem examination,\r\nâ€œtill justice is deliveredâ€. The Indian Express confirmed it from three sources\r\nwho met Amneet Wednesday and confirmed that â€œalthough visibly shattered, she was\r\nfuriously angry and has refused to get the post mortem done till justice is deliveredâ€.\r\n\"\"\"\n\r\n\r\n# ---------------------------\r\n# 3ï¸âƒ£ Generate podcast script using Gemini (latest model)\r\n# ---------------------------\r\ndef generate_podcast_script(article_text):\r\n    print(\"\\nGenerating podcast script via Gemini...\")\r\n\r\n    url = f\"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5:generateContent?key={GEMINI_API_KEY}\"\r\n    headers = {\"Content-Type\": \"application/json\"}\r\n\r\n    prompt = f\"\"\"\r\nSummarize the following article into a detailed podcast script (~300 words) as a conversation:\r\n- Include a \"Host\" and an \"Expert\".\r\n- Each line should start with \"Host:\" or \"Expert:\".\r\n- Make it engaging, explanatory, and conversational.\r\n\r\nArticle:\r\n---\r\n{article_text}\r\n---\r\n\"\"\"\r\n\r\n    data = {\r\n        \"prompt\": prompt,\r\n        \"temperature\": 0.7,\r\n        \"candidate_count\": 1,\r\n        \"max_output_tokens\": 800\r\n    }\r\n\r\n    try:\r\n        response = requests.post(url, headers=headers, json=data)\r\n        response.raise_for_status()\r\n        result = response.json()\r\n        script = result['candidates'][0]['content'][0]['text']\r\n        return script\r\n    except Exception as e:\r\n        print(f\"âš ï¸ Failed to generate script from Gemini API: {e}\")\r\n        print(\"Using fallback sample script instead.\")\r\n        # Fallback longer script (~1â€“2 minutes)\r\n        return \"\"\"\r\nHost: Welcome to our podcast. Today, we discuss a major news story from Haryana.\r\nExpert: IPS officer Y Puran Kumar, 52, left an eight-page final note, naming several retired IAS and IPS officials.\r\nHost: He alleged that he was mentally and administratively tortured during his career.\r\nExpert: His wife, Amneet P Kumar, recently returned from Japan and is demanding justice.\r\nHost: She has refused to allow the post-mortem until her demands are addressed.\r\nExpert: According to sources, she is visibly shattered but determined to see justice done.\r\nHost: This case has raised serious concerns about administrative pressures on officers.\r\nExpert: Absolutely. It's crucial to bring attention to such incidents and ensure accountability.\r\nHost: We'll continue following this story and provide updates in future episodes.\r\nExpert: Stay tuned for more insights and discussions on critical news.\r\n\"\"\"\r\n\r\n\nscript = generate_podcast_script(article)\n\r\n\r\n# ---------------------------\r\n# 4ï¸âƒ£ Generate audio for each line using ElevenLabs\r\n# ---------------------------\r\ndef generate_audio_elevenlabs(text, voice_id, output_file):\r\n    url = f\"https://api.elevenlabs.io/v1/text-to-speech/{voice_id}\"\r\n    headers = {\r\n        \"Accept\": \"audio/mpeg\",\r\n        \"Content-Type\": \"application/json\",\r\n        \"xi-api-key\": ELEVENLABS_API_KEY\r\n    }\r\n    data = {\r\n        \"text\": text,\r\n        \"model_id\": \"eleven_multilingual_v2\",\r\n        \"voice_settings\": {\n            \"stability\": 0.5,\n            \"similarity_boost\": 0.75\n        }\r\n    }\r\n    response = requests.post(url, headers=headers, json=data)\r\n    if response.status_code == 200:\r\n        with open(output_file, 'wb') as f:\r\n            f.write(response.content)\r\n        return True\r\n    else:\r\n        print(\n            f\"Error generating audio: {response.status_code} {response.text}\")\r\n        return False\r\n\r\n\n# ---------------------------\r\n# 5ï¸âƒ£ Generate audio files for each line\r\n# ---------------------------\r\nlines = script.strip().split('\\n')\r\naudio_files = []\r\ntemp_file_list = \"ffmpeg_input_files.txt\"\r\n\r\nwith open(temp_file_list, \"w\") as f:\r\n    for i, line in enumerate(lines):\r\n        line = line.strip()\r\n        if not line:\r\n            continue\r\n\r\n        speaker, text, voice_id = \"\", \"\", \"\"\r\n        if line.lower().startswith(\"host:\"):\r\n            speaker, text, voice_id = \"Host\", line[5:].strip(), HOST_VOICE_ID\r\n        elif line.lower().startswith(\"expert:\"):\r\n            speaker, text, voice_id = \"Expert\", line[7:].strip(\n            ), EXPERT_VOICE_ID\r\n        else:\r\n            continue\r\n\r\n        if not text:\r\n            continue\r\n\r\n        output_file = f\"temp_audio_{i}.mp3\"\r\n        if generate_audio_elevenlabs(text, voice_id, output_file):\r\n            audio_files.append(output_file)\r\n            f.write(f\"file '{output_file}'\\n\")\r\n\r\n# ---------------------------\r\n# 6ï¸âƒ£ Merge audio files using FFmpeg\r\n# ---------------------------\r\nif audio_files:\r\n    output_podcast = \"final_podcast.mp3\"\r\n    subprocess.run([\r\n        \"ffmpeg\", \"-y\", \"-f\", \"concat\", \"-safe\", \"0\", \"-i\", temp_file_list,\n        \"-c\", \"copy\", output_podcast\r\n    ],\n                   check=True)\r\n    print(f\"\\nğŸ‰ Podcast generated successfully: {output_podcast}\")\r\nelse:\r\n    print(\"âŒ No audio files generated. Podcast creation failed.\")\r\n\r\n# ---------------------------\r\n# 7ï¸âƒ£ Cleanup\r\n# ---------------------------\r\nif os.path.exists(temp_file_list):\r\n    os.remove(temp_file_list)\r\nfor file in audio_files:\r\n    if os.path.exists(file):\r\n        os.remove(file)\r\n","size_bytes":6174},"story_generator.py":{"content":"import os\nimport tempfile\nfrom google import genai\nfrom elevenlabs import ElevenLabs\nfrom typing import Optional, Dict, List\n\n\nclass StoryGenerator:\n    \"\"\"Handles story generation from words and emotional text-to-speech\"\"\"\n    \n    def __init__(self, gemini_api_key: str, elevenlabs_api_key: str):\n        \"\"\"Initialize story generator with Gemini and ElevenLabs APIs\"\"\"\n        self.gemini_client = genai.Client(api_key=gemini_api_key)\n        self.elevenlabs_client = ElevenLabs(api_key=elevenlabs_api_key)\n        \n        self.voice_mapping = {\n            'english': {\n                'emotional': 'EXAVITQu4vr4xnSDxMaL',\n                'narrative': '21m00Tcm4TlvDq8ikWAM'\n            },\n            'hindi': {\n                'emotional': 'pNInz6obpgDQGcFmaJgB',\n                'narrative': 'EXAVITQu4vr4xnSDxMaL'\n            }\n        }\n    \n    def generate_story(self, words: List[str], theme: str, word_count: int, language: str) -> Optional[str]:\n        \"\"\"\n        Generate a story from input words with specified theme, word count, and language\n        \"\"\"\n        try:\n            words_str = ', '.join(words)\n            \n            language_instruction = \"\"\n            if language.lower() == 'hindi':\n                language_instruction = \"Write the story in Hindi language.\"\n            else:\n                language_instruction = \"Write the story in English language.\"\n            \n            prompt = f\"\"\"\n            Create an engaging and emotional story based on the following:\n            \n            Words to include: {words_str}\n            Theme: {theme}\n            Target word count: {word_count} words\n            Language: {language}\n            \n            Instructions:\n            - {language_instruction}\n            - Create a narrative that naturally incorporates all the given words\n            - Follow the theme: {theme}\n            - Make the story emotionally engaging with vivid descriptions\n            - Include dialogue if appropriate\n            - Aim for approximately {word_count} words\n            - Make it suitable for audio narration with natural pacing\n            \n            Write the complete story below:\n            \"\"\"\n            \n            response = self.gemini_client.models.generate_content(\n                model=\"gemini-2.0-flash-exp\",\n                contents=prompt\n            )\n            \n            if response and response.text:\n                return response.text.strip()\n            else:\n                return None\n                \n        except Exception as e:\n            print(f\"Story generation error: {e}\")\n            return None\n    \n    def generate_emotional_audio(self, story_text: str, language: str) -> Optional[str]:\n        \"\"\"\n        Convert story text to emotional speech using ElevenLabs\n        Uses voices with better emotional expression\n        \"\"\"\n        try:\n            lang_key = 'hindi' if language.lower() == 'hindi' else 'english'\n            voice_id = self.voice_mapping[lang_key]['emotional']\n            \n            from elevenlabs import VoiceSettings\n            \n            audio_generator = self.elevenlabs_client.text_to_speech.convert(\n                text=story_text,\n                voice_id=voice_id,\n                model_id=\"eleven_multilingual_v2\",\n                output_format=\"mp3_44100_128\",\n                voice_settings=VoiceSettings(\n                    stability=0.5,\n                    similarity_boost=0.75,\n                    style=0.6,\n                    use_speaker_boost=True\n                )\n            )\n            \n            audio_bytes = b''.join(audio_generator)\n            \n            audio_path = tempfile.mktemp(suffix='.mp3')\n            with open(audio_path, 'wb') as f:\n                f.write(audio_bytes)\n            \n            return audio_path\n            \n        except Exception as e:\n            print(f\"Audio generation error: {e}\")\n            return None\n    \n    def create_story_with_audio(self, words: List[str], theme: str, word_count: int, language: str) -> Optional[Dict]:\n        \"\"\"\n        Complete pipeline: generate story and create emotional audio\n        Returns: {'story': text, 'audio_path': path, 'audio_bytes': bytes}\n        \"\"\"\n        try:\n            story = self.generate_story(words, theme, word_count, language)\n            if not story:\n                return None\n            \n            audio_path = self.generate_emotional_audio(story, language)\n            if not audio_path:\n                return {'story': story, 'audio_path': None, 'audio_bytes': None}\n            \n            with open(audio_path, 'rb') as f:\n                audio_bytes = f.read()\n            \n            return {\n                'story': story,\n                'audio_path': audio_path,\n                'audio_bytes': audio_bytes\n            }\n            \n        except Exception as e:\n            print(f\"Error creating story with audio: {e}\")\n            return None\n","size_bytes":4969},"youtube_summarizer.py":{"content":"import os\nimport tempfile\nimport yt_dlp\nimport speech_recognition as sr\nfrom pydub import AudioSegment\nfrom google import genai\nfrom typing import Optional, Dict\nimport time\n\n\nclass YouTubeSummarizer:\n    \"\"\"Handles YouTube video downloading, transcription, and summarization\"\"\"\n    \n    def __init__(self, gemini_api_key: str):\n        \"\"\"Initialize YouTube summarizer with Gemini API\"\"\"\n        self.gemini_client = genai.Client(api_key=gemini_api_key)\n        self.recognizer = sr.Recognizer()\n    \n    def download_video(self, youtube_url: str) -> Optional[Dict[str, str]]:\n        \"\"\"\n        Download YouTube video and extract audio\n        Returns: {'video_path': path, 'audio_path': path, 'title': title}\n        \"\"\"\n        try:\n            video_path = tempfile.mktemp(suffix='.mp4')\n            audio_path = tempfile.mktemp(suffix='.wav')\n            \n            ydl_opts = {\n                'format': 'best[ext=mp4]/best',\n                'outtmpl': video_path,\n                'quiet': True,\n                'no_warnings': True,\n            }\n            \n            with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n                info = ydl.extract_info(youtube_url, download=True)\n                title = info.get('title', 'Unknown')\n            \n            audio = AudioSegment.from_file(video_path)\n            audio.export(audio_path, format='wav')\n            \n            return {\n                'video_path': video_path,\n                'audio_path': audio_path,\n                'title': title\n            }\n            \n        except Exception as e:\n            print(f\"Error downloading YouTube video: {e}\")\n            return None\n    \n    def transcribe_audio(self, audio_path: str) -> Optional[str]:\n        \"\"\"\n        Transcribe audio to text using Google Speech Recognition\n        \"\"\"\n        try:\n            with sr.AudioFile(audio_path) as source:\n                audio_data = self.recognizer.record(source)\n                text = self.recognizer.recognize_google(audio_data)\n                return text\n        except sr.UnknownValueError:\n            print(\"Could not understand audio\")\n            return None\n        except Exception as e:\n            print(f\"Error transcribing audio: {e}\")\n            return None\n    \n    def transcribe_audio_in_chunks(self, audio_path: str, chunk_duration_ms: int = 30000) -> Optional[str]:\n        \"\"\"\n        Transcribe long audio by splitting into chunks\n        \"\"\"\n        try:\n            audio = AudioSegment.from_file(audio_path)\n            chunks = []\n            \n            for i in range(0, len(audio), chunk_duration_ms):\n                chunk = audio[i:i + chunk_duration_ms]\n                \n                chunk_path = tempfile.mktemp(suffix='.wav')\n                chunk.export(chunk_path, format='wav')\n                \n                try:\n                    with sr.AudioFile(chunk_path) as source:\n                        audio_data = self.recognizer.record(source)\n                        text = self.recognizer.recognize_google(audio_data)\n                        chunks.append(text)\n                except:\n                    pass\n                finally:\n                    if os.path.exists(chunk_path):\n                        os.unlink(chunk_path)\n            \n            return ' '.join(chunks) if chunks else None\n            \n        except Exception as e:\n            print(f\"Error transcribing audio in chunks: {e}\")\n            return None\n    \n    def summarize_text(self, text: str, word_count: int = 200) -> Optional[str]:\n        \"\"\"\n        Summarize text using Gemini AI with specified word count\n        \"\"\"\n        try:\n            prompt = f\"\"\"\n            Please provide a concise summary of the following text in approximately {word_count} words.\n            \n            Focus on the main points and key takeaways. Make the summary clear and informative.\n            \n            Text to summarize:\n            {text}\n            \n            Summary (approximately {word_count} words):\n            \"\"\"\n            \n            response = self.gemini_client.models.generate_content(\n                model=\"gemini-2.0-flash-exp\",\n                contents=prompt\n            )\n            \n            if response and response.text:\n                return response.text.strip()\n            else:\n                return None\n                \n        except Exception as e:\n            print(f\"Summarization error: {e}\")\n            return None\n    \n    def process_youtube_video(self, youtube_url: str, word_count: int = 200) -> Optional[Dict]:\n        \"\"\"\n        Complete pipeline: download, transcribe, and summarize YouTube video\n        Returns: {'title': title, 'summary': summary}\n        \"\"\"\n        try:\n            download_result = self.download_video(youtube_url)\n            if not download_result:\n                return None\n            \n            video_path = download_result['video_path']\n            audio_path = download_result['audio_path']\n            title = download_result['title']\n            \n            transcript = self.transcribe_audio_in_chunks(audio_path)\n            \n            if os.path.exists(video_path):\n                os.unlink(video_path)\n            if os.path.exists(audio_path):\n                os.unlink(audio_path)\n            \n            if not transcript:\n                return None\n            \n            summary = self.summarize_text(transcript, word_count)\n            \n            if not summary:\n                return None\n            \n            return {\n                'title': title,\n                'summary': summary,\n                'transcript': transcript\n            }\n            \n        except Exception as e:\n            print(f\"Error processing YouTube video: {e}\")\n            return None\n","size_bytes":5790},"article_to_podcast.py":{"content":"import os\nimport tempfile\nimport subprocess\nfrom typing import Optional, Dict, Callable\nfrom elevenlabs import ElevenLabs, VoiceSettings\nfrom google import genai\n\nclass ArticleToPodcast:\n    \"\"\"Handles conversion of articles to multi-speaker podcast audio\"\"\"\n    \n    def __init__(self, gemini_api_key: str, elevenlabs_api_key: str):\n        \"\"\"Initialize article to podcast service with Gemini and ElevenLabs APIs\"\"\"\n        self.gemini_client = genai.Client(api_key=gemini_api_key)\n        self.elevenlabs_client = ElevenLabs(api_key=elevenlabs_api_key)\n        \n        # Voice mapping for different speakers\n        self.host_voice_id = \"pNInz6obpgDQGcFmaJgB\"    # Adam - Host voice\n        self.expert_voice_id = \"21m00Tcm4TlvDq8ikWAM\"  # Rachel - Expert voice\n    \n    def generate_podcast_script(self, article_text: str, word_count: int = 300) -> Optional[str]:\n        \"\"\"\n        Generate a podcast script from article text using Gemini\n        Returns: Formatted script with Host: and Expert: labels\n        \"\"\"\n        try:\n            prompt = f\"\"\"\nSummarize the following article into a detailed podcast script (~{word_count} words) as a conversation:\n- Include a \"Host\" and an \"Expert\".\n- Each line should start with \"Host:\" or \"Expert:\".\n- Make it engaging, explanatory, and conversational.\n- The Host should introduce the topic and ask questions.\n- The Expert should provide insights and explanations.\n- Make it sound natural and informative.\n\nArticle:\n---\n{article_text}\n---\n\nGenerate the podcast script:\n\"\"\"\n            \n            response = self.gemini_client.models.generate_content(\n                model=\"gemini-2.0-flash-exp\",\n                contents=prompt\n            )\n            \n            if response and response.text:\n                return response.text.strip()\n            else:\n                # Fallback script\n                return self._generate_fallback_script(article_text)\n                \n        except Exception as e:\n            print(f\"Error generating podcast script: {e}\")\n            return self._generate_fallback_script(article_text)\n    \n    def _generate_fallback_script(self, article_text: str) -> str:\n        \"\"\"Generate a simple fallback script if AI generation fails\"\"\"\n        words = article_text.split()[:100]\n        summary = ' '.join(words)\n        \n        return f\"\"\"Host: Welcome to our podcast. Today, we're discussing an important article.\nExpert: {summary}\nHost: That's very interesting. Can you tell us more about this?\nExpert: Certainly. This article highlights several key points that deserve attention.\nHost: Thank you for sharing these insights with us today.\nExpert: My pleasure. It's important to stay informed about these developments.\"\"\"\n    \n    def generate_speaker_audio(self, text: str, voice_id: str, output_file: str) -> bool:\n        \"\"\"\n        Generate audio for a single speaker line using ElevenLabs\n        Returns: True if successful, False otherwise\n        \"\"\"\n        try:\n            audio_generator = self.elevenlabs_client.text_to_speech.convert(\n                text=text,\n                voice_id=voice_id,\n                model_id=\"eleven_multilingual_v2\",\n                output_format=\"mp3_44100_128\",\n                voice_settings=VoiceSettings(\n                    stability=0.5,\n                    similarity_boost=0.75\n                )\n            )\n            \n            with open(output_file, 'wb') as f:\n                for chunk in audio_generator:\n                    f.write(chunk)\n            \n            return True\n            \n        except Exception as e:\n            print(f\"Error generating audio: {e}\")\n            return False\n    \n    def merge_audio_files(self, audio_files: list, output_file: str) -> bool:\n        \"\"\"\n        Merge multiple audio files into a single file using FFmpeg\n        Returns: True if successful, False otherwise\n        \"\"\"\n        try:\n            # Create a temporary file list for FFmpeg\n            with tempfile.NamedTemporaryFile(mode='w', delete=False, suffix='.txt') as f:\n                for audio_file in audio_files:\n                    f.write(f\"file '{audio_file}'\\n\")\n                file_list_path = f.name\n            \n            # Use FFmpeg to concatenate audio files\n            subprocess.run([\n                \"ffmpeg\", \"-y\", \"-f\", \"concat\", \"-safe\", \"0\",\n                \"-i\", file_list_path, \"-c\", \"copy\", output_file\n            ], check=True, capture_output=True)\n            \n            # Clean up the file list\n            os.unlink(file_list_path)\n            return True\n            \n        except Exception as e:\n            print(f\"Error merging audio files: {e}\")\n            return False\n    \n    def create_podcast_from_article(self, article_text: str, script_word_count: int = 300, \n                                   progress_callback: Optional[Callable] = None) -> Optional[bytes]:\n        \"\"\"\n        Complete workflow to convert article to podcast audio\n        Returns: Audio bytes if successful, None otherwise\n        \"\"\"\n        try:\n            # Step 1: Generate script\n            if progress_callback:\n                progress_callback(\"Generating podcast script...\", 20)\n            \n            script = self.generate_podcast_script(article_text, script_word_count)\n            if not script:\n                return None\n            \n            # Step 2: Parse script and generate audio for each line\n            if progress_callback:\n                progress_callback(\"Generating audio for speakers...\", 40)\n            \n            lines = script.strip().split('\\n')\n            audio_files = []\n            temp_files = []\n            \n            for i, line in enumerate(lines):\n                line = line.strip()\n                if not line:\n                    continue\n                \n                speaker, text, voice_id = \"\", \"\", \"\"\n                \n                if line.lower().startswith(\"host:\"):\n                    speaker, text, voice_id = \"Host\", line[5:].strip(), self.host_voice_id\n                elif line.lower().startswith(\"expert:\"):\n                    speaker, text, voice_id = \"Expert\", line[7:].strip(), self.expert_voice_id\n                else:\n                    continue\n                \n                if not text:\n                    continue\n                \n                # Generate audio file\n                temp_audio = tempfile.mktemp(suffix=f'_speaker_{i}.mp3')\n                temp_files.append(temp_audio)\n                \n                if self.generate_speaker_audio(text, voice_id, temp_audio):\n                    audio_files.append(temp_audio)\n                \n                # Update progress\n                if progress_callback:\n                    progress = 40 + int((i / len(lines)) * 40)\n                    progress_callback(f\"Processing speaker {i+1}/{len(lines)}...\", progress)\n            \n            if not audio_files:\n                return None\n            \n            # Step 3: Merge audio files\n            if progress_callback:\n                progress_callback(\"Merging audio segments...\", 85)\n            \n            final_output = tempfile.mktemp(suffix='_podcast.mp3')\n            temp_files.append(final_output)\n            \n            if not self.merge_audio_files(audio_files, final_output):\n                return None\n            \n            # Step 4: Read the final audio file\n            if progress_callback:\n                progress_callback(\"Finalizing podcast...\", 95)\n            \n            with open(final_output, 'rb') as f:\n                audio_bytes = f.read()\n            \n            # Clean up temporary files\n            for temp_file in temp_files:\n                try:\n                    if os.path.exists(temp_file):\n                        os.unlink(temp_file)\n                except:\n                    pass\n            \n            if progress_callback:\n                progress_callback(\"Complete!\", 100)\n            \n            return audio_bytes\n            \n        except Exception as e:\n            print(f\"Error creating podcast: {e}\")\n            return None\n","size_bytes":8106},"frontend/eslint.config.js":{"content":"import js from '@eslint/js'\nimport globals from 'globals'\nimport reactHooks from 'eslint-plugin-react-hooks'\nimport reactRefresh from 'eslint-plugin-react-refresh'\nimport { defineConfig, globalIgnores } from 'eslint/config'\n\nexport default defineConfig([\n  globalIgnores(['dist']),\n  {\n    files: ['**/*.{js,jsx}'],\n    extends: [\n      js.configs.recommended,\n      reactHooks.configs['recommended-latest'],\n      reactRefresh.configs.vite,\n    ],\n    languageOptions: {\n      ecmaVersion: 2020,\n      globals: globals.browser,\n      parserOptions: {\n        ecmaVersion: 'latest',\n        ecmaFeatures: { jsx: true },\n        sourceType: 'module',\n      },\n    },\n    rules: {\n      'no-unused-vars': ['error', { varsIgnorePattern: '^[A-Z_]' }],\n    },\n  },\n])\n","size_bytes":763},"frontend/src/api.js":{"content":"import axios from 'axios';\n\nconst API_BASE_URL = '/api';\n\nexport const api = {\n  textToSpeech: async (text, voice) => {\n    const response = await axios.post(`${API_BASE_URL}/text-to-speech`, {\n      text,\n      voice\n    });\n    return response.data;\n  },\n\n  speechToText: async (audioFile) => {\n    const formData = new FormData();\n    formData.append('audio', audioFile);\n    const response = await axios.post(`${API_BASE_URL}/speech-to-text`, formData, {\n      headers: { 'Content-Type': 'multipart/form-data' }\n    });\n    return response.data;\n  },\n\n  textTranslation: async (text, fromLang, toLang) => {\n    const response = await axios.post(`${API_BASE_URL}/text-translation`, {\n      text,\n      from_lang: fromLang,\n      to_lang: toLang\n    });\n    return response.data;\n  },\n\n  youtubeSummary: async (url, wordCount) => {\n    const response = await axios.post(`${API_BASE_URL}/youtube-summary`, {\n      url,\n      word_count: wordCount\n    });\n    return response.data;\n  },\n\n  wordToStory: async (words, theme, wordCount, language) => {\n    const response = await axios.post(`${API_BASE_URL}/word-to-story`, {\n      words,\n      theme,\n      word_count: wordCount,\n      language\n    });\n    return response.data;\n  },\n\n  articleToPodcast: async (articleText, scriptWordCount) => {\n    const response = await axios.post(`${API_BASE_URL}/article-to-podcast`, {\n      article_text: articleText,\n      script_word_count: scriptWordCount\n    });\n    return response.data;\n  },\n\n  startVideoDubbing: async (videoFile, sourceLang, targetLang) => {\n    const formData = new FormData();\n    formData.append('video', videoFile);\n    formData.append('source_lang', sourceLang);\n    formData.append('target_lang', targetLang);\n    const response = await axios.post(`${API_BASE_URL}/video-dubbing/start`, formData, {\n      headers: { 'Content-Type': 'multipart/form-data' }\n    });\n    return response.data;\n  },\n\n  getDubbingStatus: async (dubbingId) => {\n    const response = await axios.get(`${API_BASE_URL}/video-dubbing/status/${dubbingId}`);\n    return response.data;\n  },\n\n  downloadDubbedVideo: async (dubbingId, targetLang) => {\n    const response = await axios.get(`${API_BASE_URL}/video-dubbing/download/${dubbingId}`, {\n      params: { target_lang: targetLang },\n      responseType: 'blob'\n    });\n    return response.data;\n  }\n};\n","size_bytes":2344},"backend.py":{"content":"from flask import Flask, request, jsonify, send_file\nfrom flask_cors import CORS\nfrom flask_jwt_extended import JWTManager, create_access_token, jwt_required, get_jwt_identity\nimport tempfile\nimport os\nfrom pathlib import Path\n\n# Load environment variables from .env file\ntry:\n    from dotenv import load_dotenv\n    load_dotenv()\nexcept ImportError:\n    print(\"Warning: python-dotenv not installed. Install it with: pip install python-dotenv\")\nimport time\nfrom video_processor import VideoProcessor\nfrom elevenlabs_dubbing import ElevenLabsDubbing\nfrom utils import format_time, validate_video_file\nimport speech_recognition as sr\nfrom elevenlabs import ElevenLabs\nfrom google import genai\nfrom pydub import AudioSegment\nfrom youtube_summarizer import YouTubeSummarizer\nfrom story_generator import StoryGenerator\nfrom article_to_podcast import ArticleToPodcast\nimport base64\nfrom werkzeug.utils import secure_filename\nimport io\nfrom models import db, bcrypt, User, UserHistory\nfrom datetime import timedelta\n\napp = Flask(__name__)\nCORS(app)\n\napp.config['SQLALCHEMY_DATABASE_URI'] = 'sqlite:///anuvaad_ai.db'\napp.config['SQLALCHEMY_TRACK_MODIFICATIONS'] = False\napp.config['JWT_SECRET_KEY'] = os.environ.get('JWT_SECRET_KEY', 'dev-secret-key-change-in-production')\napp.config['JWT_ACCESS_TOKEN_EXPIRES'] = timedelta(days=30)\n\ndb.init_app(app)\nbcrypt.init_app(app)\njwt = JWTManager(app)\n\nwith app.app_context():\n    db.create_all()\n\nelevenlabs_api_key = os.environ.get('ELEVENLABS_API_KEY')\ngemini_api_key = os.environ.get('GEMINI_API_KEY')\n\nvideo_processor = VideoProcessor()\ndubbing_service = ElevenLabsDubbing(api_key=elevenlabs_api_key) if elevenlabs_api_key else None\nelevenlabs_client = ElevenLabs(api_key=elevenlabs_api_key) if elevenlabs_api_key else None\ngemini_client = genai.Client(api_key=gemini_api_key) if gemini_api_key else None\nyoutube_summarizer = YouTubeSummarizer(gemini_api_key=gemini_api_key) if gemini_api_key else None\nstory_generator = StoryGenerator(gemini_api_key=gemini_api_key, elevenlabs_api_key=elevenlabs_api_key) if gemini_api_key and elevenlabs_api_key else None\narticle_podcast = ArticleToPodcast(gemini_api_key=gemini_api_key, elevenlabs_api_key=elevenlabs_api_key) if gemini_api_key and elevenlabs_api_key else None\n\ndubbing_projects = {}\n\n@app.route('/api/health', methods=['GET'])\ndef health_check():\n    return jsonify({'status': 'ok', 'message': 'Backend is running'})\n\n@app.route('/api/auth/signup', methods=['POST'])\ndef signup():\n    try:\n        data = request.json\n        name = data.get('name')\n        email = data.get('email')\n        password = data.get('password')\n        \n        if not name or not email or not password:\n            return jsonify({'error': 'Name, email, and password are required'}), 400\n        \n        if User.query.filter_by(email=email).first():\n            return jsonify({'error': 'Email already exists'}), 400\n        \n        user = User(name=name, email=email)\n        user.set_password(password)\n        \n        db.session.add(user)\n        db.session.commit()\n        \n        access_token = create_access_token(identity=str(user.id))\n        \n        return jsonify({\n            'success': True,\n            'message': 'User created successfully',\n            'access_token': access_token,\n            'user': user.to_dict()\n        }), 201\n        \n    except Exception as e:\n        db.session.rollback()\n        return jsonify({'error': str(e)}), 500\n\n@app.route('/api/auth/login', methods=['POST'])\ndef login():\n    try:\n        data = request.json\n        email = data.get('email')\n        password = data.get('password')\n        \n        if not email or not password:\n            return jsonify({'error': 'Email and password are required'}), 400\n        \n        user = User.query.filter_by(email=email).first()\n        \n        if not user or not user.check_password(password):\n            return jsonify({'error': 'Invalid email or password'}), 401\n        \n        access_token = create_access_token(identity=str(user.id))\n        \n        return jsonify({\n            'success': True,\n            'access_token': access_token,\n            'user': user.to_dict()\n        })\n        \n    except Exception as e:\n        return jsonify({'error': str(e)}), 500\n\n@app.route('/api/auth/me', methods=['GET'])\n@jwt_required()\ndef get_current_user():\n    try:\n        user_id = int(get_jwt_identity())\n        user = User.query.get(user_id)\n        \n        if not user:\n            return jsonify({'error': 'User not found'}), 404\n        \n        return jsonify({\n            'success': True,\n            'user': user.to_dict()\n        })\n        \n    except Exception as e:\n        return jsonify({'error': str(e)}), 500\n\n@app.route('/api/history', methods=['GET', 'POST'])\n@jwt_required()\ndef user_history():\n    try:\n        user_id = int(get_jwt_identity())\n        \n        if request.method == 'POST':\n            data = request.json\n            feature_type = data.get('feature_type')\n            feature_data = data.get('feature_data')\n            \n            history_entry = UserHistory(\n                user_id=user_id,\n                feature_type=feature_type,\n                feature_data=feature_data\n            )\n            \n            db.session.add(history_entry)\n            db.session.commit()\n            \n            return jsonify({\n                'success': True,\n                'message': 'History saved',\n                'history': history_entry.to_dict()\n            }), 201\n        \n        else:\n            history = UserHistory.query.filter_by(user_id=user_id).order_by(UserHistory.created_at.desc()).limit(50).all()\n            \n            return jsonify({\n                'success': True,\n                'history': [h.to_dict() for h in history]\n            })\n            \n    except Exception as e:\n        db.session.rollback()\n        return jsonify({'error': str(e)}), 500\n\n@app.route('/api/text-to-speech', methods=['POST'])\ndef text_to_speech():\n    try:\n        data = request.json\n        text = data.get('text')\n        voice = data.get('voice', 'Rachel')\n        \n        if not text:\n            return jsonify({'error': 'Text is required'}), 400\n        \n        voice_map = {\n            \"Rachel\": \"21m00Tcm4TlvDq8ikWAM\",\n            \"Adam\": \"pNInz6obpgDQGcFmaJgB\",\n            \"Antoni\": \"ErXwobaYiN019PkySvjV\",\n            \"Arnold\": \"VR6AewLTigWG4xSOukaG\",\n            \"Bella\": \"EXAVITQu4vr4xnSDxMaL\",\n            \"Domi\": \"AZnzlk1XvdvUeBnXmlld\",\n            \"Elli\": \"MF3mGyEYCl7XYWbV9V6O\",\n            \"Josh\": \"TxGEqnHWrfWFTfGW9XjX\",\n            \"Sam\": \"yoZ06aMxZJJ28mfd3POQ\"\n        }\n        \n        audio_generator = elevenlabs_client.text_to_speech.convert(\n            text=text,\n            voice_id=voice_map.get(voice, voice_map['Rachel']),\n            model_id=\"eleven_multilingual_v2\",\n            output_format=\"mp3_44100_128\"\n        )\n        \n        audio_bytes = b''.join(audio_generator)\n        audio_base64 = base64.b64encode(audio_bytes).decode('utf-8')\n        \n        return jsonify({\n            'success': True,\n            'audio': audio_base64,\n            'filename': f'tts_{int(time.time())}.mp3'\n        })\n    except Exception as e:\n        return jsonify({'error': str(e)}), 500\n\n@app.route('/api/speech-to-text', methods=['POST'])\ndef speech_to_text():\n    try:\n        if 'audio' not in request.files:\n            return jsonify({'error': 'Audio file is required'}), 400\n        \n        audio_file = request.files['audio']\n        file_extension = audio_file.filename.split('.')[-1].lower()\n        \n        with tempfile.NamedTemporaryFile(delete=False, suffix=f'.{file_extension}') as tmp_input:\n            audio_file.save(tmp_input.name)\n            input_path = tmp_input.name\n        \n        wav_path = tempfile.mktemp(suffix='.wav')\n        \n        try:\n            audio = AudioSegment.from_file(input_path, format=file_extension)\n            audio.export(wav_path, format='wav')\n        except Exception as e:\n            os.unlink(input_path)\n            return jsonify({'error': f'Failed to convert audio: {str(e)}'}), 500\n        \n        recognizer = sr.Recognizer()\n        \n        try:\n            with sr.AudioFile(wav_path) as source:\n                audio_data = recognizer.record(source)\n                text = recognizer.recognize_google(audio_data)\n        finally:\n            os.unlink(input_path)\n            if os.path.exists(wav_path):\n                os.unlink(wav_path)\n        \n        return jsonify({\n            'success': True,\n            'text': text\n        })\n    except sr.UnknownValueError:\n        return jsonify({'error': 'Could not understand audio'}), 400\n    except Exception as e:\n        return jsonify({'error': str(e)}), 500\n\n@app.route('/api/text-translation', methods=['POST'])\ndef text_translation():\n    try:\n        data = request.json\n        text = data.get('text')\n        from_lang = data.get('from_lang')\n        to_lang = data.get('to_lang')\n        \n        if not text or not from_lang or not to_lang:\n            return jsonify({'error': 'Text, from_lang, and to_lang are required'}), 400\n        \n        if from_lang == to_lang:\n            return jsonify({'error': 'Source and target languages must be different'}), 400\n        \n        prompt = f\"Translate the following text from {from_lang} to {to_lang}. Only provide the translation, no explanations:\\n\\n{text}\"\n        \n        response = gemini_client.models.generate_content(\n            model=\"gemini-2.0-flash-exp\",\n            contents=prompt\n        )\n        translated_text = response.text\n        \n        return jsonify({\n            'success': True,\n            'translated_text': translated_text,\n            'from_lang': from_lang,\n            'to_lang': to_lang\n        })\n    except Exception as e:\n        return jsonify({'error': str(e)}), 500\n\n@app.route('/api/youtube-summary', methods=['POST'])\ndef youtube_summary():\n    try:\n        data = request.json\n        youtube_url = data.get('url')\n        word_count = data.get('word_count', 200)\n        \n        if not youtube_url:\n            return jsonify({'error': 'YouTube URL is required'}), 400\n        \n        result = youtube_summarizer.process_youtube_video(youtube_url, word_count)\n        \n        if result:\n            return jsonify({\n                'success': True,\n                'title': result['title'],\n                'summary': result['summary']\n            })\n        else:\n            return jsonify({'error': 'Failed to process video'}), 500\n    except Exception as e:\n        return jsonify({'error': str(e)}), 500\n\n@app.route('/api/word-to-story', methods=['POST'])\ndef word_to_story():\n    try:\n        data = request.json\n        words = data.get('words')\n        theme = data.get('theme')\n        word_count = data.get('word_count', 300)\n        language = data.get('language', 'english')\n        \n        if not words or not theme:\n            return jsonify({'error': 'Words and theme are required'}), 400\n        \n        words_list = [word.strip() for word in words.split(',')]\n        \n        result = story_generator.create_story_with_audio(\n            words=words_list,\n            theme=theme,\n            word_count=word_count,\n            language=language.lower()\n        )\n        \n        if result and result['story']:\n            response_data = {\n                'success': True,\n                'story': result['story']\n            }\n            \n            if result['audio_bytes']:\n                audio_base64 = base64.b64encode(result['audio_bytes']).decode('utf-8')\n                response_data['audio'] = audio_base64\n                response_data['filename'] = f'story_{int(time.time())}.mp3'\n            \n            return jsonify(response_data)\n        else:\n            return jsonify({'error': 'Failed to generate story'}), 500\n    except Exception as e:\n        return jsonify({'error': str(e)}), 500\n\n@app.route('/api/article-to-podcast', methods=['POST'])\ndef article_to_podcast():\n    try:\n        data = request.json\n        article_text = data.get('article_text')\n        script_word_count = data.get('script_word_count', 300)\n        \n        if not article_text:\n            return jsonify({'error': 'Article text is required'}), 400\n        \n        audio_bytes = article_podcast.create_podcast_from_article(\n            article_text=article_text,\n            script_word_count=script_word_count,\n            progress_callback=None\n        )\n        \n        if audio_bytes:\n            audio_base64 = base64.b64encode(audio_bytes).decode('utf-8')\n            return jsonify({\n                'success': True,\n                'audio': audio_base64,\n                'filename': f'podcast_{int(time.time())}.mp3'\n            })\n        else:\n            return jsonify({'error': 'Failed to generate podcast'}), 500\n    except Exception as e:\n        return jsonify({'error': str(e)}), 500\n\n@app.route('/api/video-dubbing/start', methods=['POST'])\ndef start_video_dubbing():\n    try:\n        if 'video' not in request.files:\n            return jsonify({'error': 'Video file is required'}), 400\n        \n        video_file = request.files['video']\n        source_lang = request.form.get('source_lang', 'en')\n        target_lang = request.form.get('target_lang', 'hi')\n        \n        with tempfile.NamedTemporaryFile(delete=False, suffix='.mp4') as tmp_file:\n            video_file.save(tmp_file.name)\n            input_video_path = tmp_file.name\n        \n        dubbing_id = dubbing_service.create_dubbing_project(\n            video_path=input_video_path,\n            source_lang=source_lang,\n            target_lang=target_lang,\n            project_name=f\"Dubbing_{int(time.time())}\"\n        )\n        \n        if dubbing_id:\n            dubbing_projects[dubbing_id] = {\n                'source_lang': source_lang,\n                'target_lang': target_lang,\n                'status': 'processing'\n            }\n            \n            return jsonify({\n                'success': True,\n                'dubbing_id': dubbing_id\n            })\n        else:\n            os.unlink(input_video_path)\n            return jsonify({'error': 'Failed to start dubbing'}), 500\n    except Exception as e:\n        return jsonify({'error': str(e)}), 500\n\n@app.route('/api/video-dubbing/status/<dubbing_id>', methods=['GET'])\ndef get_dubbing_status(dubbing_id):\n    try:\n        status_result = dubbing_service.get_dubbing_status(dubbing_id)\n        \n        return jsonify({\n            'success': True,\n            'status': status_result['status'],\n            'dubbing_id': dubbing_id\n        })\n    except Exception as e:\n        return jsonify({'error': str(e)}), 500\n\n@app.route('/api/video-dubbing/download/<dubbing_id>', methods=['GET'])\ndef download_dubbed_video(dubbing_id):\n    try:\n        target_lang = request.args.get('target_lang', 'hi')\n        \n        dubbed_video_path = dubbing_service.download_dubbed_video(dubbing_id, target_lang)\n        \n        if dubbed_video_path and os.path.exists(dubbed_video_path):\n            return send_file(\n                dubbed_video_path,\n                as_attachment=True,\n                download_name=f'dubbed_video_{int(time.time())}.mp4',\n                mimetype='video/mp4'\n            )\n        else:\n            return jsonify({'error': 'Failed to download dubbed video'}), 500\n    except Exception as e:\n        return jsonify({'error': str(e)}), 500\n\n@app.route('/attached_assets/<path:filename>')\ndef serve_attached_assets(filename):\n    try:\n        return send_file(os.path.join('attached_assets', filename))\n    except Exception as e:\n        return jsonify({'error': str(e)}), 404\n\nif __name__ == '__main__':\n    app.run(host='0.0.0.0', port=5001, debug=False, use_reloader=False)\n","size_bytes":15786},"frontend/src/App.css":{"content":".app-netflix {\n  background: #141414;\n  min-height: 100vh;\n  color: #fff;\n}\n\n.features-section {\n  padding: 3rem 0;\n  background: #141414;\n}\n\n.active-feature-section {\n  padding: 3rem 4%;\n  background: #1a1a1a;\n  min-height: 400px;\n}\n\n.active-feature-container {\n  max-width: 1200px;\n  margin: 0 auto;\n  background: #2d2d2d;\n  border-radius: 8px;\n  padding: 2rem;\n}\n\n.close-feature-btn {\n  background: transparent;\n  border: 1px solid #808080;\n  color: #e5e5e5;\n  padding: 0.75rem 1.5rem;\n  border-radius: 4px;\n  cursor: pointer;\n  font-size: 1rem;\n  margin-bottom: 2rem;\n  transition: all 0.3s ease;\n}\n\n.close-feature-btn:hover {\n  background: #E50914;\n  border-color: #E50914;\n  color: white;\n}\n\n.feature-content {\n  margin-top: 1rem;\n}\n\n.input-field, textarea, select {\n  width: 100%;\n  padding: 0.8rem 1rem;\n  margin: 0.5rem 0;\n  background: #333;\n  border: 1px solid #555;\n  border-radius: 4px;\n  color: #fff;\n  font-size: 1rem;\n  font-family: 'Inter', sans-serif;\n  transition: all 0.3s ease;\n}\n\n.input-field:focus, textarea:focus, select:focus {\n  outline: none;\n  border-color: #E50914;\n  background: #454545;\n}\n\n.file-input {\n  width: 100%;\n  padding: 1rem;\n  margin: 0.5rem 0;\n  background: #333;\n  border: 2px dashed #555;\n  border-radius: 4px;\n  color: #fff;\n  cursor: pointer;\n  transition: all 0.3s ease;\n}\n\n.file-input:hover {\n  border-color: #E50914;\n  background: #454545;\n}\n\n.primary-btn {\n  width: 100%;\n  padding: 1rem 2rem;\n  margin: 1rem 0;\n  background: #E50914;\n  border: none;\n  border-radius: 4px;\n  color: white;\n  font-size: 1.1rem;\n  font-weight: 600;\n  cursor: pointer;\n  transition: all 0.3s ease;\n}\n\n.primary-btn:hover:not(:disabled) {\n  background: #f40612;\n  transform: translateY(-2px);\n}\n\n.primary-btn:disabled {\n  opacity: 0.6;\n  cursor: not-allowed;\n}\n\n.download-btn {\n  display: inline-block;\n  padding: 0.8rem 1.5rem;\n  margin: 0.5rem 0;\n  background: #E50914;\n  border: none;\n  border-radius: 4px;\n  color: white;\n  text-decoration: none;\n  font-weight: 600;\n  cursor: pointer;\n  transition: all 0.3s ease;\n}\n\n.download-btn:hover {\n  background: #f40612;\n  transform: translateY(-2px);\n}\n\n.error-message {\n  padding: 1rem;\n  margin: 1rem 0;\n  background: rgba(229, 9, 20, 0.1);\n  border: 1px solid rgba(229, 9, 20, 0.3);\n  border-radius: 4px;\n  color: #ff6b6b;\n}\n\n.success-message {\n  padding: 1rem;\n  margin: 1rem 0;\n  background: rgba(46, 213, 115, 0.1);\n  border: 1px solid rgba(46, 213, 115, 0.3);\n  border-radius: 4px;\n  color: #6ee7b7;\n}\n\n.info-message {\n  padding: 1rem;\n  margin: 1rem 0;\n  background: rgba(96, 165, 250, 0.1);\n  border: 1px solid rgba(96, 165, 250, 0.3);\n  border-radius: 4px;\n  color: #93c5fd;\n}\n\n.result-box {\n  margin: 1rem 0;\n}\n\n.result-box h4 {\n  margin-bottom: 0.5rem;\n  color: #e5e5e5;\n}\n\n.result-box textarea {\n  width: 100%;\n  background: #333;\n  border: 1px solid #555;\n  border-radius: 4px;\n  color: #fff;\n  padding: 1rem;\n  font-family: 'Inter', sans-serif;\n}\n\n.audio-player {\n  margin: 1rem 0;\n  padding: 1rem;\n  background: #333;\n  border-radius: 4px;\n}\n\n.audio-player audio {\n  width: 100%;\n  margin: 0.5rem 0;\n}\n\n.slider-container {\n  margin: 1rem 0;\n}\n\n.slider-container label {\n  display: block;\n  margin-bottom: 0.5rem;\n  color: #e5e5e5;\n}\n\n.slider-container input[type=\"range\"] {\n  width: 100%;\n  height: 8px;\n  background: #555;\n  border-radius: 4px;\n  outline: none;\n}\n\n.slider-container input[type=\"range\"]::-webkit-slider-thumb {\n  appearance: none;\n  width: 20px;\n  height: 20px;\n  background: #E50914;\n  border-radius: 50%;\n  cursor: pointer;\n}\n\n.language-selectors {\n  display: grid;\n  grid-template-columns: 1fr 1fr;\n  gap: 1rem;\n  margin: 1rem 0;\n}\n\n.language-selector-duo {\n  display: grid;\n  grid-template-columns: 1fr 1fr;\n  gap: 1rem;\n  margin: 1rem 0;\n}\n\n.language-selector-duo label {\n  display: block;\n  margin-bottom: 0.5rem;\n  color: #e5e5e5;\n  font-weight: 600;\n}\n\n.dual-controls {\n  display: grid;\n  grid-template-columns: 2fr 1fr;\n  gap: 1rem;\n  margin: 1rem 0;\n  align-items: center;\n}\n\n.progress-container {\n  margin: 1rem 0;\n  background: #333;\n  border-radius: 4px;\n  overflow: hidden;\n  position: relative;\n  height: 40px;\n}\n\n.progress-bar {\n  height: 100%;\n  background: #E50914;\n  transition: width 0.3s ease;\n}\n\n.status-text {\n  position: absolute;\n  top: 50%;\n  left: 50%;\n  transform: translate(-50%, -50%);\n  color: white;\n  font-weight: 600;\n}\n\n.video-title {\n  margin: 1rem 0;\n  color: #e5e5e5;\n}\n\n/* Search Results Section */\n.search-results-section {\n  padding: 3rem 4%;\n  background: #1a1a1a;\n  min-height: 300px;\n}\n\n.search-results-container {\n  max-width: 1400px;\n  margin: 0 auto;\n}\n\n.search-results-header {\n  display: flex;\n  justify-content: space-between;\n  align-items: center;\n  margin-bottom: 2rem;\n}\n\n.search-results-header h2 {\n  color: #e5e5e5;\n  font-size: 1.8rem;\n}\n\n.clear-search-btn {\n  background: #E50914;\n  border: none;\n  color: white;\n  padding: 0.75rem 1.5rem;\n  border-radius: 4px;\n  cursor: pointer;\n  font-size: 1rem;\n  transition: all 0.3s ease;\n}\n\n.clear-search-btn:hover {\n  background: #f40612;\n  transform: scale(1.05);\n}\n\n.search-results-grid {\n  display: grid;\n  grid-template-columns: repeat(auto-fill, minmax(280px, 1fr));\n  gap: 1.5rem;\n}\n\n.search-result-card {\n  background: #2d2d2d;\n  border-radius: 8px;\n  padding: 2rem;\n  cursor: pointer;\n  transition: all 0.3s ease;\n  border: 2px solid transparent;\n}\n\n.search-result-card:hover {\n  transform: translateY(-5px);\n  border-color: #E50914;\n  background: #3d3d3d;\n}\n\n.search-result-icon {\n  font-size: 3rem;\n  margin-bottom: 1rem;\n}\n\n.search-result-card h3 {\n  color: #e5e5e5;\n  margin-bottom: 0.5rem;\n  font-size: 1.3rem;\n}\n\n.search-result-card p {\n  color: #b3b3b3;\n  font-size: 0.95rem;\n  line-height: 1.5;\n}\n\n.no-results {\n  text-align: center;\n  padding: 3rem;\n  color: #b3b3b3;\n}\n\n.no-results p {\n  font-size: 1.2rem;\n  margin-bottom: 1.5rem;\n}\n\n.btn-clear {\n  background: #E50914;\n  border: none;\n  color: white;\n  padding: 0.75rem 2rem;\n  border-radius: 4px;\n  cursor: pointer;\n  font-size: 1rem;\n  transition: all 0.3s ease;\n}\n\n.btn-clear:hover {\n  background: #f40612;\n}\n\n@media (max-width: 768px) {\n  .features-section {\n    padding: 2rem 0;\n  }\n  \n  .active-feature-section {\n    padding: 2rem 4%;\n  }\n  \n  .active-feature-container {\n    padding: 1.5rem;\n  }\n  \n  .language-selectors,\n  .language-selector-duo,\n  .dual-controls {\n    grid-template-columns: 1fr;\n  }\n\n  .search-results-header {\n    flex-direction: column;\n    gap: 1rem;\n    align-items: flex-start;\n  }\n\n  .search-results-grid {\n    grid-template-columns: 1fr;\n  }\n}\n","size_bytes":6557},"frontend/src/main.jsx":{"content":"import { StrictMode } from 'react'\nimport { createRoot } from 'react-dom/client'\nimport './index.css'\nimport App from './App.jsx'\nimport { AuthProvider } from './AuthContext'\n\ncreateRoot(document.getElementById('root')).render(\n  <StrictMode>\n    <AuthProvider>\n      <App />\n    </AuthProvider>\n  </StrictMode>,\n)\n","size_bytes":315},"frontend/src/App.jsx":{"content":"import { useState } from 'react';\nimport { useAuth } from './AuthContext';\nimport './App.css';\nimport Header from './components/Header';\nimport Hero from './components/Hero';\nimport SearchBar from './components/SearchBar';\nimport FeatureRow from './components/FeatureRow';\nimport Reasons from './components/Reasons';\nimport FAQ from './components/FAQ';\nimport Footer from './components/Footer';\nimport FeatureModal from './components/FeatureModal';\nimport LoginModal from './components/LoginModal';\nimport VideoDubbing from './components/VideoDubbing';\nimport YoutubeSummarizer from './components/YoutubeSummarizer';\nimport WordToStory from './components/WordToStory';\nimport ArticleToPodcast from './components/ArticleToPodcast';\nimport TextToSpeech from './components/TextToSpeech';\nimport SpeechToText from './components/SpeechToText';\nimport TextTranslation from './components/TextTranslation';\n\nfunction App() {\n  const [selectedFeature, setSelectedFeature] = useState(null);\n  const [activeComponent, setActiveComponent] = useState(null);\n  const [showLoginPrompt, setShowLoginPrompt] = useState(false);\n  const [searchResults, setSearchResults] = useState(null);\n  const [searchTerm, setSearchTerm] = useState('');\n  const { isAuthenticated } = useAuth();\n\n  const coreFeatures = [\n    {\n      icon: 'ğŸ¬',\n      title: 'Video Dubbing',\n      shortDesc: 'Professional AI-powered video dubbing across multiple languages',\n      description: 'Transform your videos with professional AI-powered dubbing. Our advanced technology maintains voice quality and emotional tone while translating your content into multiple languages, making your videos accessible to global audiences.',\n      details: [\n        'Support for 50+ languages',\n        'Natural voice synthesis',\n        'Lip-sync optimization',\n        'High-quality audio output',\n        'Batch processing support'\n      ],\n      image: '/attached_assets/stock_images/video_dubbing_microp_c08e1bb8.jpg',\n      component: VideoDubbing,\n      id: 'video-dubbing'\n    },\n    {\n      icon: 'ğŸ“º',\n      title: 'YouTube Summarizer',\n      shortDesc: 'Get instant AI-generated summaries of YouTube videos',\n      description: 'Save time with instant AI-generated summaries of YouTube videos. Our intelligent system extracts key points, main topics, and essential information, allowing you to grasp video content in minutes instead of hours.',\n      details: [\n        'Accurate content extraction',\n        'Key point identification',\n        'Timestamp references',\n        'Multi-language support',\n        'Customizable summary length'\n      ],\n      image: '/attached_assets/stock_images/youtube_video_conten_6e650ebb.jpg',\n      component: YoutubeSummarizer,\n      id: 'youtube-summarizer'\n    },\n    {\n      icon: 'ğŸ“–',\n      title: 'Word to Story',\n      shortDesc: 'Transform simple words into engaging creative stories',\n      description: 'Unleash your creativity by transforming simple words or phrases into rich, engaging stories. Our AI storyteller uses advanced language models to craft compelling narratives that capture imagination and deliver meaningful content.',\n      details: [\n        'Creative narrative generation',\n        'Multiple genre options',\n        'Customizable story length',\n        'Character development',\n        'Plot structure optimization'\n      ],\n      image: '/attached_assets/stock_images/creative_storytellin_0ade6655.jpg',\n      component: WordToStory,\n      id: 'word-to-story'\n    },\n    {\n      icon: 'ğŸ™ï¸',\n      title: 'Article to Podcast',\n      shortDesc: 'Convert written articles into engaging audio podcasts',\n      description: 'Convert your written articles and blog posts into professional audio podcasts. Perfect for content creators who want to reach audiences on-the-go with natural-sounding voice narration and engaging audio presentation.',\n      details: [\n        'Natural voice narration',\n        'Multiple voice options',\n        'Background music support',\n        'Professional audio quality',\n        'MP3 export format'\n      ],\n      image: '/attached_assets/stock_images/podcast_microphone_a_4eec9237.jpg',\n      component: ArticleToPodcast,\n      id: 'article-to-podcast'\n    }\n  ];\n\n  const advancedTools = [\n    {\n      icon: 'ğŸ—£ï¸',\n      title: 'Text to Speech',\n      shortDesc: 'Convert text into natural-sounding speech',\n      description: 'Convert any text into natural-sounding speech with our advanced text-to-speech technology. Choose from multiple voices, accents, and languages to create professional voiceovers and audio content.',\n      details: [\n        'Multiple voice options',\n        'Natural intonation',\n        'Speed control',\n        'High-quality audio',\n        'Batch conversion'\n      ],\n      image: '/attached_assets/stock_images/text_to_speech_ai_te_2a30ee21.jpg',\n      component: TextToSpeech,\n      id: 'text-to-speech'\n    },\n    {\n      icon: 'ğŸ¤',\n      title: 'Speech to Text',\n      shortDesc: 'Transcribe audio into accurate text',\n      description: 'Accurately transcribe audio recordings into text with our speech recognition technology. Perfect for creating transcripts, subtitles, or converting voice notes into written format.',\n      details: [\n        'High accuracy transcription',\n        'Real-time processing',\n        'Multi-language support',\n        'Timestamp generation',\n        'Speaker identification'\n      ],\n      image: '/attached_assets/stock_images/speech_recognition_v_e7a309ac.jpg',\n      component: SpeechToText,\n      id: 'speech-to-text'\n    },\n    {\n      icon: 'ğŸŒ',\n      title: 'Text Translation',\n      shortDesc: 'Translate text between multiple languages instantly',\n      description: 'Break language barriers with instant text translation. Our AI-powered translation service supports multiple languages and maintains context and meaning for accurate, natural translations.',\n      details: [\n        '100+ language pairs',\n        'Context-aware translation',\n        'Instant results',\n        'Idiomatic expressions',\n        'Batch translation support'\n      ],\n      image: '/attached_assets/stock_images/translation_language_3cf3ff8e.jpg',\n      component: TextTranslation,\n      id: 'text-translation'\n    }\n  ];\n\n  const handleCardClick = (feature) => {\n    if (!isAuthenticated) {\n      setShowLoginPrompt(true);\n      return;\n    }\n    setSelectedFeature(feature);\n  };\n\n  const handleFeatureAction = (feature) => {\n    if (!isAuthenticated) {\n      setShowLoginPrompt(true);\n      return;\n    }\n    \n    setActiveComponent(feature.id);\n    setSelectedFeature(null);\n    \n    setTimeout(() => {\n      const element = document.getElementById('active-feature-section');\n      if (element) {\n        element.scrollIntoView({ behavior: 'smooth' });\n      }\n    }, 100);\n  };\n\n  const handleSearch = (term) => {\n    if (!term.trim()) {\n      setSearchResults(null);\n      setSearchTerm('');\n      return;\n    }\n\n    setSearchTerm(term);\n    const allFeatures = [...coreFeatures, ...advancedTools];\n    const results = allFeatures.filter(feature => \n      feature.title.toLowerCase().includes(term.toLowerCase()) ||\n      feature.shortDesc.toLowerCase().includes(term.toLowerCase()) ||\n      feature.description.toLowerCase().includes(term.toLowerCase())\n    );\n    \n    setSearchResults(results);\n    \n    setTimeout(() => {\n      const element = document.getElementById('search-results');\n      if (element) {\n        element.scrollIntoView({ behavior: 'smooth' });\n      }\n    }, 100);\n  };\n\n  const clearSearch = () => {\n    setSearchResults(null);\n    setSearchTerm('');\n  };\n\n  const getActiveComponent = () => {\n    const allFeatures = [...coreFeatures, ...advancedTools];\n    const feature = allFeatures.find(f => f.id === activeComponent);\n    return feature?.component || null;\n  };\n\n  const ActiveComponent = getActiveComponent();\n\n  return (\n    <div className=\"app-netflix\">\n      <Header onSearchChange={handleSearch} />\n      \n      <Hero />\n      \n      <SearchBar onSearch={handleSearch} />\n      \n      {searchResults !== null && (\n        <section id=\"search-results\" className=\"search-results-section\">\n          <div className=\"search-results-container\">\n            <div className=\"search-results-header\">\n              <h2>Search Results for \"{searchTerm}\"</h2>\n              <button className=\"clear-search-btn\" onClick={clearSearch}>\n                âœ• Clear Search\n              </button>\n            </div>\n            {searchResults.length > 0 ? (\n              <div className=\"search-results-grid\">\n                {searchResults.map((feature, idx) => (\n                  <div \n                    key={idx} \n                    className=\"search-result-card\"\n                    onClick={() => handleCardClick({ ...feature, action: () => handleFeatureAction(feature) })}\n                  >\n                    <div className=\"search-result-icon\">{feature.icon}</div>\n                    <h3>{feature.title}</h3>\n                    <p>{feature.shortDesc}</p>\n                  </div>\n                ))}\n              </div>\n            ) : (\n              <div className=\"no-results\">\n                <p>No features found matching \"{searchTerm}\"</p>\n                <button className=\"btn-clear\" onClick={clearSearch}>View All Features</button>\n              </div>\n            )}\n          </div>\n        </section>\n      )}\n      \n      <section id=\"features\" className=\"features-section\">\n        <FeatureRow \n          title=\"Core Features\" \n          features={coreFeatures}\n          onCardClick={(feature) => {\n            handleCardClick({ ...feature, action: () => handleFeatureAction(feature) });\n          }}\n        />\n        \n        <FeatureRow \n          title=\"Advanced Tools\" \n          features={advancedTools}\n          onCardClick={(feature) => {\n            handleCardClick({ ...feature, action: () => handleFeatureAction(feature) });\n          }}\n        />\n      </section>\n\n      {activeComponent && (\n        <section id=\"active-feature-section\" className=\"active-feature-section\">\n          <div className=\"active-feature-container\">\n            <button \n              className=\"close-feature-btn\" \n              onClick={() => setActiveComponent(null)}\n            >\n              â† Back to Features\n            </button>\n            {ActiveComponent && <ActiveComponent />}\n          </div>\n        </section>\n      )}\n      \n      <Reasons />\n      \n      <FAQ />\n      \n      <Footer />\n      \n      {selectedFeature && (\n        <FeatureModal \n          feature={selectedFeature} \n          onClose={() => setSelectedFeature(null)} \n        />\n      )}\n      \n      {showLoginPrompt && (\n        <LoginModal \n          onClose={() => setShowLoginPrompt(false)}\n          onSwitchToSignup={() => setShowLoginPrompt(false)}\n        />\n      )}\n    </div>\n  );\n}\n\nexport default App;\n","size_bytes":10885},"frontend/vite.config.js":{"content":"import { defineConfig } from 'vite'\nimport react from '@vitejs/plugin-react'\n\n// https://vite.dev/config/\nexport default defineConfig({\n  plugins: [react()],\n  server: {\n    host: '0.0.0.0',\n    port: 5000,\n    allowedHosts: ['.replit.dev'],\n    proxy: {\n      '/api': {\n        target: 'http://localhost:5001',\n        changeOrigin: true,\n      },\n      '/attached_assets': {\n        target: 'http://localhost:5001',\n        changeOrigin: true,\n      }\n    }\n  }\n})\n","size_bytes":467},"run-frontend.sh":{"content":"#!/bin/bash\ncd frontend\n\n# Install if node_modules doesn't exist\nif [ ! -d \"node_modules\" ]; then\n    echo \"Installing frontend dependencies (this may take a minute)...\"\n    npm install --prefer-offline --no-audit\n    if [ $? -ne 0 ]; then\n        echo \"Installation failed, trying again...\"\n        npm install\n    fi\nfi\n\necho \"Starting Vite dev server on port 5000...\"\nnpm run dev -- --host 0.0.0.0 --port 5000\n","size_bytes":413},"frontend/src/components/YoutubeSummarizer.jsx":{"content":"import { useState } from 'react';\nimport { api } from '../api';\n\nfunction YoutubeSummarizer() {\n  const [url, setUrl] = useState('');\n  const [wordCount, setWordCount] = useState(200);\n  const [loading, setLoading] = useState(false);\n  const [summary, setSummary] = useState('');\n  const [title, setTitle] = useState('');\n  const [error, setError] = useState(null);\n  const [success, setSuccess] = useState(false);\n\n  const handleSummarize = async () => {\n    if (!url.trim()) {\n      setError('Please enter a YouTube URL');\n      return;\n    }\n\n    setLoading(true);\n    setError(null);\n    setSuccess(false);\n    setSummary('');\n    setTitle('');\n\n    try {\n      const result = await api.youtubeSummary(url, wordCount);\n      setTitle(result.title);\n      setSummary(result.summary);\n      setSuccess(true);\n    } catch (err) {\n      setError(err.response?.data?.error || 'Failed to process video');\n    } finally {\n      setLoading(false);\n    }\n  };\n\n  return (\n    <div className=\"feature-content\">\n      <input\n        type=\"text\"\n        value={url}\n        onChange={(e) => setUrl(e.target.value)}\n        placeholder=\"https://www.youtube.com/watch?v=...\"\n        className=\"input-field\"\n      />\n      <div className=\"slider-container\">\n        <label>Summary word count: {wordCount}</label>\n        <input\n          type=\"range\"\n          min=\"50\"\n          max=\"500\"\n          step=\"50\"\n          value={wordCount}\n          onChange={(e) => setWordCount(Number(e.target.value))}\n        />\n      </div>\n      <button onClick={handleSummarize} disabled={loading} className=\"primary-btn\">\n        {loading ? 'Processing...' : 'ğŸ“ Summarize Video'}\n      </button>\n      {error && <div className=\"error-message\">{error}</div>}\n      {success && <div className=\"success-message\">âœ… Summary generated successfully!</div>}\n      {title && <h4 className=\"video-title\">ğŸ“¹ Video: {title}</h4>}\n      {summary && (\n        <div className=\"result-box\">\n          <h4>ğŸ“ Summary:</h4>\n          <textarea value={summary} readOnly rows={8} />\n        </div>\n      )}\n    </div>\n  );\n}\n\nexport default YoutubeSummarizer;\n","size_bytes":2127},"frontend/src/index.css":{"content":"@import url('https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700;800&display=swap');\n\n* {\n  margin: 0;\n  padding: 0;\n  box-sizing: border-box;\n  font-family: 'Inter', sans-serif;\n}\n\nbody {\n  margin: 0;\n  background: #0f172a;\n  color: rgba(255, 255, 255, 0.87);\n  min-height: 100vh;\n  font-synthesis: none;\n  text-rendering: optimizeLegibility;\n  -webkit-font-smoothing: antialiased;\n  -moz-osx-font-smoothing: grayscale;\n}\n\n#root {\n  width: 100%;\n  min-height: 100vh;\n}\n","size_bytes":490},"frontend/src/components/TextToSpeech.jsx":{"content":"import { useState } from 'react';\nimport { api } from '../api';\n\nfunction TextToSpeech() {\n  const [text, setText] = useState('');\n  const [voice, setVoice] = useState('Rachel');\n  const [loading, setLoading] = useState(false);\n  const [audioUrl, setAudioUrl] = useState(null);\n  const [error, setError] = useState(null);\n  const [success, setSuccess] = useState(false);\n\n  const voices = ['Rachel', 'Adam', 'Antoni', 'Arnold', 'Bella', 'Domi', 'Elli', 'Josh', 'Sam'];\n\n  const handleGenerate = async () => {\n    if (!text.trim()) {\n      setError('Please enter some text first');\n      return;\n    }\n\n    setLoading(true);\n    setError(null);\n    setSuccess(false);\n    setAudioUrl(null);\n\n    try {\n      const result = await api.textToSpeech(text, voice);\n      const audioBlob = new Blob(\n        [Uint8Array.from(atob(result.audio), c => c.charCodeAt(0))],\n        { type: 'audio/mpeg' }\n      );\n      const url = URL.createObjectURL(audioBlob);\n      setAudioUrl(url);\n      setSuccess(true);\n    } catch (err) {\n      setError(err.response?.data?.error || 'Failed to generate speech');\n    } finally {\n      setLoading(false);\n    }\n  };\n\n  return (\n    <div className=\"tool-content\">\n      <h3>ğŸ—£ï¸ Text to Speech</h3>\n      <textarea\n        value={text}\n        onChange={(e) => setText(e.target.value)}\n        placeholder=\"Type or paste your text here...\"\n        rows={6}\n      />\n      <select value={voice} onChange={(e) => setVoice(e.target.value)}>\n        {voices.map(v => (\n          <option key={v} value={v}>{v}</option>\n        ))}\n      </select>\n      <button onClick={handleGenerate} disabled={loading} className=\"primary-btn\">\n        {loading ? 'Generating...' : 'ğŸµ Generate Speech'}\n      </button>\n      {error && <div className=\"error-message\">{error}</div>}\n      {success && <div className=\"success-message\">âœ… Speech generated successfully!</div>}\n      {audioUrl && (\n        <div className=\"audio-player\">\n          <audio controls src={audioUrl} />\n          <a href={audioUrl} download=\"speech.mp3\" className=\"download-btn\">ğŸ“¥ Download Audio</a>\n        </div>\n      )}\n    </div>\n  );\n}\n\nexport default TextToSpeech;\n","size_bytes":2165},"start-frontend.sh":{"content":"#!/bin/bash\ncd frontend\n\n# Check if node_modules exists\nif [ ! -d \"node_modules\" ]; then\n    echo \"Installing dependencies...\"\n    npm install\nfi\n\necho \"Starting frontend server...\"\nnpm run dev\n","size_bytes":194},"frontend/src/components/VideoDubbing.jsx":{"content":"import { useState } from 'react';\nimport { api } from '../api';\n\nfunction VideoDubbing() {\n  const [videoFile, setVideoFile] = useState(null);\n  const [sourceLang, setSourceLang] = useState('en');\n  const [targetLang, setTargetLang] = useState('hi');\n  const [loading, setLoading] = useState(false);\n  const [dubbingId, setDubbingId] = useState(null);\n  const [status, setStatus] = useState('');\n  const [error, setError] = useState(null);\n  const [progress, setProgress] = useState(0);\n\n  const languages = {\n    'en': 'ğŸ‡¬ğŸ‡§ English',\n    'hi': 'ğŸ‡®ğŸ‡³ Hindi',\n    'es': 'ğŸ‡ªğŸ‡¸ Spanish',\n    'fr': 'ğŸ‡«ğŸ‡· French',\n    'de': 'ğŸ‡©ğŸ‡ª German',\n    'it': 'ğŸ‡®ğŸ‡¹ Italian',\n    'pt': 'ğŸ‡µğŸ‡¹ Portuguese',\n    'ja': 'ğŸ‡¯ğŸ‡µ Japanese',\n    'ko': 'ğŸ‡°ğŸ‡· Korean',\n    'zh': 'ğŸ‡¨ğŸ‡³ Chinese'\n  };\n\n  const handleFileChange = (e) => {\n    const file = e.target.files[0];\n    if (file && file.size > 100 * 1024 * 1024) {\n      setError('File too large (max 100MB)');\n      return;\n    }\n    setVideoFile(file);\n    setError(null);\n    setDubbingId(null);\n    setStatus('');\n    setProgress(0);\n  };\n\n  const checkStatus = async (id) => {\n    try {\n      const result = await api.getDubbingStatus(id);\n      setStatus(result.status);\n      \n      if (result.status === 'dubbing') {\n        setProgress(50);\n        setTimeout(() => checkStatus(id), 3000);\n      } else if (result.status === 'dubbed') {\n        setProgress(100);\n        setLoading(false);\n      } else if (result.status === 'error' || result.status === 'failed') {\n        setError('Dubbing failed. Please try again.');\n        setLoading(false);\n      }\n    } catch (err) {\n      setError('Failed to check status');\n      setLoading(false);\n    }\n  };\n\n  const handleDub = async () => {\n    if (!videoFile) {\n      setError('Please upload a video file first');\n      return;\n    }\n\n    setLoading(true);\n    setError(null);\n    setProgress(10);\n    setStatus('Starting dubbing...');\n\n    try {\n      const result = await api.startVideoDubbing(videoFile, sourceLang, targetLang);\n      setDubbingId(result.dubbing_id);\n      setProgress(30);\n      setStatus('Processing...');\n      \n      checkStatus(result.dubbing_id);\n    } catch (err) {\n      setError(err.response?.data?.error || 'Failed to start dubbing');\n      setLoading(false);\n    }\n  };\n\n  const handleDownload = async () => {\n    if (!dubbingId) return;\n\n    try {\n      const blob = await api.downloadDubbedVideo(dubbingId, targetLang);\n      const url = URL.createObjectURL(blob);\n      const a = document.createElement('a');\n      a.href = url;\n      a.download = `dubbed_video_${Date.now()}.mp4`;\n      document.body.appendChild(a);\n      a.click();\n      document.body.removeChild(a);\n      URL.revokeObjectURL(url);\n    } catch (err) {\n      setError('Failed to download video');\n    }\n  };\n\n  return (\n    <div className=\"feature-content\">\n      <input\n        type=\"file\"\n        accept=\".mp4,.avi,.mov,.mkv\"\n        onChange={handleFileChange}\n        className=\"file-input\"\n      />\n      {videoFile && (\n        <div className=\"success-message\">âœ… Uploaded: {videoFile.name}</div>\n      )}\n      \n      {videoFile && (\n        <div className=\"language-selector-duo\">\n          <div>\n            <label>ğŸŒ Source Language</label>\n            <select value={sourceLang} onChange={(e) => setSourceLang(e.target.value)}>\n              {Object.entries(languages).map(([code, name]) => (\n                <option key={code} value={code}>{name}</option>\n              ))}\n            </select>\n          </div>\n          <div>\n            <label>ğŸ¯ Target Language</label>\n            <select value={targetLang} onChange={(e) => setTargetLang(e.target.value)}>\n              {Object.entries(languages).map(([code, name]) => (\n                <option key={code} value={code}>{name}</option>\n              ))}\n            </select>\n          </div>\n        </div>\n      )}\n\n      <button onClick={handleDub} disabled={loading || !videoFile} className=\"primary-btn\">\n        {loading ? 'Processing...' : 'ğŸ¬ Start Dubbing'}\n      </button>\n\n      {loading && (\n        <div className=\"progress-container\">\n          <div className=\"progress-bar\" style={{ width: `${progress}%` }}></div>\n          <div className=\"status-text\">{status}</div>\n        </div>\n      )}\n\n      {error && <div className=\"error-message\">{error}</div>}\n\n      {status === 'dubbed' && dubbingId && (\n        <>\n          <div className=\"success-message\">âœ… Dubbing completed successfully! ğŸ‰</div>\n          <button onClick={handleDownload} className=\"download-btn\">\n            ğŸ“¥ Download Dubbed Video\n          </button>\n        </>\n      )}\n    </div>\n  );\n}\n\nexport default VideoDubbing;\n","size_bytes":4722},"frontend/src/components/SpeechToText.jsx":{"content":"import { useState } from 'react';\nimport { api } from '../api';\n\nfunction SpeechToText() {\n  const [audioFile, setAudioFile] = useState(null);\n  const [loading, setLoading] = useState(false);\n  const [transcription, setTranscription] = useState('');\n  const [error, setError] = useState(null);\n  const [success, setSuccess] = useState(false);\n\n  const handleFileChange = (e) => {\n    setAudioFile(e.target.files[0]);\n    setTranscription('');\n    setError(null);\n    setSuccess(false);\n  };\n\n  const handleTranscribe = async () => {\n    if (!audioFile) {\n      setError('Please upload an audio file first');\n      return;\n    }\n\n    setLoading(true);\n    setError(null);\n    setSuccess(false);\n    setTranscription('');\n\n    try {\n      const result = await api.speechToText(audioFile);\n      setTranscription(result.text);\n      setSuccess(true);\n    } catch (err) {\n      setError(err.response?.data?.error || 'Failed to transcribe audio');\n    } finally {\n      setLoading(false);\n    }\n  };\n\n  return (\n    <div className=\"tool-content\">\n      <h3>ğŸ¤ Speech to Text</h3>\n      <input\n        type=\"file\"\n        accept=\".wav,.mp3,.ogg,.flac,.m4a\"\n        onChange={handleFileChange}\n        className=\"file-input\"\n      />\n      {audioFile && (\n        <audio controls src={URL.createObjectURL(audioFile)} style={{ width: '100%', margin: '10px 0' }} />\n      )}\n      <button onClick={handleTranscribe} disabled={loading || !audioFile} className=\"primary-btn\">\n        {loading ? 'Transcribing...' : 'ğŸ“ Transcribe'}\n      </button>\n      {error && <div className=\"error-message\">{error}</div>}\n      {success && <div className=\"success-message\">âœ… Transcription completed!</div>}\n      {transcription && (\n        <div className=\"result-box\">\n          <h4>ğŸ“„ Transcription:</h4>\n          <textarea value={transcription} readOnly rows={6} />\n        </div>\n      )}\n    </div>\n  );\n}\n\nexport default SpeechToText;\n","size_bytes":1927},"frontend/src/components/ArticleToPodcast.jsx":{"content":"import { useState } from 'react';\nimport { api } from '../api';\n\nfunction ArticleToPodcast() {\n  const [articleText, setArticleText] = useState('');\n  const [scriptWordCount, setScriptWordCount] = useState(300);\n  const [loading, setLoading] = useState(false);\n  const [audioUrl, setAudioUrl] = useState(null);\n  const [error, setError] = useState(null);\n  const [success, setSuccess] = useState(false);\n\n  const handleGenerate = async () => {\n    if (!articleText.trim()) {\n      setError('Please enter an article or text');\n      return;\n    }\n\n    setLoading(true);\n    setError(null);\n    setSuccess(false);\n    setAudioUrl(null);\n\n    try {\n      const result = await api.articleToPodcast(articleText, scriptWordCount);\n      \n      if (result.audio) {\n        const audioBlob = new Blob(\n          [Uint8Array.from(atob(result.audio), c => c.charCodeAt(0))],\n          { type: 'audio/mpeg' }\n        );\n        const url = URL.createObjectURL(audioBlob);\n        setAudioUrl(url);\n      }\n      \n      setSuccess(true);\n    } catch (err) {\n      setError(err.response?.data?.error || 'Failed to generate podcast');\n    } finally {\n      setLoading(false);\n    }\n  };\n\n  return (\n    <div className=\"feature-content\">\n      <textarea\n        value={articleText}\n        onChange={(e) => setArticleText(e.target.value)}\n        placeholder=\"Paste your article or text here...\"\n        rows={6}\n        className=\"input-field\"\n      />\n      <div className=\"slider-container\">\n        <label>Podcast script length: {scriptWordCount} words</label>\n        <input\n          type=\"range\"\n          min=\"100\"\n          max=\"500\"\n          step=\"50\"\n          value={scriptWordCount}\n          onChange={(e) => setScriptWordCount(Number(e.target.value))}\n        />\n      </div>\n      <button onClick={handleGenerate} disabled={loading} className=\"primary-btn\">\n        {loading ? 'Creating...' : 'ğŸ§ Generate Podcast'}\n      </button>\n      {error && <div className=\"error-message\">{error}</div>}\n      {success && (\n        <>\n          <div className=\"success-message\">âœ… Podcast created successfully!</div>\n          <div className=\"info-message\">ğŸ’¡ This podcast features a conversational format with a Host and an Expert discussing your article.</div>\n        </>\n      )}\n      {audioUrl && (\n        <div className=\"audio-player\">\n          <h4>ğŸ§ Your Podcast:</h4>\n          <audio controls src={audioUrl} />\n          <a href={audioUrl} download=\"podcast.mp3\" className=\"download-btn\">ğŸ“¥ Download Podcast</a>\n        </div>\n      )}\n    </div>\n  );\n}\n\nexport default ArticleToPodcast;\n","size_bytes":2602},"frontend/src/components/WordToStory.jsx":{"content":"import { useState } from 'react';\nimport { api } from '../api';\n\nfunction WordToStory() {\n  const [words, setWords] = useState('');\n  const [theme, setTheme] = useState('');\n  const [wordCount, setWordCount] = useState(300);\n  const [language, setLanguage] = useState('english');\n  const [loading, setLoading] = useState(false);\n  const [story, setStory] = useState('');\n  const [audioUrl, setAudioUrl] = useState(null);\n  const [error, setError] = useState(null);\n  const [success, setSuccess] = useState(false);\n\n  const handleGenerate = async () => {\n    if (!words.trim()) {\n      setError('Please enter some words');\n      return;\n    }\n    if (!theme.trim()) {\n      setError('Please enter a theme');\n      return;\n    }\n\n    setLoading(true);\n    setError(null);\n    setSuccess(false);\n    setStory('');\n    setAudioUrl(null);\n\n    try {\n      const result = await api.wordToStory(words, theme, wordCount, language);\n      setStory(result.story);\n      \n      if (result.audio) {\n        const audioBlob = new Blob(\n          [Uint8Array.from(atob(result.audio), c => c.charCodeAt(0))],\n          { type: 'audio/mpeg' }\n        );\n        const url = URL.createObjectURL(audioBlob);\n        setAudioUrl(url);\n      }\n      \n      setSuccess(true);\n    } catch (err) {\n      setError(err.response?.data?.error || 'Failed to generate story');\n    } finally {\n      setLoading(false);\n    }\n  };\n\n  return (\n    <div className=\"feature-content\">\n      <input\n        type=\"text\"\n        value={words}\n        onChange={(e) => setWords(e.target.value)}\n        placeholder=\"adventure, forest, mystery, courage\"\n        className=\"input-field\"\n      />\n      <input\n        type=\"text\"\n        value={theme}\n        onChange={(e) => setTheme(e.target.value)}\n        placeholder=\"Fantasy adventure, Mystery thriller, etc.\"\n        className=\"input-field\"\n      />\n      <div className=\"dual-controls\">\n        <div className=\"slider-container\">\n          <label>Story word count: {wordCount}</label>\n          <input\n            type=\"range\"\n            min=\"100\"\n            max=\"1000\"\n            step=\"50\"\n            value={wordCount}\n            onChange={(e) => setWordCount(Number(e.target.value))}\n          />\n        </div>\n        <select value={language} onChange={(e) => setLanguage(e.target.value)}>\n          <option value=\"english\">English</option>\n          <option value=\"hindi\">Hindi</option>\n        </select>\n      </div>\n      <button onClick={handleGenerate} disabled={loading} className=\"primary-btn\">\n        {loading ? 'Creating...' : 'âœ¨ Generate Story'}\n      </button>\n      {error && <div className=\"error-message\">{error}</div>}\n      {success && <div className=\"success-message\">âœ… Story generated successfully!</div>}\n      {story && (\n        <div className=\"result-box\">\n          <h4>ğŸ“ Your Story:</h4>\n          <textarea value={story} readOnly rows={12} />\n        </div>\n      )}\n      {audioUrl && (\n        <div className=\"audio-player\">\n          <h4>ğŸ§ Audio Narration:</h4>\n          <audio controls src={audioUrl} />\n          <a href={audioUrl} download=\"story.mp3\" className=\"download-btn\">ğŸ“¥ Download Audio</a>\n        </div>\n      )}\n    </div>\n  );\n}\n\nexport default WordToStory;\n","size_bytes":3241},"frontend/src/components/TextTranslation.jsx":{"content":"import { useState } from 'react';\nimport { api } from '../api';\n\nfunction TextTranslation() {\n  const [text, setText] = useState('');\n  const [fromLang, setFromLang] = useState('English');\n  const [toLang, setToLang] = useState('Hindi');\n  const [loading, setLoading] = useState(false);\n  const [translation, setTranslation] = useState('');\n  const [error, setError] = useState(null);\n  const [success, setSuccess] = useState(false);\n\n  const languages = ['English', 'Hindi', 'Spanish', 'French', 'German', 'Italian', 'Portuguese', 'Japanese', 'Korean', 'Chinese'];\n\n  const handleTranslate = async () => {\n    if (!text.trim()) {\n      setError('Please enter some text to translate');\n      return;\n    }\n    if (fromLang === toLang) {\n      setError('Source and target languages must be different');\n      return;\n    }\n\n    setLoading(true);\n    setError(null);\n    setSuccess(false);\n    setTranslation('');\n\n    try {\n      const result = await api.textTranslation(text, fromLang, toLang);\n      setTranslation(result.translated_text);\n      setSuccess(true);\n    } catch (err) {\n      setError(err.response?.data?.error || 'Failed to translate');\n    } finally {\n      setLoading(false);\n    }\n  };\n\n  return (\n    <div className=\"tool-content\">\n      <h3>ğŸŒ Text Translation</h3>\n      <textarea\n        value={text}\n        onChange={(e) => setText(e.target.value)}\n        placeholder=\"Type or paste text here...\"\n        rows={5}\n      />\n      <div className=\"language-selectors\">\n        <div>\n          <label>From:</label>\n          <select value={fromLang} onChange={(e) => setFromLang(e.target.value)}>\n            {languages.map(lang => (\n              <option key={lang} value={lang}>{lang}</option>\n            ))}\n          </select>\n        </div>\n        <div>\n          <label>To:</label>\n          <select value={toLang} onChange={(e) => setToLang(e.target.value)}>\n            {languages.map(lang => (\n              <option key={lang} value={lang}>{lang}</option>\n            ))}\n          </select>\n        </div>\n      </div>\n      <button onClick={handleTranslate} disabled={loading} className=\"primary-btn\">\n        {loading ? 'Translating...' : 'ğŸ”„ Translate'}\n      </button>\n      {error && <div className=\"error-message\">{error}</div>}\n      {success && <div className=\"success-message\">âœ… Translated from {fromLang} to {toLang}</div>}\n      {translation && (\n        <div className=\"result-box\">\n          <h4>ğŸ“ Translation:</h4>\n          <textarea value={translation} readOnly rows={5} />\n        </div>\n      )}\n    </div>\n  );\n}\n\nexport default TextTranslation;\n","size_bytes":2608},"frontend/README.md":{"content":"# React + Vite\n\nThis template provides a minimal setup to get React working in Vite with HMR and some ESLint rules.\n\nCurrently, two official plugins are available:\n\n- [@vitejs/plugin-react](https://github.com/vitejs/vite-plugin-react/blob/main/packages/plugin-react) uses [Babel](https://babeljs.io/) (or [oxc](https://oxc.rs) when used in [rolldown-vite](https://vite.dev/guide/rolldown)) for Fast Refresh\n- [@vitejs/plugin-react-swc](https://github.com/vitejs/vite-plugin-react/blob/main/packages/plugin-react-swc) uses [SWC](https://swc.rs/) for Fast Refresh\n\n## React Compiler\n\nThe React Compiler is not enabled on this template because of its impact on dev & build performances. To add it, see [this documentation](https://react.dev/learn/react-compiler/installation).\n\n## Expanding the ESLint configuration\n\nIf you are developing a production application, we recommend using TypeScript with type-aware lint rules enabled. Check out the [TS template](https://github.com/vitejs/vite/tree/main/packages/create-vite/template-react-ts) for information on how to integrate TypeScript and [`typescript-eslint`](https://typescript-eslint.io) in your project.\n","size_bytes":1157},"frontend/src/components/Footer.jsx":{"content":"import './Footer.css';\n\nfunction Footer() {\n  const scrollToFAQ = () => {\n    const element = document.getElementById('faq');\n    if (element) {\n      element.scrollIntoView({ behavior: 'smooth' });\n    }\n  };\n\n  return (\n    <footer id=\"footer\" className=\"netflix-footer\">\n      <div className=\"footer-container\">\n        <div className=\"footer-links\">\n          <div className=\"footer-column\">\n            <h3>Company</h3>\n            <a href=\"#\">About Us</a>\n            <a href=\"#\">Careers</a>\n            <a href=\"#\">Press</a>\n          </div>\n          \n          <div className=\"footer-column\">\n            <h3>Support</h3>\n            <a href=\"#\">Help Center</a>\n            <a href=\"#\">Contact Us</a>\n            <a href=\"#\" onClick={(e) => { e.preventDefault(); scrollToFAQ(); }}>FAQ</a>\n          </div>\n          \n          <div className=\"footer-column\">\n            <h3>Legal</h3>\n            <a href=\"#\">Privacy Policy</a>\n            <a href=\"#\">Terms of Service</a>\n            <a href=\"#\">Cookie Preferences</a>\n          </div>\n          \n          <div className=\"footer-column\">\n            <h3>Follow Us</h3>\n            <div className=\"social-links\">\n              <a href=\"#\" className=\"social-icon\">ğŸ“˜</a>\n              <a href=\"#\" className=\"social-icon\">ğŸ¦</a>\n              <a href=\"#\" className=\"social-icon\">ğŸ“·</a>\n              <a href=\"#\" className=\"social-icon\">ğŸ’¼</a>\n            </div>\n          </div>\n        </div>\n        \n        <div className=\"footer-bottom\">\n          <p>&copy; 2025 Anuvaad AI. All rights reserved.</p>\n        </div>\n      </div>\n    </footer>\n  );\n}\n\nexport default Footer;\n","size_bytes":1645},"frontend/src/components/FeatureCard.css":{"content":".feature-card-netflix {\n  min-width: 280px;\n  height: 350px;\n  background: linear-gradient(135deg, #1a1a1a 0%, #2d2d2d 100%);\n  border-radius: 8px;\n  overflow: hidden;\n  cursor: pointer;\n  transition: all 0.3s ease;\n  position: relative;\n}\n\n.feature-card-netflix:hover {\n  transform: scale(1.05);\n  box-shadow: 0 8px 30px rgba(229, 9, 20, 0.4);\n  z-index: 10;\n}\n\n.card-image {\n  height: 60%;\n  background: linear-gradient(135deg, #E50914 0%, #831010 100%);\n  display: flex;\n  align-items: center;\n  justify-content: center;\n  transition: all 0.3s ease;\n}\n\n.card-icon {\n  font-size: 4rem;\n  filter: brightness(1.2);\n}\n\n.feature-card-netflix:hover .card-image {\n  height: 50%;\n}\n\n.card-overlay {\n  padding: 1.5rem;\n  height: 40%;\n  display: flex;\n  flex-direction: column;\n  justify-content: center;\n  transition: all 0.3s ease;\n}\n\n.feature-card-netflix:hover .card-overlay {\n  height: 50%;\n  background: rgba(229, 9, 20, 0.1);\n}\n\n.card-overlay h3 {\n  color: #fff;\n  margin: 0 0 0.75rem 0;\n  font-size: 1.3rem;\n  font-weight: 600;\n}\n\n.card-overlay p {\n  color: #b3b3b3;\n  margin: 0;\n  font-size: 0.95rem;\n  line-height: 1.4;\n  display: -webkit-box;\n  -webkit-line-clamp: 3;\n  -webkit-box-orient: vertical;\n  overflow: hidden;\n}\n\n.feature-card-netflix:hover .card-overlay p {\n  color: #e5e5e5;\n}\n\n@media (max-width: 768px) {\n  .feature-card-netflix {\n    min-width: 220px;\n    height: 300px;\n  }\n  \n  .card-icon {\n    font-size: 3rem;\n  }\n  \n  .card-overlay h3 {\n    font-size: 1.1rem;\n  }\n  \n  .card-overlay p {\n    font-size: 0.85rem;\n  }\n}\n","size_bytes":1540},"frontend/src/components/Modal.css":{"content":".modal-overlay {\n  position: fixed;\n  top: 0;\n  left: 0;\n  right: 0;\n  bottom: 0;\n  background: rgba(0, 0, 0, 0.75);\n  display: flex;\n  align-items: center;\n  justify-content: center;\n  z-index: 2000;\n  animation: fadeIn 0.3s ease;\n}\n\n@keyframes fadeIn {\n  from {\n    opacity: 0;\n  }\n  to {\n    opacity: 1;\n  }\n}\n\n.modal-content {\n  background: #141414;\n  border-radius: 8px;\n  padding: 3rem 4rem;\n  max-width: 450px;\n  width: 90%;\n  position: relative;\n  animation: slideUp 0.3s ease;\n  box-shadow: 0 8px 32px rgba(0, 0, 0, 0.6);\n}\n\n@keyframes slideUp {\n  from {\n    transform: translateY(50px);\n    opacity: 0;\n  }\n  to {\n    transform: translateY(0);\n    opacity: 1;\n  }\n}\n\n.modal-close {\n  position: absolute;\n  top: 1rem;\n  right: 1rem;\n  background: transparent;\n  border: none;\n  color: #fff;\n  font-size: 2rem;\n  cursor: pointer;\n  width: 40px;\n  height: 40px;\n  display: flex;\n  align-items: center;\n  justify-content: center;\n  transition: opacity 0.2s ease;\n}\n\n.modal-close:hover {\n  opacity: 0.7;\n}\n\n.modal-content h2 {\n  color: #fff;\n  margin-bottom: 2rem;\n  font-size: 2rem;\n  font-weight: 700;\n}\n\n.form-group {\n  margin-bottom: 1.5rem;\n}\n\n.form-group input {\n  width: 100%;\n  padding: 1rem;\n  background: #333;\n  border: none;\n  border-radius: 4px;\n  color: #fff;\n  font-size: 1rem;\n  transition: background 0.2s ease;\n}\n\n.form-group input:focus {\n  outline: none;\n  background: #454545;\n}\n\n.form-group input::placeholder {\n  color: #8c8c8c;\n}\n\n.form-options {\n  display: flex;\n  justify-content: space-between;\n  align-items: center;\n  margin-bottom: 2rem;\n}\n\n.checkbox-label {\n  display: flex;\n  align-items: center;\n  gap: 0.5rem;\n  color: #b3b3b3;\n  font-size: 0.9rem;\n  cursor: pointer;\n}\n\n.checkbox-label input[type=\"checkbox\"] {\n  width: auto;\n  cursor: pointer;\n}\n\n.forgot-link {\n  color: #b3b3b3;\n  text-decoration: none;\n  font-size: 0.9rem;\n  transition: color 0.2s ease;\n}\n\n.forgot-link:hover {\n  color: #fff;\n}\n\n.btn-submit {\n  width: 100%;\n  padding: 1rem;\n  background: #E50914;\n  color: white;\n  border: none;\n  border-radius: 4px;\n  font-size: 1rem;\n  font-weight: 600;\n  cursor: pointer;\n  transition: background 0.3s ease;\n}\n\n.btn-submit:hover {\n  background: #f40612;\n}\n\n.modal-footer {\n  margin-top: 1.5rem;\n  text-align: center;\n}\n\n.modal-footer p {\n  color: #737373;\n  font-size: 0.95rem;\n}\n\n.modal-footer a {\n  color: #fff;\n  text-decoration: none;\n  font-weight: 600;\n  transition: text-decoration 0.2s ease;\n}\n\n.modal-footer a:hover {\n  text-decoration: underline;\n}\n\n.feature-modal {\n  max-width: 800px;\n  padding: 0;\n  overflow: hidden;\n}\n\n.feature-modal-image {\n  width: 100%;\n  height: 300px;\n  overflow: hidden;\n}\n\n.feature-modal-image img {\n  width: 100%;\n  height: 100%;\n  object-fit: cover;\n}\n\n.feature-modal-header {\n  display: flex;\n  align-items: center;\n  gap: 1rem;\n  margin-bottom: 2rem;\n  padding: 2rem 2.5rem 0;\n}\n\n.feature-icon {\n  font-size: 3rem;\n}\n\n.feature-modal-header h2 {\n  margin: 0;\n  font-size: 1.8rem;\n}\n\n.feature-modal-body {\n  padding: 0 2.5rem;\n}\n\n.feature-modal-body p {\n  color: #e5e5e5;\n  line-height: 1.6;\n  margin-bottom: 1.5rem;\n  font-size: 1.05rem;\n}\n\n.feature-details h3 {\n  color: #fff;\n  margin-bottom: 1rem;\n  font-size: 1.2rem;\n}\n\n.feature-details ul {\n  list-style: none;\n  padding: 0;\n}\n\n.feature-details li {\n  color: #b3b3b3;\n  padding: 0.5rem 0;\n  padding-left: 1.5rem;\n  position: relative;\n}\n\n.feature-details li:before {\n  content: \"âœ“\";\n  position: absolute;\n  left: 0;\n  color: #E50914;\n  font-weight: bold;\n}\n\n.feature-modal-footer {\n  margin-top: 2rem;\n  padding: 0 2.5rem 2.5rem;\n  display: flex;\n  justify-content: flex-end;\n}\n\n.btn-get-started {\n  padding: 0.75rem 2.5rem;\n  background: #E50914;\n  color: white;\n  border: none;\n  border-radius: 4px;\n  font-size: 1rem;\n  font-weight: 600;\n  cursor: pointer;\n  transition: all 0.3s ease;\n}\n\n.btn-get-started:hover {\n  background: #f40612;\n  transform: scale(1.05);\n}\n\n@media (max-width: 768px) {\n  .modal-content {\n    padding: 2rem 1.5rem;\n    width: 95%;\n  }\n  \n  .feature-modal {\n    padding: 2rem 1.5rem;\n  }\n  \n  .modal-content h2 {\n    font-size: 1.5rem;\n  }\n}\n","size_bytes":4117},"frontend/src/components/FeatureCard.jsx":{"content":"import './FeatureCard.css';\n\nfunction FeatureCard({ feature, onClick }) {\n  return (\n    <div className=\"feature-card-netflix\" onClick={onClick}>\n      <div className=\"card-image\" style={{\n        backgroundImage: feature.image ? `url(${feature.image})` : 'none',\n        backgroundSize: 'cover',\n        backgroundPosition: 'center'\n      }}>\n        {!feature.image && <span className=\"card-icon\">{feature.icon}</span>}\n      </div>\n      <div className=\"card-overlay\">\n        <h3>{feature.title}</h3>\n        <p>{feature.shortDesc}</p>\n      </div>\n    </div>\n  );\n}\n\nexport default FeatureCard;\n","size_bytes":600},"models.py":{"content":"from flask_sqlalchemy import SQLAlchemy\nfrom flask_bcrypt import Bcrypt\nfrom datetime import datetime\n\ndb = SQLAlchemy()\nbcrypt = Bcrypt()\n\nclass User(db.Model):\n    __tablename__ = 'users'\n    \n    id = db.Column(db.Integer, primary_key=True)\n    name = db.Column(db.String(100), nullable=False)\n    email = db.Column(db.String(120), unique=True, nullable=False)\n    password_hash = db.Column(db.String(200), nullable=False)\n    created_at = db.Column(db.DateTime, default=datetime.utcnow)\n    \n    history = db.relationship('UserHistory', backref='user', lazy=True, cascade='all, delete-orphan')\n    \n    def set_password(self, password):\n        self.password_hash = bcrypt.generate_password_hash(password).decode('utf-8')\n    \n    def check_password(self, password):\n        return bcrypt.check_password_hash(self.password_hash, password)\n    \n    def to_dict(self):\n        return {\n            'id': self.id,\n            'name': self.name,\n            'email': self.email,\n            'created_at': self.created_at.isoformat()\n        }\n\nclass UserHistory(db.Model):\n    __tablename__ = 'user_history'\n    \n    id = db.Column(db.Integer, primary_key=True)\n    user_id = db.Column(db.Integer, db.ForeignKey('users.id'), nullable=False)\n    feature_type = db.Column(db.String(50), nullable=False)\n    feature_data = db.Column(db.JSON)\n    created_at = db.Column(db.DateTime, default=datetime.utcnow)\n    \n    def to_dict(self):\n        return {\n            'id': self.id,\n            'feature_type': self.feature_type,\n            'feature_data': self.feature_data,\n            'created_at': self.created_at.isoformat()\n        }\n","size_bytes":1634},"frontend/src/components/SearchBar.css":{"content":".search-container {\n  padding: 2rem 4%;\n  display: flex;\n  justify-content: center;\n}\n\n.search-form {\n  position: relative;\n  width: 100%;\n  max-width: 800px;\n}\n\n.search-input {\n  width: 100%;\n  padding: 1rem 4rem 1rem 1.5rem;\n  background: #2d2d2d;\n  border: 2px solid transparent;\n  border-radius: 50px;\n  color: #fff;\n  font-size: 1rem;\n  transition: all 0.3s ease;\n}\n\n.search-input:focus {\n  outline: none;\n  border-color: #E50914;\n  background: #3d3d3d;\n}\n\n.search-input::placeholder {\n  color: #8c8c8c;\n}\n\n.search-btn {\n  position: absolute;\n  right: 5px;\n  top: 50%;\n  transform: translateY(-50%);\n  background: #E50914;\n  border: none;\n  width: 50px;\n  height: 50px;\n  border-radius: 50%;\n  font-size: 1.3rem;\n  cursor: pointer;\n  transition: all 0.3s ease;\n  display: flex;\n  align-items: center;\n  justify-content: center;\n}\n\n.search-btn:hover {\n  background: #f40612;\n  transform: translateY(-50%) scale(1.1);\n}\n\n@media (max-width: 768px) {\n  .search-input {\n    padding: 0.9rem 3.5rem 0.9rem 1.2rem;\n    font-size: 0.95rem;\n  }\n  \n  .search-btn {\n    width: 45px;\n    height: 45px;\n    font-size: 1.1rem;\n  }\n}\n","size_bytes":1123},"frontend/src/components/Hero.jsx":{"content":"import './Hero.css';\n\nfunction Hero() {\n  const scrollToFeatures = () => {\n    const element = document.getElementById('features');\n    if (element) {\n      element.scrollIntoView({ behavior: 'smooth' });\n    }\n  };\n\n  return (\n    <section id=\"hero\" className=\"hero-section\">\n      <div className=\"hero-overlay\"></div>\n      <div className=\"hero-content\">\n        <h1 className=\"hero-title\">Welcome to Anuvaad AI</h1>\n        <p className=\"hero-subtitle\">\n          Transform your content across languages with AI-powered dubbing, translation, and creative tools. \n          Experience the future of multimedia content transformation.\n        </p>\n        <button className=\"btn-hero\" onClick={scrollToFeatures}>\n          Get Started\n        </button>\n      </div>\n    </section>\n  );\n}\n\nexport default Hero;\n","size_bytes":811},"frontend/src/AuthContext.jsx":{"content":"import { createContext, useState, useContext, useEffect } from 'react';\nimport axios from 'axios';\n\nconst AuthContext = createContext();\n\nexport const useAuth = () => {\n  const context = useContext(AuthContext);\n  if (!context) {\n    throw new Error('useAuth must be used within an AuthProvider');\n  }\n  return context;\n};\n\nexport const AuthProvider = ({ children }) => {\n  const [user, setUser] = useState(null);\n  const [token, setToken] = useState(localStorage.getItem('token'));\n  const [loading, setLoading] = useState(true);\n\n  useEffect(() => {\n    if (token) {\n      fetchCurrentUser();\n    } else {\n      setLoading(false);\n    }\n  }, [token]);\n\n  const fetchCurrentUser = async () => {\n    try {\n      const response = await axios.get('/api/auth/me', {\n        headers: { Authorization: `Bearer ${token}` }\n      });\n      setUser(response.data.user);\n    } catch (error) {\n      console.error('Failed to fetch user:', error);\n      logout();\n    } finally {\n      setLoading(false);\n    }\n  };\n\n  const login = async (email, password) => {\n    try {\n      const response = await axios.post('/api/auth/login', { email, password });\n      const { access_token, user: userData } = response.data;\n      \n      localStorage.setItem('token', access_token);\n      setToken(access_token);\n      setUser(userData);\n      \n      return { success: true };\n    } catch (error) {\n      return { \n        success: false, \n        error: error.response?.data?.error || 'Login failed' \n      };\n    }\n  };\n\n  const signup = async (name, email, password) => {\n    try {\n      const response = await axios.post('/api/auth/signup', { name, email, password });\n      const { access_token, user: userData } = response.data;\n      \n      localStorage.setItem('token', access_token);\n      setToken(access_token);\n      setUser(userData);\n      \n      return { success: true };\n    } catch (error) {\n      return { \n        success: false, \n        error: error.response?.data?.error || 'Signup failed' \n      };\n    }\n  };\n\n  const logout = () => {\n    localStorage.removeItem('token');\n    setToken(null);\n    setUser(null);\n  };\n\n  const value = {\n    user,\n    token,\n    loading,\n    login,\n    signup,\n    logout,\n    isAuthenticated: !!user\n  };\n\n  return <AuthContext.Provider value={value}>{children}</AuthContext.Provider>;\n};\n","size_bytes":2325},"frontend/src/components/FeatureModal.jsx":{"content":"import './Modal.css';\n\nfunction FeatureModal({ feature, onClose }) {\n  if (!feature) return null;\n\n  return (\n    <div className=\"modal-overlay\" onClick={onClose}>\n      <div className=\"modal-content feature-modal\" onClick={(e) => e.stopPropagation()}>\n        <button className=\"modal-close\" onClick={onClose}>&times;</button>\n        \n        {feature.image && (\n          <div className=\"feature-modal-image\">\n            <img src={feature.image} alt={feature.title} />\n          </div>\n        )}\n        \n        <div className=\"feature-modal-header\">\n          <span className=\"feature-icon\">{feature.icon}</span>\n          <h2>{feature.title}</h2>\n        </div>\n        \n        <div className=\"feature-modal-body\">\n          <p>{feature.description}</p>\n          \n          {feature.details && (\n            <div className=\"feature-details\">\n              <h3>Key Features:</h3>\n              <ul>\n                {feature.details.map((detail, idx) => (\n                  <li key={idx}>{detail}</li>\n                ))}\n              </ul>\n            </div>\n          )}\n        </div>\n        \n        <div className=\"feature-modal-footer\">\n          <button className=\"btn-get-started\" onClick={() => {\n            if (feature.action) feature.action();\n            onClose();\n          }}>\n            Get Started\n          </button>\n        </div>\n      </div>\n    </div>\n  );\n}\n\nexport default FeatureModal;\n","size_bytes":1423},"frontend/src/components/Hero.css":{"content":".hero-section {\n  position: relative;\n  height: 85vh;\n  min-height: 500px;\n  display: flex;\n  align-items: center;\n  justify-content: center;\n  background: linear-gradient(to bottom, rgba(0,0,0,0.7) 0%, rgba(0,0,0,0.5) 100%), \n              url('/attached_assets/stock_images/ai_technology_video__ae02401c.jpg');\n  background-size: cover;\n  background-position: center;\n  margin-top: 70px;\n  overflow: hidden;\n}\n\n.hero-overlay {\n  position: absolute;\n  top: 0;\n  left: 0;\n  right: 0;\n  bottom: 0;\n  background: linear-gradient(to bottom, transparent 0%, rgba(20, 20, 20, 0.8) 100%);\n}\n\n.hero-content {\n  position: relative;\n  z-index: 1;\n  text-align: center;\n  max-width: 900px;\n  padding: 0 4%;\n}\n\n.hero-title {\n  font-size: 4rem;\n  font-weight: 700;\n  color: #fff;\n  margin-bottom: 1.5rem;\n  line-height: 1.1;\n  text-shadow: 2px 2px 8px rgba(0, 0, 0, 0.8);\n}\n\n.hero-subtitle {\n  font-size: 1.4rem;\n  color: #e5e5e5;\n  margin-bottom: 2.5rem;\n  line-height: 1.6;\n  text-shadow: 1px 1px 4px rgba(0, 0, 0, 0.8);\n}\n\n.btn-hero {\n  padding: 1rem 3rem;\n  font-size: 1.2rem;\n  font-weight: 600;\n  background: #E50914;\n  color: white;\n  border: none;\n  border-radius: 4px;\n  cursor: pointer;\n  transition: all 0.3s ease;\n  box-shadow: 0 4px 15px rgba(229, 9, 20, 0.4);\n}\n\n.btn-hero:hover {\n  background: #f40612;\n  transform: translateY(-2px);\n  box-shadow: 0 6px 20px rgba(229, 9, 20, 0.6);\n}\n\n@media (max-width: 768px) {\n  .hero-section {\n    height: 70vh;\n    min-height: 400px;\n  }\n  \n  .hero-title {\n    font-size: 2.5rem;\n  }\n  \n  .hero-subtitle {\n    font-size: 1.1rem;\n  }\n  \n  .btn-hero {\n    padding: 0.8rem 2rem;\n    font-size: 1rem;\n  }\n}\n","size_bytes":1644},"frontend/src/components/Footer.css":{"content":".netflix-footer {\n  background: #141414;\n  padding: 4rem 4% 2rem;\n  margin-top: 5rem;\n  border-top: 1px solid #333;\n}\n\n.footer-container {\n  max-width: 1920px;\n  margin: 0 auto;\n}\n\n.footer-links {\n  display: grid;\n  grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));\n  gap: 3rem;\n  margin-bottom: 3rem;\n}\n\n.footer-column h3 {\n  color: #e5e5e5;\n  font-size: 1.1rem;\n  font-weight: 600;\n  margin-bottom: 1rem;\n}\n\n.footer-column a {\n  display: block;\n  color: #808080;\n  text-decoration: none;\n  margin-bottom: 0.75rem;\n  font-size: 0.95rem;\n  transition: color 0.2s ease;\n}\n\n.footer-column a:hover {\n  color: #e5e5e5;\n}\n\n.social-links {\n  display: flex;\n  gap: 1rem;\n  margin-top: 0.5rem;\n}\n\n.social-icon {\n  font-size: 1.5rem;\n  transition: transform 0.2s ease;\n}\n\n.social-icon:hover {\n  transform: scale(1.2);\n}\n\n.footer-bottom {\n  text-align: center;\n  padding-top: 2rem;\n  border-top: 1px solid #333;\n}\n\n.footer-bottom p {\n  color: #808080;\n  font-size: 0.9rem;\n}\n\n@media (max-width: 768px) {\n  .netflix-footer {\n    padding: 3rem 4% 1.5rem;\n  }\n  \n  .footer-links {\n    grid-template-columns: repeat(2, 1fr);\n    gap: 2rem;\n  }\n  \n  .footer-column h3 {\n    font-size: 1rem;\n  }\n  \n  .footer-column a {\n    font-size: 0.9rem;\n  }\n}\n","size_bytes":1244},"frontend/src/components/SearchBar.jsx":{"content":"import { useState } from 'react';\nimport './SearchBar.css';\n\nfunction SearchBar({ onSearch }) {\n  const [searchTerm, setSearchTerm] = useState('');\n\n  const handleSubmit = (e) => {\n    e.preventDefault();\n    if (onSearch) onSearch(searchTerm);\n  };\n\n  return (\n    <div className=\"search-container\">\n      <form onSubmit={handleSubmit} className=\"search-form\">\n        <input\n          type=\"text\"\n          placeholder=\"Search for features, tools, or content...\"\n          value={searchTerm}\n          onChange={(e) => setSearchTerm(e.target.value)}\n          className=\"search-input\"\n        />\n        <button type=\"submit\" className=\"search-btn\">\n          ğŸ”\n        </button>\n      </form>\n    </div>\n  );\n}\n\nexport default SearchBar;\n","size_bytes":744},"frontend/src/components/FeatureRow.css":{"content":".feature-row {\n  margin-bottom: 3rem;\n}\n\n.row-title {\n  color: #e5e5e5;\n  font-size: 1.6rem;\n  font-weight: 600;\n  margin-bottom: 1rem;\n  padding: 0 4%;\n}\n\n.row-container {\n  position: relative;\n  padding: 0 4%;\n}\n\n.cards-container {\n  display: flex;\n  gap: 1rem;\n  overflow-x: auto;\n  scroll-behavior: smooth;\n  padding: 1rem 0;\n  scrollbar-width: none;\n  -ms-overflow-style: none;\n}\n\n.cards-container::-webkit-scrollbar {\n  display: none;\n}\n\n.scroll-btn {\n  position: absolute;\n  top: 50%;\n  transform: translateY(-50%);\n  background: rgba(20, 20, 20, 0.7);\n  color: white;\n  border: none;\n  width: 50px;\n  height: 50px;\n  border-radius: 50%;\n  font-size: 2rem;\n  cursor: pointer;\n  z-index: 5;\n  transition: all 0.3s ease;\n  display: flex;\n  align-items: center;\n  justify-content: center;\n  opacity: 0;\n}\n\n.row-container:hover .scroll-btn {\n  opacity: 1;\n}\n\n.scroll-btn:hover {\n  background: rgba(229, 9, 20, 0.9);\n  transform: translateY(-50%) scale(1.1);\n}\n\n.scroll-left {\n  left: 1%;\n}\n\n.scroll-right {\n  right: 1%;\n}\n\n@media (max-width: 768px) {\n  .row-title {\n    font-size: 1.3rem;\n  }\n  \n  .scroll-btn {\n    display: none;\n  }\n  \n  .cards-container {\n    gap: 0.75rem;\n  }\n}\n","size_bytes":1186},"frontend/src/components/Header.css":{"content":".netflix-header {\n  position: fixed;\n  top: 0;\n  left: 0;\n  right: 0;\n  z-index: 1000;\n  transition: background-color 0.3s ease;\n  background: linear-gradient(to bottom, rgba(0,0,0,0.8) 0%, rgba(0,0,0,0) 100%);\n}\n\n.netflix-header.scrolled {\n  background-color: #141414;\n}\n\n.header-container {\n  display: flex;\n  justify-content: space-between;\n  align-items: center;\n  padding: 1.5rem 4%;\n  max-width: 1920px;\n  margin: 0 auto;\n}\n\n.header-left {\n  display: flex;\n  align-items: center;\n  gap: 2rem;\n}\n\n.app-logo {\n  font-size: 1.8rem;\n  font-weight: 700;\n  color: #E50914;\n  cursor: pointer;\n  margin: 0;\n  letter-spacing: -0.5px;\n}\n\n.nav-links {\n  display: flex;\n  gap: 1.5rem;\n  align-items: center;\n}\n\n.nav-links a {\n  color: #e5e5e5;\n  text-decoration: none;\n  font-size: 0.95rem;\n  font-weight: 500;\n  cursor: pointer;\n  transition: color 0.2s ease;\n}\n\n.nav-links a:hover {\n  color: #b3b3b3;\n}\n\n.header-right {\n  display: flex;\n  gap: 1rem;\n  align-items: center;\n}\n\n.btn-login,\n.btn-signup {\n  padding: 0.5rem 1.5rem;\n  font-size: 0.95rem;\n  font-weight: 600;\n  border-radius: 4px;\n  cursor: pointer;\n  transition: all 0.3s ease;\n  border: none;\n}\n\n.btn-login {\n  background: transparent;\n  color: white;\n  border: 1px solid white;\n}\n\n.btn-login:hover {\n  background: rgba(255, 255, 255, 0.1);\n}\n\n.btn-signup {\n  background: #E50914;\n  color: white;\n}\n\n.btn-signup:hover {\n  background: #f40612;\n}\n\n.user-name {\n  color: #e5e5e5;\n  font-size: 0.95rem;\n  font-weight: 500;\n  margin-right: 0.5rem;\n}\n\n.btn-logout {\n  padding: 0.5rem 1.5rem;\n  font-size: 0.95rem;\n  font-weight: 600;\n  border-radius: 4px;\n  cursor: pointer;\n  transition: all 0.3s ease;\n  background: #E50914;\n  color: white;\n  border: none;\n}\n\n.btn-logout:hover {\n  background: #f40612;\n}\n\n@media (max-width: 768px) {\n  .header-container {\n    padding: 1rem 4%;\n  }\n  \n  .nav-links {\n    display: none;\n  }\n  \n  .app-logo {\n    font-size: 1.4rem;\n  }\n  \n  .btn-login,\n  .btn-signup {\n    padding: 0.4rem 1rem;\n    font-size: 0.85rem;\n  }\n}\n","size_bytes":2011},"frontend/src/components/Header.jsx":{"content":"import { useState, useEffect } from 'react';\nimport { useAuth } from '../AuthContext';\nimport LoginModal from './LoginModal';\nimport SignupModal from './SignupModal';\nimport './Header.css';\n\nfunction Header({ onSearchChange }) {\n  const [showLoginModal, setShowLoginModal] = useState(false);\n  const [showSignupModal, setShowSignupModal] = useState(false);\n  const [isScrolled, setIsScrolled] = useState(false);\n  const { user, logout } = useAuth();\n\n  useEffect(() => {\n    const handleScroll = () => {\n      setIsScrolled(window.scrollY > 50);\n    };\n\n    window.addEventListener('scroll', handleScroll);\n    return () => window.removeEventListener('scroll', handleScroll);\n  }, []);\n\n  const scrollToSection = (id) => {\n    const element = document.getElementById(id);\n    if (element) {\n      element.scrollIntoView({ behavior: 'smooth' });\n    }\n  };\n\n  return (\n    <>\n      <header className={`netflix-header ${isScrolled ? 'scrolled' : ''}`}>\n        <div className=\"header-container\">\n          <div className=\"header-left\">\n            <h1 className=\"app-logo\">Anuvaad AI</h1>\n            <nav className=\"nav-links\">\n              <a onClick={() => scrollToSection('hero')}>Home</a>\n              <a onClick={() => scrollToSection('features')}>Features</a>\n              <a onClick={() => scrollToSection('footer')}>Contact</a>\n            </nav>\n          </div>\n          \n          <div className=\"header-right\">\n            {user ? (\n              <>\n                <span className=\"user-name\">Welcome, {user.name}</span>\n                <button className=\"btn-logout\" onClick={logout}>\n                  Log Out\n                </button>\n              </>\n            ) : (\n              <>\n                <button className=\"btn-login\" onClick={() => setShowLoginModal(true)}>\n                  Log In\n                </button>\n                <button className=\"btn-signup\" onClick={() => setShowSignupModal(true)}>\n                  Sign Up\n                </button>\n              </>\n            )}\n          </div>\n        </div>\n      </header>\n\n      {showLoginModal && (\n        <LoginModal \n          onClose={() => setShowLoginModal(false)} \n          onSwitchToSignup={() => {\n            setShowLoginModal(false);\n            setShowSignupModal(true);\n          }}\n        />\n      )}\n      {showSignupModal && (\n        <SignupModal \n          onClose={() => setShowSignupModal(false)} \n          onSwitchToLogin={() => {\n            setShowSignupModal(false);\n            setShowLoginModal(true);\n          }}\n        />\n      )}\n    </>\n  );\n}\n\nexport default Header;\n","size_bytes":2599},"frontend/src/components/LoginModal.jsx":{"content":"import { useState } from 'react';\nimport { useAuth } from '../AuthContext';\nimport './Modal.css';\n\nfunction LoginModal({ onClose, onSwitchToSignup }) {\n  const [email, setEmail] = useState('');\n  const [password, setPassword] = useState('');\n  const [rememberMe, setRememberMe] = useState(false);\n  const [error, setError] = useState('');\n  const [loading, setLoading] = useState(false);\n  const { login } = useAuth();\n\n  const handleSubmit = async (e) => {\n    e.preventDefault();\n    setError('');\n    setLoading(true);\n    \n    const result = await login(email, password);\n    \n    if (result.success) {\n      onClose();\n    } else {\n      setError(result.error);\n    }\n    \n    setLoading(false);\n  };\n\n  return (\n    <div className=\"modal-overlay\" onClick={onClose}>\n      <div className=\"modal-content\" onClick={(e) => e.stopPropagation()}>\n        <button className=\"modal-close\" onClick={onClose}>&times;</button>\n        \n        <h2>Sign In</h2>\n        \n        {error && <div className=\"error-message\">{error}</div>}\n        \n        <form onSubmit={handleSubmit}>\n          <div className=\"form-group\">\n            <input\n              type=\"email\"\n              placeholder=\"Email or username\"\n              value={email}\n              onChange={(e) => setEmail(e.target.value)}\n              required\n            />\n          </div>\n          \n          <div className=\"form-group\">\n            <input\n              type=\"password\"\n              placeholder=\"Password\"\n              value={password}\n              onChange={(e) => setPassword(e.target.value)}\n              required\n            />\n          </div>\n          \n          <div className=\"form-options\">\n            <label className=\"checkbox-label\">\n              <input\n                type=\"checkbox\"\n                checked={rememberMe}\n                onChange={(e) => setRememberMe(e.target.checked)}\n              />\n              <span>Remember me</span>\n            </label>\n            <a href=\"#\" className=\"forgot-link\">Forgot password?</a>\n          </div>\n          \n          <button type=\"submit\" className=\"btn-submit\" disabled={loading}>\n            {loading ? 'Signing In...' : 'Sign In'}\n          </button>\n        </form>\n        \n        <div className=\"modal-footer\">\n          <p>New to Anuvaad AI? <a href=\"#\" onClick={(e) => { \n            e.preventDefault(); \n            onClose(); \n            if (onSwitchToSignup) onSwitchToSignup();\n          }}>Sign up now</a></p>\n        </div>\n      </div>\n    </div>\n  );\n}\n\nexport default LoginModal;\n","size_bytes":2551},"frontend/src/components/SignupModal.jsx":{"content":"import { useState } from 'react';\nimport { useAuth } from '../AuthContext';\nimport './Modal.css';\n\nfunction SignupModal({ onClose, onSwitchToLogin }) {\n  const [name, setName] = useState('');\n  const [email, setEmail] = useState('');\n  const [password, setPassword] = useState('');\n  const [error, setError] = useState('');\n  const [loading, setLoading] = useState(false);\n  const { signup } = useAuth();\n\n  const handleSubmit = async (e) => {\n    e.preventDefault();\n    setError('');\n    setLoading(true);\n    \n    const result = await signup(name, email, password);\n    \n    if (result.success) {\n      onClose();\n    } else {\n      setError(result.error);\n    }\n    \n    setLoading(false);\n  };\n\n  return (\n    <div className=\"modal-overlay\" onClick={onClose}>\n      <div className=\"modal-content\" onClick={(e) => e.stopPropagation()}>\n        <button className=\"modal-close\" onClick={onClose}>&times;</button>\n        \n        <h2>Sign Up</h2>\n        \n        {error && <div className=\"error-message\">{error}</div>}\n        \n        <form onSubmit={handleSubmit}>\n          <div className=\"form-group\">\n            <input\n              type=\"text\"\n              placeholder=\"Full Name\"\n              value={name}\n              onChange={(e) => setName(e.target.value)}\n              required\n            />\n          </div>\n          \n          <div className=\"form-group\">\n            <input\n              type=\"email\"\n              placeholder=\"Email address\"\n              value={email}\n              onChange={(e) => setEmail(e.target.value)}\n              required\n            />\n          </div>\n          \n          <div className=\"form-group\">\n            <input\n              type=\"password\"\n              placeholder=\"Password\"\n              value={password}\n              onChange={(e) => setPassword(e.target.value)}\n              required\n              minLength=\"8\"\n            />\n          </div>\n          \n          <button type=\"submit\" className=\"btn-submit\" disabled={loading}>\n            {loading ? 'Creating Account...' : 'Sign Up'}\n          </button>\n        </form>\n        \n        <div className=\"modal-footer\">\n          <p>Already have an account? <a href=\"#\" onClick={(e) => { \n            e.preventDefault(); \n            onClose(); \n            if (onSwitchToLogin) onSwitchToLogin();\n          }}>Sign in</a></p>\n        </div>\n      </div>\n    </div>\n  );\n}\n\nexport default SignupModal;\n","size_bytes":2428},"frontend/src/components/FeatureRow.jsx":{"content":"import { useRef } from 'react';\nimport FeatureCard from './FeatureCard';\nimport './FeatureRow.css';\n\nfunction FeatureRow({ title, features, onCardClick }) {\n  const rowRef = useRef(null);\n\n  const scroll = (direction) => {\n    if (rowRef.current) {\n      const scrollAmount = 300;\n      rowRef.current.scrollBy({\n        left: direction === 'left' ? -scrollAmount : scrollAmount,\n        behavior: 'smooth'\n      });\n    }\n  };\n\n  return (\n    <div className=\"feature-row\">\n      <h2 className=\"row-title\">{title}</h2>\n      <div className=\"row-container\">\n        <button className=\"scroll-btn scroll-left\" onClick={() => scroll('left')}>\n          â€¹\n        </button>\n        <div className=\"cards-container\" ref={rowRef}>\n          {features.map((feature, idx) => (\n            <FeatureCard \n              key={idx} \n              feature={feature} \n              onClick={() => onCardClick(feature)}\n            />\n          ))}\n        </div>\n        <button className=\"scroll-btn scroll-right\" onClick={() => scroll('right')}>\n          â€º\n        </button>\n      </div>\n    </div>\n  );\n}\n\nexport default FeatureRow;\n","size_bytes":1126},"README.md":{"content":"# ğŸ¬ Anuvaad AI - AI-Powered Video Dubbing Platform\n\nTransform your videos across languages with professional AI dubbing, translation, and creative tools.\n\n## âœ¨ Features\n\n- ğŸ¥ **Video Dubbing** - AI-powered dubbing in 50+ languages\n- ğŸ“º **YouTube Summarizer** - Get instant AI summaries of videos  \n- ğŸ“– **Word to Story** - Transform words into creative stories\n- ğŸ™ï¸ **Article to Podcast** - Convert articles to audio\n- ğŸ—£ï¸ **Text to Speech** - Natural voice synthesis\n- ğŸ¤ **Speech to Text** - Accurate transcription\n- ğŸŒ **Text Translation** - AI-powered translation\n\n## ğŸš€ Quick Start\n\n### Prerequisites\n\n- Python 3.10+\n- Node.js 18+\n- FFmpeg\n\n### Installation\n\n1. **Clone the repository**\n   ```bash\n   cd anuvaad-ai\n   ```\n\n2. **Create virtual environment**\n   ```bash\n   python -m venv venv\n   ```\n\n3. **Activate virtual environment**\n   - Windows: `venv\\Scripts\\activate`\n   - Mac/Linux: `source venv/bin/activate`\n\n4. **Install dependencies**\n   ```bash\n   pip install -r app_requirements.txt\n   ```\n\n5. **Configure API keys**\n   ```bash\n   cp .env.example .env\n   ```\n   Then edit `.env` and add your API keys:\n   - Get ElevenLabs key: https://elevenlabs.io/app/settings/api-keys\n   - Get Gemini key: https://ai.google.dev/\n\n### Run the App\n\nJust one command:\n\n```bash\npython start.py\n```\n\nThat's it! ğŸ‰\n\nThe app will be available at **http://localhost:5000**\n\nPress `Ctrl+C` to stop.\n\n## ğŸ“ Project Structure\n\n```\nanuvaad-ai/\nâ”œâ”€â”€ start.py              # ğŸš€ Single command to start everything\nâ”œâ”€â”€ .env.example          # Template for API keys\nâ”œâ”€â”€ .env                  # Your API keys (create this)\nâ”œâ”€â”€ app_requirements.txt  # Python dependencies\nâ”œâ”€â”€ backend.py            # Flask backend\nâ”œâ”€â”€ frontend/             # React frontend\nâ””â”€â”€ ...\n```\n\n## ğŸ”‘ API Keys\n\nYou need:\n\n1. **ElevenLabs** (AI voice generation)\n   - Sign up: https://elevenlabs.io/\n   - Get key: https://elevenlabs.io/app/settings/api-keys\n\n2. **Google Gemini** (AI translation)\n   - Get key: https://ai.google.dev/\n\n## ğŸ“– Documentation\n\nSee `setup_guide.txt` for detailed setup instructions and troubleshooting.\n\n## ğŸ› ï¸ Tech Stack\n\n- **Frontend**: React + Vite\n- **Backend**: Flask (Python)\n- **AI**: ElevenLabs + Google Gemini\n- **Processing**: MoviePy, FFmpeg, Pydub\n\n## ğŸ’¡ Usage Tips\n\n- First-time setup takes 2-5 minutes (dependency installation)\n- Video processing is resource-intensive\n- Requires stable internet for AI APIs\n- Keep your `.env` file secure\n\n## ğŸ› Common Issues\n\n**Module not found?**\n```bash\npip install -r app_requirements.txt\n```\n\n**FFmpeg not found?**\nInstall FFmpeg and add to PATH\n\n**Port in use?**\nStop other apps using ports 5000/5001\n\n## ğŸ“ License\n\nAll rights reserved.\n\n---\n\nMade with â¤ï¸ by Anuvaad AI Team\n","size_bytes":2807},"frontend/src/components/FAQ.css":{"content":".faq-section {\n  padding: 4rem 4%;\n  background: #000;\n}\n\n.faq-title {\n  color: #e5e5e5;\n  font-size: 2rem;\n  font-weight: 700;\n  margin-bottom: 2rem;\n  text-align: left;\n}\n\n.faq-container {\n  max-width: 1200px;\n  margin: 0 auto;\n}\n\n.faq-item {\n  margin-bottom: 0.5rem;\n}\n\n.faq-question {\n  width: 100%;\n  background: #2d2d2d;\n  border: none;\n  padding: 1.5rem;\n  display: flex;\n  justify-content: space-between;\n  align-items: center;\n  cursor: pointer;\n  transition: background 0.3s ease;\n  color: #e5e5e5;\n  font-size: 1.25rem;\n  font-weight: 400;\n  text-align: left;\n}\n\n.faq-question:hover {\n  background: #3d3d3d;\n}\n\n.faq-question.active {\n  background: #3d3d3d;\n}\n\n.faq-icon {\n  font-size: 2.5rem;\n  font-weight: 300;\n  line-height: 1;\n  transition: transform 0.3s ease;\n  min-width: 40px;\n  text-align: center;\n}\n\n.faq-question.active .faq-icon {\n  transform: rotate(45deg);\n}\n\n.faq-answer {\n  max-height: 0;\n  overflow: hidden;\n  transition: max-height 0.3s ease, padding 0.3s ease;\n  background: #2d2d2d;\n  margin-top: 1px;\n}\n\n.faq-answer.open {\n  max-height: 500px;\n  padding: 1.5rem;\n}\n\n.faq-answer p {\n  color: #b3b3b3;\n  font-size: 1.125rem;\n  line-height: 1.8;\n  margin: 0;\n}\n\n@media (max-width: 768px) {\n  .faq-section {\n    padding: 3rem 4%;\n  }\n\n  .faq-title {\n    font-size: 1.5rem;\n  }\n\n  .faq-question {\n    font-size: 1.1rem;\n    padding: 1.25rem;\n  }\n\n  .faq-icon {\n    font-size: 2rem;\n  }\n\n  .faq-answer p {\n    font-size: 1rem;\n  }\n}\n","size_bytes":1459},"frontend/src/components/Reasons.jsx":{"content":"import './Reasons.css';\n\nfunction Reasons() {\n  const reasons = [\n    {\n      icon: 'ğŸŒ',\n      title: 'Break Language Barriers',\n      description: 'Reach global audiences by translating and dubbing your content into 50+ languages with AI-powered precision and natural-sounding voices.'\n    },\n    {\n      icon: 'âš¡',\n      title: 'Lightning-Fast Processing',\n      description: 'Get your dubbed videos and translations in minutes, not days. Our AI technology processes content at incredible speeds without compromising quality.'\n    },\n    {\n      icon: 'ğŸ¯',\n      title: 'Professional Quality Output',\n      description: 'Experience studio-quality results with natural voice synthesis, perfect lip-sync, and emotional tone preservation in every translation.'\n    },\n    {\n      icon: 'ğŸ’¡',\n      title: 'All-in-One Creative Suite',\n      description: 'Access video dubbing, YouTube summarization, story generation, and more - all the tools you need for content creation in one place.'\n    }\n  ];\n\n  return (\n    <section className=\"reasons-section\">\n      <h2 className=\"reasons-title\">More reasons to use Anuvaad AI</h2>\n      <div className=\"reasons-grid\">\n        {reasons.map((reason, idx) => (\n          <div key={idx} className=\"reason-card\">\n            <div className=\"reason-icon-wrapper\">\n              <span className=\"reason-icon\">{reason.icon}</span>\n            </div>\n            <h3>{reason.title}</h3>\n            <p>{reason.description}</p>\n          </div>\n        ))}\n      </div>\n    </section>\n  );\n}\n\nexport default Reasons;\n","size_bytes":1559},"frontend/src/components/FAQ.jsx":{"content":"import { useState } from 'react';\nimport './FAQ.css';\n\nfunction FAQ() {\n  const [openIndex, setOpenIndex] = useState(null);\n\n  const faqs = [\n    {\n      question: 'What is Anuvaad AI?',\n      answer: 'Anuvaad AI is a professional AI-powered platform that helps you translate and dub videos, summarize YouTube content, generate creative stories, and perform various text and audio transformations. We use advanced AI technology from ElevenLabs and Google Gemini to deliver high-quality, natural-sounding results across 50+ languages.'\n    },\n    {\n      question: 'How does the video dubbing feature work?',\n      answer: 'Our video dubbing feature uses AI to extract audio from your video, translate it to your chosen language, generate natural-sounding dubbed audio with the same emotional tone, and synchronize it perfectly with your video. The entire process is automated and typically takes just a few minutes, delivering professional-quality dubbed videos ready for global audiences.'\n    },\n    {\n      question: 'Which languages are supported?',\n      answer: 'Anuvaad AI supports 50+ languages including English, Spanish, French, German, Hindi, Mandarin, Japanese, Korean, Arabic, Portuguese, Italian, Russian, and many more. Our AI ensures natural translations while preserving context, tone, and cultural nuances.'\n    },\n    {\n      question: 'What file formats are supported?',\n      answer: 'For video dubbing, we support common formats like MP4, AVI, MOV, and MKV. Audio files can be WAV, MP3, or M4A. Our YouTube Summarizer works with any YouTube video URL. All processed files are returned in widely compatible formats for easy use.'\n    },\n    {\n      question: 'How accurate is the translation?',\n      answer: 'Our translations use Google Gemini AI, which provides highly accurate, context-aware translations. The AI understands idioms, cultural references, and maintains the original meaning while adapting to the target language naturally. For critical content, we recommend reviewing translations, but our accuracy rate is excellent for most use cases.'\n    },\n    {\n      question: 'Is there a limit on video length or file size?',\n      answer: 'File size and video length limits depend on your subscription plan. Free users can process videos up to 10 minutes and 100MB. Premium users enjoy extended limits with videos up to 2 hours and 1GB file sizes. Check your account dashboard for specific limits.'\n    },\n    {\n      question: 'Can I use Anuvaad AI for commercial purposes?',\n      answer: 'Yes! Anuvaad AI can be used for commercial projects including marketing videos, educational content, podcasts, YouTube videos, and business presentations. All content you create with our platform is yours to use commercially. Please review our Terms of Service for complete licensing details.'\n    },\n    {\n      question: 'How do I get started?',\n      answer: 'Getting started is easy! Simply sign up for a free account, choose the feature you want to use (video dubbing, YouTube summarizer, etc.), upload your content or provide a URL, select your target language, and let our AI do the work. Your processed content will be ready for download in minutes.'\n    }\n  ];\n\n  const toggleFAQ = (index) => {\n    setOpenIndex(openIndex === index ? null : index);\n  };\n\n  return (\n    <section id=\"faq\" className=\"faq-section\">\n      <h2 className=\"faq-title\">Frequently Asked Questions</h2>\n      <div className=\"faq-container\">\n        {faqs.map((faq, index) => (\n          <div key={index} className=\"faq-item\">\n            <button \n              className={`faq-question ${openIndex === index ? 'active' : ''}`}\n              onClick={() => toggleFAQ(index)}\n            >\n              <span>{faq.question}</span>\n              <span className=\"faq-icon\">{openIndex === index ? 'Ã—' : '+'}</span>\n            </button>\n            <div className={`faq-answer ${openIndex === index ? 'open' : ''}`}>\n              <p>{faq.answer}</p>\n            </div>\n          </div>\n        ))}\n      </div>\n    </section>\n  );\n}\n\nexport default FAQ;\n","size_bytes":4059},"frontend/src/components/Reasons.css":{"content":".reasons-section {\n  padding: 4rem 4%;\n  background: #000;\n}\n\n.reasons-title {\n  color: #e5e5e5;\n  font-size: 2rem;\n  font-weight: 700;\n  margin-bottom: 2.5rem;\n  text-align: left;\n}\n\n.reasons-grid {\n  display: grid;\n  grid-template-columns: repeat(auto-fit, minmax(250px, 1fr));\n  gap: 1rem;\n}\n\n.reason-card {\n  background: linear-gradient(149deg, #192247 0%, #210e17 96.86%);\n  border-radius: 16px;\n  padding: 2rem 1.5rem;\n  transition: transform 0.3s ease;\n}\n\n.reason-card:hover {\n  transform: scale(1.02);\n}\n\n.reason-icon-wrapper {\n  margin-bottom: 1.5rem;\n}\n\n.reason-icon {\n  font-size: 3rem;\n  display: block;\n}\n\n.reason-card h3 {\n  color: #e5e5e5;\n  font-size: 1.5rem;\n  font-weight: 600;\n  margin-bottom: 1rem;\n}\n\n.reason-card p {\n  color: #b3b3b3;\n  font-size: 1rem;\n  line-height: 1.6;\n}\n\n@media (max-width: 1024px) {\n  .reasons-grid {\n    grid-template-columns: repeat(2, 1fr);\n  }\n}\n\n@media (max-width: 640px) {\n  .reasons-section {\n    padding: 3rem 4%;\n  }\n\n  .reasons-title {\n    font-size: 1.5rem;\n  }\n\n  .reasons-grid {\n    grid-template-columns: 1fr;\n  }\n\n  .reason-card {\n    padding: 1.5rem 1rem;\n  }\n\n  .reason-icon {\n    font-size: 2.5rem;\n  }\n\n  .reason-card h3 {\n    font-size: 1.3rem;\n  }\n}\n","size_bytes":1216},"start.py":{"content":"#!/usr/bin/env python3\n\"\"\"\nAnuvaad AI Startup Script\nStarts both backend and frontend servers with a single command\n\"\"\"\n\nimport os\nimport sys\nimport subprocess\nimport signal\nimport time\nfrom pathlib import Path\n\ndef check_env_file():\n    \"\"\"Check if .env file exists\"\"\"\n    if not os.path.exists('.env'):\n        print(\"âŒ Error: .env file not found!\")\n        print(\"\\nğŸ“ Please create a .env file with your API keys.\")\n        print(\"   You can copy .env.example and fill in your keys:\")\n        print(\"   cp .env.example .env\")\n        print(\"\\n   Then edit .env and add your API keys.\")\n        sys.exit(1)\n    print(\"âœ… .env file found\")\n\ndef check_dependencies():\n    \"\"\"Check if required dependencies are installed\"\"\"\n    print(\"\\nğŸ” Checking dependencies...\")\n    \n    # Check Node.js\n    try:\n        subprocess.run(['node', '--version'], check=True, capture_output=True)\n        print(\"âœ… Node.js is installed\")\n    except (subprocess.CalledProcessError, FileNotFoundError):\n        print(\"âŒ Node.js is not installed. Please install it from https://nodejs.org/\")\n        sys.exit(1)\n    \n    # Check if frontend dependencies are installed\n    if not os.path.exists('frontend/node_modules'):\n        print(\"\\nğŸ“¦ Installing frontend dependencies...\")\n        try:\n            subprocess.run(['npm', 'install'], cwd='frontend', check=True)\n            print(\"âœ… Frontend dependencies installed\")\n        except subprocess.CalledProcessError:\n            print(\"âŒ Failed to install frontend dependencies\")\n            sys.exit(1)\n    else:\n        print(\"âœ… Frontend dependencies are installed\")\n\ndef start_servers():\n    \"\"\"Start both backend and frontend servers\"\"\"\n    processes = []\n    \n    try:\n        print(\"\\n\" + \"=\"*60)\n        print(\"ğŸš€ Starting Anuvaad AI...\")\n        print(\"=\"*60)\n        \n        # Start backend\n        print(\"\\nğŸ“¡ Starting Backend Server (Flask)...\")\n        backend_process = subprocess.Popen(\n            [sys.executable, 'backend.py'],\n            stdout=subprocess.PIPE,\n            stderr=subprocess.STDOUT,\n            text=True,\n            bufsize=1\n        )\n        processes.append(backend_process)\n        time.sleep(2)  # Give backend time to start\n        print(\"âœ… Backend started on http://localhost:5001\")\n        \n        # Start frontend\n        print(\"\\nğŸ¨ Starting Frontend Server (Vite)...\")\n        frontend_process = subprocess.Popen(\n            ['npm', 'run', 'dev', '--', '--host', '0.0.0.0', '--port', '5000'],\n            cwd='frontend',\n            stdout=subprocess.PIPE,\n            stderr=subprocess.STDOUT,\n            text=True,\n            bufsize=1\n        )\n        processes.append(frontend_process)\n        time.sleep(3)  # Give frontend time to start\n        \n        print(\"\\n\" + \"=\"*60)\n        print(\"âœ… Anuvaad AI is now running!\")\n        print(\"=\"*60)\n        print(\"\\nğŸŒ Open your browser and go to:\")\n        print(\"   ğŸ‘‰ http://localhost:5000\")\n        print(\"\\nğŸ“Š Backend API running at:\")\n        print(\"   ğŸ‘‰ http://localhost:5001\")\n        print(\"\\nâ¹ï¸  Press Ctrl+C to stop both servers\")\n        print(\"=\"*60 + \"\\n\")\n        \n        # Keep the script running and show output\n        while True:\n            time.sleep(1)\n            \n            # Check if any process has died\n            for proc in processes:\n                if proc.poll() is not None:\n                    print(f\"\\nâŒ A server process stopped unexpectedly\")\n                    raise KeyboardInterrupt\n                    \n    except KeyboardInterrupt:\n        print(\"\\n\\nâ¹ï¸  Stopping servers...\")\n        for proc in processes:\n            try:\n                proc.terminate()\n                proc.wait(timeout=5)\n            except:\n                proc.kill()\n        print(\"âœ… All servers stopped\")\n        print(\"\\nThank you for using Anuvaad AI! ğŸ‘‹\\n\")\n        sys.exit(0)\n\nif __name__ == \"__main__\":\n    print(\"\"\"\n    â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n    â•‘                                                           â•‘\n    â•‘              ğŸ¬ ANUVAAD AI STARTUP SCRIPT ğŸ¬             â•‘\n    â•‘                                                           â•‘\n    â•‘           AI-Powered Video Dubbing Platform               â•‘\n    â•‘                                                           â•‘\n    â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n    \"\"\")\n    \n    check_env_file()\n    check_dependencies()\n    start_servers()\n","size_bytes":4742}},"version":2}